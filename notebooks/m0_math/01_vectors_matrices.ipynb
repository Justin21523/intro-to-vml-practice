{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0.1: 向量與矩陣基礎\n",
    "\n",
    "這個 notebook 會帶你從零開始理解向量和矩陣，並實作基本運算。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解向量的幾何意義\n",
    "2. 內積 (dot product) 的計算和意義\n",
    "3. 各種範數 (norm)\n",
    "4. 矩陣乘法作為線性變換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: 向量的直覺\n",
    "\n",
    "### 什麼是向量？\n",
    "\n",
    "向量可以用兩種方式理解：\n",
    "\n",
    "1. **作為一個點的座標**：向量 `[3, 4]` 代表 2D 空間中的一個點\n",
    "2. **作為方向和長度**：從原點出發，指向某個方向，有一定的長度\n",
    "\n",
    "在機器學習中，我們常常把**資料表示成向量**：\n",
    "- 一張 28×28 的灰階圖片 → 784 維向量\n",
    "- 一個人的特徵（身高、體重、年齡）→ 3 維向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vector(v, origin=(0, 0), color='blue', label=None):\n",
    "    \"\"\"畫出從 origin 出發的向量\"\"\"\n",
    "    plt.quiver(origin[0], origin[1], v[0], v[1], \n",
    "               angles='xy', scale_units='xy', scale=1, \n",
    "               color=color, label=label, width=0.02)\n",
    "\n",
    "# 範例：畫幾個向量\n",
    "v1 = np.array([3, 2])\n",
    "v2 = np.array([1, 4])\n",
    "v3 = v1 + v2  # 向量加法\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_vector(v1, color='blue', label=f'v1 = {v1}')\n",
    "plot_vector(v2, color='red', label=f'v2 = {v2}')\n",
    "plot_vector(v3, color='green', label=f'v1 + v2 = {v3}')\n",
    "# 顯示向量加法的平行四邊形法則\n",
    "plot_vector(v2, origin=v1, color='red', label=None)\n",
    "\n",
    "plt.xlim(-1, 6)\n",
    "plt.ylim(-1, 7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.title('向量加法：平行四邊形法則')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: 向量的長度 (Norm)\n",
    "\n",
    "向量的「長度」有很多種定義方式：\n",
    "\n",
    "### L2 Norm（歐幾里得距離）- 最常用！\n",
    "\n",
    "$$||\\mathbf{v}||_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} = \\sqrt{\\sum_{i=1}^n v_i^2}$$\n",
    "\n",
    "這就是我們直覺上的「長度」，也是畢氏定理的推廣。\n",
    "\n",
    "### L1 Norm（曼哈頓距離）\n",
    "\n",
    "$$||\\mathbf{v}||_1 = |v_1| + |v_2| + \\cdots + |v_n| = \\sum_{i=1}^n |v_i|$$\n",
    "\n",
    "想像在格子狀的街道上走路，只能走直線（不能走對角線）。\n",
    "\n",
    "### L∞ Norm（最大值範數）\n",
    "\n",
    "$$||\\mathbf{v}||_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)$$\n",
    "\n",
    "### 為什麼要知道這些？\n",
    "\n",
    "- **L2 正則化**（Ridge）使用 L2 norm\n",
    "- **L1 正則化**（Lasso）使用 L1 norm，會產生稀疏解\n",
    "- 不同的 norm 定義不同的「距離」概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化不同 norm 的「單位圓」\n",
    "# 單位圓 = 所有 norm = 1 的點的集合\n",
    "\n",
    "theta = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "# L2 單位圓（真正的圓）\n",
    "x_l2 = np.cos(theta)\n",
    "y_l2 = np.sin(theta)\n",
    "\n",
    "# L1 單位圓（菱形）\n",
    "# |x| + |y| = 1\n",
    "x_l1 = np.concatenate([np.linspace(0, 1, 250), np.linspace(1, 0, 250),\n",
    "                       np.linspace(0, -1, 250), np.linspace(-1, 0, 250)])\n",
    "y_l1 = np.concatenate([1 - x_l1[:250], -1 + x_l1[250:500],\n",
    "                       -1 - x_l1[500:750], 1 + x_l1[750:]])\n",
    "\n",
    "# L∞ 單位圓（正方形）\n",
    "# max(|x|, |y|) = 1\n",
    "x_linf = np.array([1, 1, -1, -1, 1])\n",
    "y_linf = np.array([1, -1, -1, 1, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(x_l2, y_l2, 'b-', linewidth=2, label='L2 (圓)')\n",
    "plt.plot(x_l1, y_l1, 'r-', linewidth=2, label='L1 (菱形)')\n",
    "plt.plot(x_linf, y_linf, 'g-', linewidth=2, label='L∞ (正方形)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('不同 Norm 的「單位圓」')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlim(-1.5, 1.5)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1: 實作各種 Norm\n",
    "\n",
    "**提示**：\n",
    "- L2: 先平方、再加總、最後開根號\n",
    "- L1: 先取絕對值、再加總\n",
    "- L∞: 取絕對值中的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(v):\n",
    "    \"\"\"\n",
    "    計算向量的 L2 norm（歐幾里得長度）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    v : np.ndarray\n",
    "        輸入向量\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        L2 norm\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> l2_norm(np.array([3, 4]))\n",
    "    5.0\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    return np.sqrt(np.sum(v ** 2))\n",
    "\n",
    "\n",
    "def l1_norm(v):\n",
    "    \"\"\"\n",
    "    計算向量的 L1 norm（曼哈頓距離）\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> l1_norm(np.array([3, -4]))\n",
    "    7.0\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    return np.sum(np.abs(v))\n",
    "\n",
    "\n",
    "def linf_norm(v):\n",
    "    \"\"\"\n",
    "    計算向量的 L∞ norm（最大絕對值）\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> linf_norm(np.array([3, -4, 2]))\n",
    "    4.0\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    return np.max(np.abs(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "test_v = np.array([3, 4])\n",
    "\n",
    "print(f\"v = {test_v}\")\n",
    "print(f\"L2 norm: {l2_norm(test_v)} (expected: 5.0)\")\n",
    "print(f\"L1 norm: {l1_norm(test_v)} (expected: 7.0)\")\n",
    "print(f\"L∞ norm: {linf_norm(test_v)} (expected: 4.0)\")\n",
    "\n",
    "# 和 numpy 比較驗證\n",
    "print(\"\\n✓ 驗證結果：\")\n",
    "assert np.isclose(l2_norm(test_v), np.linalg.norm(test_v, 2))\n",
    "assert np.isclose(l1_norm(test_v), np.linalg.norm(test_v, 1))\n",
    "assert np.isclose(linf_norm(test_v), np.linalg.norm(test_v, np.inf))\n",
    "print(\"所有測試通過！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: 內積 (Dot Product)\n",
    "\n",
    "兩個向量的內積定義為：\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n = \\sum_{i=1}^n a_i b_i$$\n",
    "\n",
    "### 內積的幾何意義（超重要！）\n",
    "\n",
    "$$\\mathbf{a} \\cdot \\mathbf{b} = ||\\mathbf{a}|| \\cdot ||\\mathbf{b}|| \\cdot \\cos\\theta$$\n",
    "\n",
    "其中 θ 是兩向量的夾角。\n",
    "\n",
    "這告訴我們：\n",
    "- **內積 > 0**：兩向量夾角 < 90°（大致同方向）\n",
    "- **內積 = 0**：兩向量**垂直**（正交）\n",
    "- **內積 < 0**：兩向量夾角 > 90°（大致反方向）\n",
    "\n",
    "### 內積作為「投影」\n",
    "\n",
    "如果 b 是單位向量（||b|| = 1），則 a·b 就是 a 在 b 方向上的投影長度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化內積與角度的關係\n",
    "a = np.array([1, 0])  # 固定向量 a\n",
    "\n",
    "angles = [0, 45, 90, 135, 180]\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, angle in enumerate(angles):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    \n",
    "    # 創建角度為 angle 的向量 b\n",
    "    theta = np.radians(angle)\n",
    "    b = np.array([np.cos(theta), np.sin(theta)])\n",
    "    \n",
    "    # 計算內積\n",
    "    dot = np.dot(a, b)\n",
    "    \n",
    "    plot_vector(a, color='blue', label='a')\n",
    "    plot_vector(b, color='red', label='b')\n",
    "    \n",
    "    plt.xlim(-1.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title(f'θ = {angle}°\\na·b = {dot:.2f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 實作內積和 Cosine Similarity\n",
    "\n",
    "**Cosine Similarity** 是機器學習中非常常用的相似度度量：\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\cdot ||\\mathbf{b}||}$$\n",
    "\n",
    "這個值介於 -1 和 1 之間，只看方向，不看長度。\n",
    "\n",
    "**提示**：\n",
    "- 內積可以用 `np.sum(a * b)` 計算（元素相乘再加總）\n",
    "- cosine similarity 就是內積除以兩個 L2 norm 的乘積"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a, b):\n",
    "    \"\"\"\n",
    "    計算兩個向量的內積\n",
    "    \n",
    "    不要使用 np.dot 或 @\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    return np.sum(a * b)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    計算兩個向量的 cosine similarity\n",
    "    \n",
    "    cosine_similarity = (a · b) / (||a|| × ||b||)\n",
    "    \n",
    "    這個值介於 -1 和 1 之間：\n",
    "    - 1: 完全同向\n",
    "    - 0: 垂直\n",
    "    - -1: 完全反向\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    dot = dot_product(a, b)\n",
    "    norm_a = l2_norm(a)\n",
    "    norm_b = l2_norm(b)\n",
    "    return dot / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "a = np.array([1, 0])\n",
    "b = np.array([1, 1])\n",
    "c = np.array([0, 1])\n",
    "d = np.array([-1, 0])\n",
    "\n",
    "print(\"測試內積：\")\n",
    "print(f\"a·b = {dot_product(a, b)} (expected: 1)\")\n",
    "print(f\"a·c = {dot_product(a, c)} (expected: 0)\")\n",
    "print(f\"a·d = {dot_product(a, d)} (expected: -1)\")\n",
    "\n",
    "print(\"\\n測試 cosine similarity：\")\n",
    "print(f\"cosine(a, b) = {cosine_similarity(a, b):.4f} (expected: 0.7071 = cos(45°))\")\n",
    "print(f\"cosine(a, c) = {cosine_similarity(a, c):.4f} (expected: 0 = cos(90°))\")\n",
    "print(f\"cosine(a, d) = {cosine_similarity(a, d):.4f} (expected: -1 = cos(180°))\")\n",
    "\n",
    "# 驗證\n",
    "print(\"\\n✓ 驗證：\")\n",
    "assert np.isclose(dot_product(a, b), np.dot(a, b))\n",
    "assert np.isclose(cosine_similarity(a, b), np.cos(np.pi/4))\n",
    "print(\"所有測試通過！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: 矩陣乘法\n",
    "\n",
    "矩陣乘法 `C = A @ B` 的規則：\n",
    "\n",
    "- A 的 shape 是 (m, n)\n",
    "- B 的 shape 是 (n, p)\n",
    "- C 的 shape 是 (m, p)\n",
    "\n",
    "$$C_{ij} = \\sum_{k=1}^n A_{ik} B_{kj}$$\n",
    "\n",
    "**直覺理解**：`C[i,j]` 是 A 的第 i **行**和 B 的第 j **列**的內積。\n",
    "\n",
    "### 矩陣乘法的意義：線性變換\n",
    "\n",
    "把 `y = Ax` 想成：矩陣 A 把向量 x「變換」成向量 y。\n",
    "\n",
    "例如：\n",
    "- **旋轉矩陣**可以旋轉向量\n",
    "- **縮放矩陣**可以拉伸/壓縮向量\n",
    "- **神經網路的全連接層**就是矩陣乘法 + 偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範：各種線性變換\n",
    "\n",
    "def rotation_matrix(theta):\n",
    "    \"\"\"旋轉 theta 弧度\"\"\"\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    return np.array([[c, -s],\n",
    "                     [s,  c]])\n",
    "\n",
    "def scaling_matrix(sx, sy):\n",
    "    \"\"\"x 方向縮放 sx 倍，y 方向縮放 sy 倍\"\"\"\n",
    "    return np.array([[sx, 0],\n",
    "                     [0, sy]])\n",
    "\n",
    "def shear_matrix(k):\n",
    "    \"\"\"剪切變換\"\"\"\n",
    "    return np.array([[1, k],\n",
    "                     [0, 1]])\n",
    "\n",
    "# 原始的正方形頂點\n",
    "square = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]]).T  # shape: (2, 5)\n",
    "\n",
    "# 各種變換\n",
    "transforms = [\n",
    "    ('原始', np.eye(2)),\n",
    "    ('旋轉 45°', rotation_matrix(np.pi/4)),\n",
    "    ('縮放 (2, 0.5)', scaling_matrix(2, 0.5)),\n",
    "    ('剪切', shear_matrix(0.5)),\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, (name, M) in enumerate(transforms):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    transformed = M @ square\n",
    "    plt.plot(square[0], square[1], 'b--', alpha=0.5, label='原始')\n",
    "    plt.fill(transformed[0], transformed[1], alpha=0.3)\n",
    "    plt.plot(transformed[0], transformed[1], 'r-', linewidth=2)\n",
    "    plt.xlim(-1, 2.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3: 實作矩陣乘法\n",
    "\n",
    "**提示**：\n",
    "- 用三層迴圈：外兩層遍歷 C 的每個元素 (i, j)，內層計算內積\n",
    "- 或者用兩層迴圈 + numpy 的向量運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A, B):\n",
    "    \"\"\"\n",
    "    矩陣乘法 C = A @ B\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np.ndarray, shape (m, n)\n",
    "    B : np.ndarray, shape (n, p)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    C : np.ndarray, shape (m, p)\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2, f\"Matrix dimensions don't match: {A.shape} and {B.shape}\"\n",
    "    \n",
    "    # 解答（方法 1：三層迴圈）\n",
    "    C = np.zeros((m, p))\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    return C\n",
    "\n",
    "\n",
    "def matmul_v2(A, B):\n",
    "    \"\"\"\n",
    "    矩陣乘法（更簡潔的寫法）\n",
    "    \n",
    "    使用 numpy 的向量運算，只需要兩層迴圈\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2\n",
    "    \n",
    "    # 解答（方法 2：兩層迴圈 + 內積）\n",
    "    C = np.zeros((m, p))\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            # C[i,j] 是 A 的第 i 行和 B 的第 j 列的內積\n",
    "            C[i, j] = np.sum(A[i, :] * B[:, j])\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "C_v1 = matmul(A, B)\n",
    "C_v2 = matmul_v2(A, B)\n",
    "C_numpy = A @ B\n",
    "\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "print(\"\\nB:\")\n",
    "print(B)\n",
    "print(\"\\nA @ B (你的實作):\")\n",
    "print(C_v1)\n",
    "print(\"\\nA @ B (numpy):\")\n",
    "print(C_numpy)\n",
    "\n",
    "# 驗證\n",
    "print(\"\\n✓ 驗證：\")\n",
    "assert np.allclose(C_v1, C_numpy)\n",
    "assert np.allclose(C_v2, C_numpy)\n",
    "print(\"所有測試通過！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: 影像作為向量\n",
    "\n",
    "在機器學習中，我們常常把影像「攤平」成向量：\n",
    "\n",
    "- 一張 28×28 的灰階圖 → 784 維向量\n",
    "- 一張 32×32×3 的彩色圖 → 3072 維向量\n",
    "\n",
    "這樣就可以對影像做向量運算了！\n",
    "\n",
    "### 為什麼這很重要？\n",
    "\n",
    "1. **神經網路的輸入**：全連接層需要向量輸入\n",
    "2. **計算相似度**：可以用 cosine similarity 比較兩張圖\n",
    "3. **PCA 降維**：需要把影像當作高維向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建一些簡單的「影像」\n",
    "img1 = np.array([[0, 1, 2],\n",
    "                 [3, 4, 5],\n",
    "                 [6, 7, 8]])\n",
    "\n",
    "print(\"Original image (3x3):\")\n",
    "print(img1)\n",
    "print(f\"Shape: {img1.shape}\")\n",
    "\n",
    "# 攤平成向量\n",
    "vec = img1.flatten()  # 或 img1.reshape(-1) 或 img1.ravel()\n",
    "print(f\"\\nFlattened vector: {vec}\")\n",
    "print(f\"Shape: {vec.shape}\")\n",
    "\n",
    "# 可以再 reshape 回來\n",
    "img_back = vec.reshape(3, 3)\n",
    "print(f\"\\nReshaped back to image:\\n{img_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 4: 計算兩張影像的相似度\n",
    "\n",
    "**提示**：\n",
    "1. 把影像攤平成向量\n",
    "2. 用你前面寫的 `cosine_similarity` 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_similarity(img1, img2):\n",
    "    \"\"\"\n",
    "    計算兩張影像的 cosine similarity\n",
    "    \n",
    "    1. 把影像攤平成向量\n",
    "    2. 計算 cosine similarity\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    vec1 = img1.flatten().astype(float)\n",
    "    vec2 = img2.flatten().astype(float)\n",
    "    return cosine_similarity(vec1, vec2)\n",
    "\n",
    "\n",
    "def euclidean_distance(img1, img2):\n",
    "    \"\"\"\n",
    "    計算兩張影像的歐幾里得距離\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    vec1 = img1.flatten().astype(float)\n",
    "    vec2 = img2.flatten().astype(float)\n",
    "    return l2_norm(vec1 - vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "img_a = np.array([[1, 2], [3, 4]])\n",
    "img_b = np.array([[1, 2], [3, 4]])  # 完全相同\n",
    "img_c = np.array([[2, 4], [6, 8]])  # 比例相同（方向相同）\n",
    "img_d = np.array([[4, 3], [2, 1]])  # 不同\n",
    "\n",
    "print(\"Cosine Similarity:\")\n",
    "print(f\"sim(a, b) = {image_similarity(img_a, img_b):.4f} (expected: 1.0，完全相同)\")\n",
    "print(f\"sim(a, c) = {image_similarity(img_a, img_c):.4f} (expected: 1.0，方向相同)\")\n",
    "print(f\"sim(a, d) = {image_similarity(img_a, img_d):.4f} (不同圖片)\")\n",
    "\n",
    "print(\"\\nEuclidean Distance:\")\n",
    "print(f\"dist(a, b) = {euclidean_distance(img_a, img_b):.4f} (expected: 0.0)\")\n",
    "print(f\"dist(a, c) = {euclidean_distance(img_a, img_c):.4f}\")\n",
    "print(f\"dist(a, d) = {euclidean_distance(img_a, img_d):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "這個 notebook 你學到了：\n",
    "\n",
    "1. **向量**可以表示點或方向，在 ML 中用來表示資料\n",
    "2. **Norm** 是向量的「長度」，有 L1, L2, L∞ 等不同定義\n",
    "3. **內積**可以測量兩向量的相似度（方向）\n",
    "4. **Cosine Similarity** = 內積 / (兩個長度的乘積)，只看方向不看長度\n",
    "5. **矩陣乘法**可以視為線性變換\n",
    "6. **影像**可以攤平成向量來做運算\n",
    "\n",
    "### 這些概念在 ML 中的應用：\n",
    "\n",
    "| 概念 | 應用 |\n",
    "|------|------|\n",
    "| L2 Norm | 計算距離、L2 正則化 |\n",
    "| L1 Norm | L1 正則化（產生稀疏解） |\n",
    "| 內積 | 全連接層 y = Wx + b |\n",
    "| Cosine Similarity | 文字/影像相似度、推薦系統 |\n",
    "| 矩陣乘法 | 神經網路的核心運算 |\n",
    "\n",
    "---\n",
    "\n",
    "**下一個 notebook**: `02_gradients.ipynb` - 梯度與微分"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
