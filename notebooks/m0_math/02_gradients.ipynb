{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0.2: 梯度與微分\n",
    "\n",
    "這個 notebook 會教你理解梯度的意義，並學會手推簡單函數的梯度。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解梯度的幾何意義\n",
    "2. 對純量函數求向量梯度\n",
    "3. 鏈式法則 (Chain Rule)\n",
    "4. 用數值微分驗證解析梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: 什麼是梯度？\n",
    "\n",
    "### 回顧：一維的導數\n",
    "\n",
    "對於一維函數 $f(x)$，導數 $f'(x)$ 告訴我們：\n",
    "- 函數在 $x$ 點的**變化率**\n",
    "- 如果 $f'(x) > 0$，函數在增加\n",
    "- 如果 $f'(x) < 0$，函數在減少\n",
    "\n",
    "### 多維的梯度\n",
    "\n",
    "對於多維函數 $f(\\mathbf{x})$ 其中 $\\mathbf{x} = [x_1, x_2, ..., x_n]$：\n",
    "\n",
    "$$\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "梯度是一個**向量**，它的每個元素是 $f$ 對每個變數的偏導數。\n",
    "\n",
    "### 梯度的幾何意義（超重要！）\n",
    "\n",
    "**梯度指向函數增長最快的方向**\n",
    "\n",
    "這就是為什麼梯度下降要往 $-\\nabla f$ 方向走——因為那是函數**減少**最快的方向！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 2D 函數的梯度\n",
    "# f(x, y) = x² + y²（一個碗狀的函數）\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def grad_f(x, y):\n",
    "    \"\"\"梯度 = [2x, 2y]\"\"\"\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "# 創建網格\n",
    "x = np.linspace(-2, 2, 20)\n",
    "y = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# 畫等高線和梯度向量\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=15, cmap='viridis')\n",
    "plt.colorbar(label='f(x, y)')\n",
    "\n",
    "# 在幾個點畫梯度向量\n",
    "points = [(-1.5, -1), (0.5, 1.5), (1, -0.5), (-0.5, 0.5)]\n",
    "for px, py in points:\n",
    "    gx, gy = grad_f(px, py)\n",
    "    # 縮放梯度長度以便顯示\n",
    "    scale = 0.2\n",
    "    plt.arrow(px, py, gx*scale, gy*scale, \n",
    "              head_width=0.1, head_length=0.05, fc='red', ec='red')\n",
    "    plt.plot(px, py, 'ro', markersize=8)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('f(x,y) = x² + y² 的等高線和梯度\\n紅色箭頭指向函數增長最快的方向')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: 常見函數的梯度\n",
    "\n",
    "### 例 1：線性函數\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$$\n",
    "\n",
    "$$\\nabla_\\mathbf{x} f = \\mathbf{w}$$\n",
    "\n",
    "### 例 2：L2 範數平方\n",
    "\n",
    "$$f(\\mathbf{x}) = ||\\mathbf{x}||_2^2 = x_1^2 + x_2^2 + \\cdots + x_n^2$$\n",
    "\n",
    "$$\\nabla f = 2\\mathbf{x}$$\n",
    "\n",
    "### 例 3：二次型（Quadratic Form）\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{x}^T A \\mathbf{x}$$\n",
    "\n",
    "其中 A 是對稱矩陣：\n",
    "\n",
    "$$\\nabla f = 2A\\mathbf{x}$$\n",
    "\n",
    "如果 A 不是對稱的：\n",
    "\n",
    "$$\\nabla f = (A + A^T)\\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1: 實作這些梯度\n",
    "\n",
    "**提示**：\n",
    "- 線性函數的梯度就是係數向量\n",
    "- L2 範數平方的梯度是 2x\n",
    "- 二次型需要用矩陣運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(x, w):\n",
    "    \"\"\"f(x) = w^T x\"\"\"\n",
    "    return np.dot(w, x)\n",
    "\n",
    "def grad_linear(x, w):\n",
    "    \"\"\"∇f = w\"\"\"\n",
    "    # 解答：\n",
    "    return w.copy()\n",
    "\n",
    "\n",
    "def l2_squared(x):\n",
    "    \"\"\"f(x) = ||x||² = x₁² + x₂² + ...\"\"\"\n",
    "    return np.sum(x ** 2)\n",
    "\n",
    "def grad_l2_squared(x):\n",
    "    \"\"\"∇f = 2x\"\"\"\n",
    "    # 解答：\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def quadratic_form(x, A):\n",
    "    \"\"\"f(x) = x^T A x\"\"\"\n",
    "    return x @ A @ x\n",
    "\n",
    "def grad_quadratic_form(x, A):\n",
    "    \"\"\"∇f = (A + A^T) x\"\"\"\n",
    "    # 解答：\n",
    "    return (A + A.T) @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: 數值微分（用來驗證你的梯度）\n",
    "\n",
    "如何知道你手推的梯度對不對？用**數值微分**來驗證！\n",
    "\n",
    "### 前向差分\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + \\epsilon e_i) - f(x)}{\\epsilon}$$\n",
    "\n",
    "### 中央差分（更準確！）\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + \\epsilon e_i) - f(x - \\epsilon e_i)}{2\\epsilon}$$\n",
    "\n",
    "其中 $e_i$ 是第 $i$ 個維度的單位向量，$\\epsilon$ 是很小的數（通常 $10^{-5}$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    用中央差分計算數值梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        純量函數 f(x) -> float\n",
    "    x : np.ndarray\n",
    "        計算梯度的位置\n",
    "    eps : float\n",
    "        差分步長\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : np.ndarray\n",
    "        數值梯度\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    grad = np.zeros_like(x, dtype=float)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        # 保存原值\n",
    "        old_val = x[i]\n",
    "        \n",
    "        # f(x + eps)\n",
    "        x[i] = old_val + eps\n",
    "        f_plus = f(x)\n",
    "        \n",
    "        # f(x - eps)\n",
    "        x[i] = old_val - eps\n",
    "        f_minus = f(x)\n",
    "        \n",
    "        # 中央差分\n",
    "        grad[i] = (f_plus - f_minus) / (2 * eps)\n",
    "        \n",
    "        # 恢復原值\n",
    "        x[i] = old_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_check(analytic_grad, f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    比較解析梯度和數值梯度\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    relative_error : float\n",
    "        相對誤差，應該 < 1e-5\n",
    "    \"\"\"\n",
    "    numeric_grad = numerical_gradient(f, x.copy(), eps)\n",
    "    \n",
    "    # 計算相對誤差\n",
    "    diff = np.linalg.norm(analytic_grad - numeric_grad)\n",
    "    norm_sum = np.linalg.norm(analytic_grad) + np.linalg.norm(numeric_grad)\n",
    "    \n",
    "    if norm_sum == 0:\n",
    "        relative_error = 0\n",
    "    else:\n",
    "        relative_error = diff / norm_sum\n",
    "    \n",
    "    print(f\"Analytic gradient:  {analytic_grad}\")\n",
    "    print(f\"Numerical gradient: {numeric_grad}\")\n",
    "    print(f\"Relative error: {relative_error:.2e}\")\n",
    "    \n",
    "    if relative_error < 1e-5:\n",
    "        print(\"✓ Gradient check PASSED!\")\n",
    "    else:\n",
    "        print(\"✗ Gradient check FAILED!\")\n",
    "    \n",
    "    return relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 1: 線性函數\n",
    "print(\"=\" * 50)\n",
    "print(\"Test 1: Linear function f(x) = w^T x\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "w = np.array([1.0, 2.0, 3.0])\n",
    "x = np.array([0.5, -0.3, 0.8])\n",
    "\n",
    "f_linear = lambda x: linear_function(x, w)\n",
    "analytic = grad_linear(x, w)\n",
    "\n",
    "gradient_check(analytic, f_linear, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 2: L2 範數平方\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 2: L2 squared f(x) = ||x||²\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "analytic = grad_l2_squared(x)\n",
    "\n",
    "gradient_check(analytic, l2_squared, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 3: 二次型\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Test 3: Quadratic form f(x) = x^T A x\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "A = np.array([[2.0, 1.0],\n",
    "              [1.0, 3.0]])\n",
    "x = np.array([1.0, 2.0])\n",
    "\n",
    "f_quad = lambda x: quadratic_form(x, A)\n",
    "analytic = grad_quadratic_form(x, A)\n",
    "\n",
    "gradient_check(analytic, f_quad, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: 鏈式法則 (Chain Rule)\n",
    "\n",
    "這是反向傳播的數學基礎！\n",
    "\n",
    "### 一維版本\n",
    "\n",
    "如果 $y = f(x)$ 且 $z = g(y)$，則：\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n",
    "\n",
    "### 多維版本\n",
    "\n",
    "如果 $\\mathbf{y} = f(\\mathbf{x})$ 且 $L = g(\\mathbf{y})$（$L$ 是純量），則：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_i} = \\sum_j \\frac{\\partial L}{\\partial y_j} \\cdot \\frac{\\partial y_j}{\\partial x_i}$$\n",
    "\n",
    "用矩陣表示：\n",
    "\n",
    "$$\\nabla_\\mathbf{x} L = J^T \\nabla_\\mathbf{y} L$$\n",
    "\n",
    "其中 $J$ 是 Jacobian 矩陣。\n",
    "\n",
    "### 為什麼這很重要？\n",
    "\n",
    "神經網路的反向傳播就是不斷應用鏈式法則：\n",
    "\n",
    "```\n",
    "x → [Layer 1] → h1 → [Layer 2] → h2 → ... → [Loss] → L\n",
    "```\n",
    "\n",
    "計算 $\\frac{\\partial L}{\\partial \\text{Layer 1 參數}}$ 需要用鏈式法則把梯度從 L 傳回去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 鏈式法則\n",
    "\n",
    "考慮複合函數：\n",
    "\n",
    "$$L = ||\\mathbf{y}||^2$$\n",
    "$$\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "求 $\\nabla_\\mathbf{x} L$、$\\nabla_W L$、$\\nabla_\\mathbf{b} L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W, b):\n",
    "    \"\"\"\n",
    "    Forward pass: y = Wx + b, L = ||y||²\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    L : float\n",
    "        Loss value\n",
    "    cache : dict\n",
    "        保存的中間值，backward 會用到\n",
    "    \"\"\"\n",
    "    y = W @ x + b\n",
    "    L = np.sum(y ** 2)\n",
    "    cache = {'x': x, 'W': W, 'b': b, 'y': y}\n",
    "    return L, cache\n",
    "\n",
    "\n",
    "def backward(cache):\n",
    "    \"\"\"\n",
    "    Backward pass: 計算梯度\n",
    "    \n",
    "    用鏈式法則：\n",
    "    ∂L/∂y = 2y (因為 L = ||y||²)\n",
    "    ∂L/∂W = ∂L/∂y · ∂y/∂W = 2y · x^T\n",
    "    ∂L/∂b = ∂L/∂y · ∂y/∂b = 2y\n",
    "    ∂L/∂x = ∂L/∂y · ∂y/∂x = W^T · 2y\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad_x : np.ndarray\n",
    "    grad_W : np.ndarray\n",
    "    grad_b : np.ndarray\n",
    "    \"\"\"\n",
    "    x, W, b, y = cache['x'], cache['W'], cache['b'], cache['y']\n",
    "    \n",
    "    # 解答：\n",
    "    # Step 1: ∂L/∂y\n",
    "    grad_y = 2 * y\n",
    "    \n",
    "    # Step 2: 用鏈式法則\n",
    "    grad_x = W.T @ grad_y\n",
    "    grad_W = np.outer(grad_y, x)  # grad_y @ x.T\n",
    "    grad_b = grad_y\n",
    "    \n",
    "    return grad_x, grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試鏈式法則\n",
    "np.random.seed(42)\n",
    "\n",
    "# 隨機初始化\n",
    "x = np.random.randn(3)\n",
    "W = np.random.randn(2, 3)\n",
    "b = np.random.randn(2)\n",
    "\n",
    "# Forward\n",
    "L, cache = forward(x, W, b)\n",
    "print(f\"Loss L = {L:.4f}\")\n",
    "\n",
    "# Backward\n",
    "grad_x, grad_W, grad_b = backward(cache)\n",
    "\n",
    "# 驗證 grad_x\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Checking ∂L/∂x\")\n",
    "print(\"=\" * 50)\n",
    "f_x = lambda x: forward(x, W, b)[0]\n",
    "gradient_check(grad_x, f_x, x.copy())\n",
    "\n",
    "# 驗證 grad_b\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Checking ∂L/∂b\")\n",
    "print(\"=\" * 50)\n",
    "f_b = lambda b: forward(x, W, b)[0]\n",
    "gradient_check(grad_b, f_b, b.copy())\n",
    "\n",
    "# 驗證 grad_W (需要 flatten)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Checking ∂L/∂W\")\n",
    "print(\"=\" * 50)\n",
    "def f_W(W_flat):\n",
    "    W_mat = W_flat.reshape(W.shape)\n",
    "    return forward(x, W_mat, b)[0]\n",
    "\n",
    "gradient_check(grad_W.flatten(), f_W, W.flatten().copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: 梯度下降預熱\n",
    "\n",
    "現在你知道梯度是什麼了，來看看梯度下降如何找函數的最小值。\n",
    "\n",
    "### 梯度下降算法\n",
    "\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)$$\n",
    "\n",
    "其中 $\\eta$ 是學習率 (learning rate)。\n",
    "\n",
    "**直覺**：往「下坡」方向走一小步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化梯度下降\n",
    "# 最小化 f(x, y) = x² + 2y²（橢圓形的碗）\n",
    "\n",
    "def f(xy):\n",
    "    x, y = xy\n",
    "    return x**2 + 2*y**2\n",
    "\n",
    "def grad_f(xy):\n",
    "    x, y = xy\n",
    "    return np.array([2*x, 4*y])\n",
    "\n",
    "# 梯度下降\n",
    "def gradient_descent(f, grad_f, x0, lr=0.1, n_iter=20):\n",
    "    x = x0.copy()\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        g = grad_f(x)\n",
    "        x = x - lr * g\n",
    "        history.append(x.copy())\n",
    "    \n",
    "    return np.array(history)\n",
    "\n",
    "# 從 (3, 3) 開始\n",
    "x0 = np.array([3.0, 3.0])\n",
    "history = gradient_descent(f, grad_f, x0, lr=0.15, n_iter=15)\n",
    "\n",
    "# 畫圖\n",
    "x = np.linspace(-4, 4, 100)\n",
    "y = np.linspace(-4, 4, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 2*Y**2\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "plt.colorbar(label='f(x, y)')\n",
    "\n",
    "# 畫軌跡\n",
    "plt.plot(history[:, 0], history[:, 1], 'ro-', markersize=8, linewidth=2, label='GD path')\n",
    "plt.plot(history[0, 0], history[0, 1], 'go', markersize=15, label='Start')\n",
    "plt.plot(0, 0, 'b*', markersize=20, label='Minimum')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('梯度下降尋找 f(x,y) = x² + 2y² 的最小值')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"起點: {history[0]}\")\n",
    "print(f\"終點: {history[-1]}\")\n",
    "print(f\"終點的 f 值: {f(history[-1]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "這個 notebook 你學到了：\n",
    "\n",
    "1. **梯度**是多維導數，指向函數增長最快的方向\n",
    "2. **常見函數的梯度**：\n",
    "   - $f = \\mathbf{w}^T\\mathbf{x}$ → $\\nabla f = \\mathbf{w}$\n",
    "   - $f = ||\\mathbf{x}||^2$ → $\\nabla f = 2\\mathbf{x}$\n",
    "   - $f = \\mathbf{x}^T A \\mathbf{x}$ → $\\nabla f = (A + A^T)\\mathbf{x}$\n",
    "3. **數值微分**可以用來驗證你手推的梯度\n",
    "4. **鏈式法則**是反向傳播的基礎\n",
    "5. **梯度下降**往 $-\\nabla f$ 方向走來找最小值\n",
    "\n",
    "---\n",
    "\n",
    "**下一個 notebook**: `03_probability.ipynb` - 機率基礎"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
