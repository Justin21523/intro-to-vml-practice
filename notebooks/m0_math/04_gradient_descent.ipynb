{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0.4: 梯度下降實作\n",
    "\n",
    "這個 notebook 會讓你完整實作梯度下降，並理解各種變體。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 實作基本梯度下降\n",
    "2. 理解學習率的影響\n",
    "3. 實作 Momentum\n",
    "4. 用梯度下降做線性迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: 基本梯度下降\n",
    "\n",
    "### 算法\n",
    "\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f(\\mathbf{x}_t)$$\n",
    "\n",
    "- $\\eta$：學習率 (learning rate)\n",
    "- $\\nabla f$：梯度（指向函數增長最快的方向）\n",
    "\n",
    "**直覺**：不斷往「下坡」方向走一小步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, lr=0.1, n_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    基本梯度下降\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        目標函數\n",
    "    grad_f : callable\n",
    "        梯度函數\n",
    "    x0 : np.ndarray\n",
    "        初始點\n",
    "    lr : float\n",
    "        學習率\n",
    "    n_iter : int\n",
    "        最大迭代次數\n",
    "    tol : float\n",
    "        收斂容忍度（梯度小於此值時停止）\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        最終解\n",
    "    history : dict\n",
    "        包含 'x', 'f', 'grad_norm' 的歷史記錄\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    x = x0.copy().astype(float)\n",
    "    history = {\n",
    "        'x': [x.copy()],\n",
    "        'f': [f(x)],\n",
    "        'grad_norm': []\n",
    "    }\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        history['grad_norm'].append(grad_norm)\n",
    "        \n",
    "        # 檢查收斂\n",
    "        if grad_norm < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "        \n",
    "        # 更新\n",
    "        x = x - lr * grad\n",
    "        \n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "    \n",
    "    history['x'] = np.array(history['x'])\n",
    "    history['f'] = np.array(history['f'])\n",
    "    history['grad_norm'] = np.array(history['grad_norm'])\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試：最小化 f(x, y) = x² + y²\n",
    "def f_simple(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def grad_f_simple(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "x0 = np.array([5.0, 5.0])\n",
    "x_opt, history = gradient_descent(f_simple, grad_f_simple, x0, lr=0.1, n_iter=50)\n",
    "\n",
    "print(f\"起點: {x0}\")\n",
    "print(f\"終點: {x_opt}\")\n",
    "print(f\"f(終點) = {f_simple(x_opt):.6f}\")\n",
    "print(f\"迭代次數: {len(history['f']) - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化收斂過程\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. 等高線圖和軌跡\n",
    "ax = axes[0]\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = np.linspace(-6, 6, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "ax.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax.plot(history['x'][:, 0], history['x'][:, 1], 'ro-', markersize=4)\n",
    "ax.plot(history['x'][0, 0], history['x'][0, 1], 'go', markersize=10, label='Start')\n",
    "ax.plot(0, 0, 'b*', markersize=15, label='Minimum')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('梯度下降軌跡')\n",
    "ax.legend()\n",
    "\n",
    "# 2. Loss 曲線\n",
    "ax = axes[1]\n",
    "ax.plot(history['f'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Loss 曲線')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 梯度大小\n",
    "ax = axes[2]\n",
    "ax.plot(history['grad_norm'], 'r-', linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('||∇f||')\n",
    "ax.set_title('梯度大小')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: 學習率的影響\n",
    "\n",
    "學習率太大或太小都有問題：\n",
    "\n",
    "- **太小**：收斂太慢\n",
    "- **太大**：震盪或發散\n",
    "- **剛好**：快速收斂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較不同學習率\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.95]\n",
    "x0 = np.array([5.0, 5.0])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-7, 7, 100)\n",
    "y = np.linspace(-7, 7, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "for ax, lr in zip(axes, learning_rates):\n",
    "    _, history = gradient_descent(f_simple, grad_f_simple, x0, lr=lr, n_iter=30)\n",
    "    \n",
    "    ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "    ax.plot(history['x'][:, 0], history['x'][:, 1], 'ro-', markersize=4)\n",
    "    ax.plot(history['x'][0, 0], history['x'][0, 1], 'go', markersize=10)\n",
    "    ax.set_xlim(-7, 7)\n",
    "    ax.set_ylim(-7, 7)\n",
    "    ax.set_title(f'Learning Rate = {lr}\\nFinal f = {history[\"f\"][-1]:.4f}')\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Momentum\n",
    "\n",
    "加入「動量」可以：\n",
    "1. 加速收斂（尤其在有窄谷的情況）\n",
    "2. 幫助跳出局部最小值\n",
    "\n",
    "### 算法\n",
    "\n",
    "$$\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t - \\eta \\nabla f(\\mathbf{x}_t)$$\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\mathbf{v}_{t+1}$$\n",
    "\n",
    "- $\\beta$：動量係數（通常 0.9）\n",
    "- $\\mathbf{v}$：速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(f, grad_f, x0, lr=0.1, momentum=0.9, n_iter=100):\n",
    "    \"\"\"\n",
    "    帶 Momentum 的梯度下降\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    x = x0.copy().astype(float)\n",
    "    v = np.zeros_like(x)  # 初始速度為 0\n",
    "    \n",
    "    history = {\n",
    "        'x': [x.copy()],\n",
    "        'f': [f(x)]\n",
    "    }\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        # 更新速度\n",
    "        v = momentum * v - lr * grad\n",
    "        \n",
    "        # 更新位置\n",
    "        x = x + v\n",
    "        \n",
    "        history['x'].append(x.copy())\n",
    "        history['f'].append(f(x))\n",
    "    \n",
    "    history['x'] = np.array(history['x'])\n",
    "    history['f'] = np.array(history['f'])\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在「窄谷」函數上比較 GD vs Momentum\n",
    "# f(x, y) = x² + 10y² （y 方向很「陡」）\n",
    "\n",
    "def f_valley(x):\n",
    "    return x[0]**2 + 10*x[1]**2\n",
    "\n",
    "def grad_f_valley(x):\n",
    "    return np.array([2*x[0], 20*x[1]])\n",
    "\n",
    "x0 = np.array([5.0, 2.0])\n",
    "\n",
    "# 普通 GD\n",
    "_, history_gd = gradient_descent(f_valley, grad_f_valley, x0, lr=0.05, n_iter=50)\n",
    "\n",
    "# Momentum\n",
    "_, history_mom = gradient_descent_momentum(f_valley, grad_f_valley, x0, lr=0.05, momentum=0.9, n_iter=50)\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 等高線\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + 10*Y**2\n",
    "\n",
    "ax = axes[0]\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', alpha=0.5)\n",
    "ax.plot(history_gd['x'][:, 0], history_gd['x'][:, 1], 'ro-', markersize=3, label='GD')\n",
    "ax.plot(history_mom['x'][:, 0], history_mom['x'][:, 1], 'bs-', markersize=3, label='Momentum')\n",
    "ax.plot(x0[0], x0[1], 'go', markersize=10)\n",
    "ax.set_title('軌跡比較\\n（窄谷函數 f = x² + 10y²）')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history_gd['f'], 'r-', label='GD')\n",
    "ax.plot(history_mom['f'], 'b-', label='Momentum')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Loss 曲線')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(history_gd['f'][:20], 'r-', label='GD')\n",
    "ax.plot(history_mom['f'][:20], 'b-', label='Momentum')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('前 20 次迭代（線性尺度）')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"GD 50 次後: f = {history_gd['f'][-1]:.6f}\")\n",
    "print(f\"Momentum 50 次後: f = {history_mom['f'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: 用梯度下降做線性迴歸\n",
    "\n",
    "把學到的梯度下降應用到實際問題！\n",
    "\n",
    "### 線性迴歸\n",
    "\n",
    "模型：$\\hat{y} = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "\n",
    "Loss：$L = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2$ (MSE)\n",
    "\n",
    "梯度：\n",
    "- $\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{2}{N} X^T (X\\mathbf{w} + b - \\mathbf{y})$\n",
    "- $\\frac{\\partial L}{\\partial b} = \\frac{2}{N} \\sum_i (\\hat{y}_i - y_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD:\n",
    "    \"\"\"\n",
    "    用梯度下降訓練的線性迴歸\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.history = {'loss': []}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        訓練模型\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "            訓練資料\n",
    "        y : np.ndarray, shape (N,)\n",
    "            目標值\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # 初始化參數\n",
    "        self.w = np.zeros(D)\n",
    "        self.b = 0.0\n",
    "        \n",
    "        # 解答：梯度下降\n",
    "        for i in range(self.n_iter):\n",
    "            # 預測\n",
    "            y_pred = X @ self.w + self.b\n",
    "            \n",
    "            # 計算 loss\n",
    "            loss = np.mean((y_pred - y) ** 2)\n",
    "            self.history['loss'].append(loss)\n",
    "            \n",
    "            # 計算梯度\n",
    "            error = y_pred - y  # shape: (N,)\n",
    "            grad_w = (2 / N) * (X.T @ error)\n",
    "            grad_b = (2 / N) * np.sum(error)\n",
    "            \n",
    "            # 更新參數\n",
    "            self.w = self.w - self.lr * grad_w\n",
    "            self.b = self.b - self.lr * grad_b\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成模擬資料\n",
    "np.random.seed(42)\n",
    "N = 100\n",
    "X = np.random.randn(N, 1) * 2\n",
    "y_true = 3 * X[:, 0] + 2  # 真實關係：y = 3x + 2\n",
    "y = y_true + np.random.randn(N) * 0.5  # 加入噪音\n",
    "\n",
    "# 訓練\n",
    "model = LinearRegressionGD(lr=0.1, n_iter=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"真實參數: w = 3, b = 2\")\n",
    "print(f\"學到的參數: w = {model.w[0]:.4f}, b = {model.b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 1. 資料和擬合線\n",
    "ax = axes[0]\n",
    "ax.scatter(X, y, alpha=0.5, label='Data')\n",
    "x_line = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_line = model.predict(x_line)\n",
    "ax.plot(x_line, y_line, 'r-', linewidth=2, label=f'Fit: y = {model.w[0]:.2f}x + {model.b:.2f}')\n",
    "ax.plot(x_line, 3*x_line + 2, 'g--', linewidth=2, label='True: y = 3x + 2')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('線性迴歸結果')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Loss 曲線\n",
    "ax = axes[1]\n",
    "ax.plot(model.history['loss'], 'b-', linewidth=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('訓練過程')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 練習題\n\n### 練習 1：實作 Nesterov Momentum\n\nNesterov Accelerated Gradient (NAG) 是 Momentum 的改進版，先「向前看」再計算梯度：\n\n$$\\mathbf{v}_{t+1} = \\beta \\mathbf{v}_t - \\eta \\nabla f(\\mathbf{x}_t + \\beta \\mathbf{v}_t)$$\n$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\mathbf{v}_{t+1}$$\n\n**任務**：實作 `gradient_descent_nesterov` 函數"
  },
  {
   "cell_type": "code",
   "source": "# 練習 1 解答：Nesterov Momentum\n\ndef gradient_descent_nesterov(f, grad_f, x0, lr=0.1, momentum=0.9, n_iter=100):\n    \"\"\"\n    Nesterov Accelerated Gradient\n    \n    與一般 Momentum 的差別：先用當前速度「預測」下一個位置，\n    然後在那個位置計算梯度。這讓算法有「前瞻性」。\n    \"\"\"\n    x = x0.copy().astype(float)\n    v = np.zeros_like(x)\n    \n    history = {\n        'x': [x.copy()],\n        'f': [f(x)]\n    }\n    \n    for i in range(n_iter):\n        # 關鍵差異：在「預測位置」計算梯度\n        x_lookahead = x + momentum * v\n        grad = grad_f(x_lookahead)\n        \n        # 更新速度\n        v = momentum * v - lr * grad\n        \n        # 更新位置\n        x = x + v\n        \n        history['x'].append(x.copy())\n        history['f'].append(f(x))\n    \n    history['x'] = np.array(history['x'])\n    history['f'] = np.array(history['f'])\n    \n    return x, history\n\n# 測試並比較三種方法\nx0 = np.array([5.0, 2.0])\n\n_, hist_gd = gradient_descent(f_valley, grad_f_valley, x0, lr=0.05, n_iter=50)\n_, hist_mom = gradient_descent_momentum(f_valley, grad_f_valley, x0, lr=0.05, momentum=0.9, n_iter=50)\n_, hist_nag = gradient_descent_nesterov(f_valley, grad_f_valley, x0, lr=0.05, momentum=0.9, n_iter=50)\n\nplt.figure(figsize=(10, 4))\nplt.plot(hist_gd['f'], 'r-', label='GD')\nplt.plot(hist_mom['f'], 'b-', label='Momentum')\nplt.plot(hist_nag['f'], 'g-', label='Nesterov')\nplt.xlabel('Iteration')\nplt.ylabel('f(x)')\nplt.title('三種優化方法比較')\nplt.legend()\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"GD:       f = {hist_gd['f'][-1]:.6f}\")\nprint(f\"Momentum: f = {hist_mom['f'][-1]:.6f}\")\nprint(f\"Nesterov: f = {hist_nag['f'][-1]:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 練習 2：實作 Learning Rate Decay\n\n隨著訓練進行，逐漸減小學習率可以幫助收斂到更好的解。\n\n常見的 decay 策略：\n- Step decay: $\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/k \\rfloor}$（每 k 步乘以 γ）\n- Exponential decay: $\\eta_t = \\eta_0 \\cdot e^{-\\lambda t}$\n\n**任務**：在梯度下降中加入 learning rate decay",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 練習 2 解答：Learning Rate Decay\n\ndef gradient_descent_with_decay(f, grad_f, x0, lr=0.1, n_iter=100, \n                                 decay_type='step', decay_rate=0.5, decay_steps=20):\n    \"\"\"\n    帶學習率衰減的梯度下降\n    \n    Parameters\n    ----------\n    decay_type : str\n        'step' 或 'exponential'\n    decay_rate : float\n        衰減率（step: gamma, exponential: lambda）\n    decay_steps : int\n        step decay 的步數間隔\n    \"\"\"\n    x = x0.copy().astype(float)\n    \n    history = {\n        'x': [x.copy()],\n        'f': [f(x)],\n        'lr': []\n    }\n    \n    for i in range(n_iter):\n        # 計算當前學習率\n        if decay_type == 'step':\n            current_lr = lr * (decay_rate ** (i // decay_steps))\n        elif decay_type == 'exponential':\n            current_lr = lr * np.exp(-decay_rate * i)\n        else:\n            current_lr = lr\n        \n        history['lr'].append(current_lr)\n        \n        grad = grad_f(x)\n        x = x - current_lr * grad\n        \n        history['x'].append(x.copy())\n        history['f'].append(f(x))\n    \n    history['x'] = np.array(history['x'])\n    history['f'] = np.array(history['f'])\n    history['lr'] = np.array(history['lr'])\n    \n    return x, history\n\n# 測試\nx0 = np.array([5.0, 5.0])\n\n_, hist_const = gradient_descent(f_simple, grad_f_simple, x0, lr=0.3, n_iter=50)\n_, hist_step = gradient_descent_with_decay(f_simple, grad_f_simple, x0, lr=0.3, n_iter=50, \n                                            decay_type='step', decay_rate=0.5, decay_steps=15)\n_, hist_exp = gradient_descent_with_decay(f_simple, grad_f_simple, x0, lr=0.3, n_iter=50,\n                                           decay_type='exponential', decay_rate=0.05)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nax = axes[0]\nax.plot(hist_const['f'], 'r-', label='Constant LR')\nax.plot(hist_step['f'], 'b-', label='Step Decay')\nax.plot(hist_exp['f'], 'g-', label='Exponential Decay')\nax.set_xlabel('Iteration')\nax.set_ylabel('f(x)')\nax.set_title('Loss 曲線')\nax.legend()\nax.set_yscale('log')\nax.grid(True, alpha=0.3)\n\nax = axes[1]\nax.plot([0.3]*50, 'r-', label='Constant')\nax.plot(hist_step['lr'], 'b-', label='Step Decay')\nax.plot(hist_exp['lr'], 'g-', label='Exponential Decay')\nax.set_xlabel('Iteration')\nax.set_ylabel('Learning Rate')\nax.set_title('學習率變化')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}