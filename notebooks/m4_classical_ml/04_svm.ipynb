{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.4: SVM - 支持向量機\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "完成這個 notebook 後，你將能夠：\n",
    "\n",
    "1. 理解 SVM 的幾何直覺（最大間隔分類器）\n",
    "2. 理解 Hinge Loss 的定義與性質\n",
    "3. 使用 Subgradient Descent 優化 SVM\n",
    "4. 視覺化支持向量與決策邊界\n",
    "\n",
    "## 背景知識\n",
    "\n",
    "SVM (Support Vector Machine) 是一種強大的分類器，核心想法是：\n",
    "\n",
    "> 找一個超平面，使得兩類資料之間的「間隔」最大化\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- Bishop PRML Ch. 7: Sparse Kernel Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境設定完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: SVM 的幾何直覺\n",
    "\n",
    "### 線性分類器回顧\n",
    "\n",
    "決策函數：$f(x) = w^T x + b$\n",
    "\n",
    "- $f(x) > 0$：預測類別 +1\n",
    "- $f(x) < 0$：預測類別 -1\n",
    "\n",
    "### 間隔（Margin）\n",
    "\n",
    "點 $x$ 到超平面的距離：\n",
    "\n",
    "$$\\text{distance} = \\frac{|w^T x + b|}{\\|w\\|}$$\n",
    "\n",
    "**間隔** = 最近點到超平面的距離\n",
    "\n",
    "### SVM 目標\n",
    "\n",
    "最大化間隔，同時正確分類所有點：\n",
    "\n",
    "$$\\max_{w, b} \\frac{2}{\\|w\\|} \\quad \\text{s.t.} \\quad y_i (w^T x_i + b) \\geq 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成線性可分資料\n",
    "def generate_linearly_separable_data(n_samples=100, margin=1.0, seed=42):\n",
    "    \"\"\"\n",
    "    生成線性可分的二分類資料\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    margin : float\n",
    "        兩類之間的最小距離\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # 類別 -1\n",
    "    X_neg = np.random.randn(n_per_class, 2) * 0.8\n",
    "    X_neg[:, 0] -= margin\n",
    "    \n",
    "    # 類別 +1\n",
    "    X_pos = np.random.randn(n_per_class, 2) * 0.8\n",
    "    X_pos[:, 0] += margin\n",
    "    \n",
    "    X = np.vstack([X_neg, X_pos])\n",
    "    y = np.array([-1] * n_per_class + [1] * n_per_class)\n",
    "    \n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "# 視覺化\n",
    "X, y = generate_linearly_separable_data(n_samples=100, margin=1.5)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[y==-1, 0], X[y==-1, 1], c='blue', marker='o', s=50, label='Class -1')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s', s=50, label='Class +1')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('線性可分資料')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Hinge Loss\n",
    "\n",
    "### 定義\n",
    "\n",
    "對於標籤 $y \\in \\{-1, +1\\}$：\n",
    "\n",
    "$$L_{\\text{hinge}}(y, f(x)) = \\max(0, 1 - y \\cdot f(x))$$\n",
    "\n",
    "其中 $f(x) = w^T x + b$\n",
    "\n",
    "### 直覺\n",
    "\n",
    "- 如果 $y \\cdot f(x) \\geq 1$（正確分類且距離超平面夠遠），loss = 0\n",
    "- 如果 $y \\cdot f(x) < 1$，loss = $1 - y \\cdot f(x)$\n",
    "\n",
    "### SVM 優化問題\n",
    "\n",
    "$$\\min_{w, b} \\frac{1}{N} \\sum_{i=1}^N \\max(0, 1 - y_i (w^T x_i + b)) + \\frac{\\lambda}{2} \\|w\\|^2$$\n",
    "\n",
    "這是 Hinge Loss + L2 正則化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(y_true, scores):\n",
    "    \"\"\"\n",
    "    計算 Hinge Loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray, shape (N,)\n",
    "        真實標籤，值為 -1 或 +1\n",
    "    scores : np.ndarray, shape (N,)\n",
    "        決策函數的輸出 f(x) = w^T x + b\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        平均 Hinge Loss\n",
    "    \"\"\"\n",
    "    margins = y_true * scores  # y * f(x)\n",
    "    losses = np.maximum(0, 1 - margins)  # max(0, 1 - y*f(x))\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "# 視覺化 Hinge Loss vs 其他 Loss\n",
    "scores = np.linspace(-3, 3, 100)\n",
    "y = 1  # 假設真實標籤是 +1\n",
    "\n",
    "# 不同的 loss 函數\n",
    "hinge = np.maximum(0, 1 - y * scores)\n",
    "logistic = np.log(1 + np.exp(-y * scores))  # Logistic loss\n",
    "zero_one = (y * scores < 0).astype(float)  # 0-1 loss\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scores, hinge, 'b-', linewidth=2, label='Hinge Loss')\n",
    "plt.plot(scores, logistic, 'g-', linewidth=2, label='Logistic Loss')\n",
    "plt.plot(scores, zero_one, 'r--', linewidth=2, label='0-1 Loss')\n",
    "plt.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
    "plt.axvline(x=1, color='gray', linestyle=':', alpha=0.5, label='Margin boundary')\n",
    "plt.xlabel('$f(x) = w^T x + b$ (for y=+1)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('不同 Loss 函數的比較')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-0.5, 4)\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：\")\n",
    "print(\"- Hinge Loss 在 f(x) >= 1 時為 0（滿足 margin）\")\n",
    "print(\"- Hinge Loss 是 0-1 Loss 的凸上界\")\n",
    "print(\"- Logistic Loss 永遠不為 0，但比較平滑\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Subgradient Descent\n",
    "\n",
    "### 問題\n",
    "\n",
    "Hinge Loss 在 $1 - y \\cdot f(x) = 0$ 處不可微分（有折點）。\n",
    "\n",
    "### Subgradient\n",
    "\n",
    "對於 $\\max(0, z)$：\n",
    "\n",
    "$$\\frac{\\partial \\max(0, z)}{\\partial z} = \\begin{cases} 0 & \\text{if } z < 0 \\\\ 1 & \\text{if } z > 0 \\\\ [0, 1] & \\text{if } z = 0 \\end{cases}$$\n",
    "\n",
    "實作中，$z = 0$ 時任選 0 或 1。\n",
    "\n",
    "### SVM 梯度\n",
    "\n",
    "令 $L_i = \\max(0, 1 - y_i (w^T x_i + b))$\n",
    "\n",
    "$$\\frac{\\partial L_i}{\\partial w} = \\begin{cases} 0 & \\text{if } y_i (w^T x_i + b) \\geq 1 \\\\ -y_i x_i & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "$$\\frac{\\partial L_i}{\\partial b} = \\begin{cases} 0 & \\text{if } y_i (w^T x_i + b) \\geq 1 \\\\ -y_i & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "加上正則化項：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_i \\frac{\\partial L_i}{\\partial w} + \\lambda w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svm_gradients(X, y, w, b, reg=0.01):\n",
    "    \"\"\"\n",
    "    計算 SVM 的 subgradients\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "        標籤為 -1 或 +1\n",
    "    w : np.ndarray, shape (D,)\n",
    "    b : float\n",
    "    reg : float\n",
    "        正則化參數 λ\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dw : np.ndarray, shape (D,)\n",
    "    db : float\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # 計算 margin\n",
    "    scores = X @ w + b  # shape (N,)\n",
    "    margins = y * scores  # y * f(x)\n",
    "    \n",
    "    # 找出違反 margin 的樣本\n",
    "    # margin < 1 表示有 loss\n",
    "    violating = margins < 1  # shape (N,) boolean\n",
    "    \n",
    "    # 計算梯度\n",
    "    # 對於違反 margin 的樣本：∂L/∂w = -y*x, ∂L/∂b = -y\n",
    "    # 對於滿足 margin 的樣本：梯度為 0\n",
    "    \n",
    "    dw = np.zeros(D)\n",
    "    db = 0.0\n",
    "    \n",
    "    for i in range(N):\n",
    "        if violating[i]:\n",
    "            dw += -y[i] * X[i]\n",
    "            db += -y[i]\n",
    "    \n",
    "    dw = dw / N + reg * w  # 平均 + 正則化\n",
    "    db = db / N  # bias 不正則化\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# 向量化版本（更快）\n",
    "def compute_svm_gradients_vectorized(X, y, w, b, reg=0.01):\n",
    "    \"\"\"\n",
    "    向量化的 SVM subgradient 計算\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    scores = X @ w + b\n",
    "    margins = y * scores\n",
    "    \n",
    "    # 違反 margin 的樣本\n",
    "    violating = (margins < 1).astype(float)  # shape (N,)\n",
    "    \n",
    "    # 梯度（向量化）\n",
    "    # dw = -1/N * sum_{violating} y_i * x_i + λw\n",
    "    dw = -(1/N) * (X.T @ (violating * y)) + reg * w\n",
    "    db = -(1/N) * np.sum(violating * y)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# 驗證兩個版本一致\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(50, 3)\n",
    "y_test = np.sign(np.random.randn(50))\n",
    "w_test = np.random.randn(3)\n",
    "b_test = np.random.randn()\n",
    "\n",
    "dw1, db1 = compute_svm_gradients(X_test, y_test, w_test, b_test)\n",
    "dw2, db2 = compute_svm_gradients_vectorized(X_test, y_test, w_test, b_test)\n",
    "\n",
    "print(\"=== 驗證梯度實作 ===\")\n",
    "print(f\"dw 差異: {np.linalg.norm(dw1 - dw2):.10f}\")\n",
    "print(f\"db 差異: {abs(db1 - db2):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 完整 SVM 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM:\n",
    "    \"\"\"\n",
    "    Linear SVM 使用 Hinge Loss + L2 正則化\n",
    "    \n",
    "    優化問題：\n",
    "    min (1/N) Σ max(0, 1 - y_i(w·x_i + b)) + (λ/2)||w||²\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    C : float\n",
    "        正則化參數。C = 1/λ，較大的 C 表示較少正則化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0):\n",
    "        self.C = C\n",
    "        self.reg = 1.0 / C  # λ = 1/C\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.history = None\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"計算總 loss（hinge + regularization）\"\"\"\n",
    "        scores = X @ self.w + self.b\n",
    "        margins = y * scores\n",
    "        hinge = np.mean(np.maximum(0, 1 - margins))\n",
    "        reg_loss = 0.5 * self.reg * np.sum(self.w ** 2)\n",
    "        return hinge + reg_loss\n",
    "    \n",
    "    def fit(self, X, y, lr=0.01, n_iter=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        使用 Subgradient Descent 訓練 SVM\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "        y : np.ndarray, shape (N,)\n",
    "            標籤為 -1 或 +1\n",
    "        lr : float\n",
    "            學習率\n",
    "        n_iter : int\n",
    "            迭代次數\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # 確保標籤是 -1 或 +1\n",
    "        y = np.where(y == 0, -1, y)\n",
    "        \n",
    "        # 初始化\n",
    "        self.w = np.zeros(D)\n",
    "        self.b = 0.0\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # 計算梯度\n",
    "            scores = X @ self.w + self.b\n",
    "            margins = y * scores\n",
    "            violating = (margins < 1).astype(float)\n",
    "            \n",
    "            dw = -(1/N) * (X.T @ (violating * y)) + self.reg * self.w\n",
    "            db = -(1/N) * np.sum(violating * y)\n",
    "            \n",
    "            # 更新\n",
    "            self.w = self.w - lr * dw\n",
    "            self.b = self.b - lr * db\n",
    "            \n",
    "            # 記錄\n",
    "            loss = self._compute_loss(X, y)\n",
    "            predictions = np.sign(X @ self.w + self.b)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            if verbose and (i + 1) % (n_iter // 10) == 0:\n",
    "                print(f\"Iter {i+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"預測類別（-1 或 +1）\"\"\"\n",
    "        scores = X @ self.w + self.b\n",
    "        return np.sign(scores)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"返回決策分數 f(x) = w·x + b\"\"\"\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"計算準確率\"\"\"\n",
    "        y = np.where(y == 0, -1, y)\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def get_support_vectors(self, X, y, tol=1e-3):\n",
    "        \"\"\"\n",
    "        找出支持向量\n",
    "        \n",
    "        支持向量是滿足 |y * f(x) - 1| < tol 的點\n",
    "        \"\"\"\n",
    "        y = np.where(y == 0, -1, y)\n",
    "        margins = y * self.decision_function(X)\n",
    "        \n",
    "        # 支持向量：margin 接近 1 或 margin < 1\n",
    "        sv_mask = margins <= 1 + tol\n",
    "        return X[sv_mask], y[sv_mask], np.where(sv_mask)[0]\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 Linear SVM ===\")\n",
    "\n",
    "X, y = generate_linearly_separable_data(n_samples=100, margin=1.5)\n",
    "\n",
    "print(f\"資料大小: {X.shape}\")\n",
    "print(f\"標籤分布: {np.bincount(((y + 1) / 2).astype(int))}\")\n",
    "\n",
    "# 訓練\n",
    "svm = LinearSVM(C=1.0)\n",
    "svm.fit(X, y, lr=0.01, n_iter=1000, verbose=True)\n",
    "\n",
    "print(f\"\\n最終準確率: {svm.score(X, y):.4f}\")\n",
    "print(f\"權重: w = {svm.w}\")\n",
    "print(f\"偏置: b = {svm.b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 SVM 決策邊界與支持向量\n",
    "\n",
    "def plot_svm_decision_boundary(svm, X, y, title=\"SVM Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    繪製 SVM 決策邊界、間隔和支持向量\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # 決策分數\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 決策函數等高線\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(-3, 3, 13), cmap='RdYlBu', alpha=0.4)\n",
    "    plt.colorbar(label='$f(x) = w^T x + b$')\n",
    "    \n",
    "    # 決策邊界 (f=0) 和 margin 邊界 (f=±1)\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'black', 'red'],\n",
    "                linestyles=['--', '-', '--'], linewidths=[1.5, 2, 1.5])\n",
    "    \n",
    "    # 資料點\n",
    "    y_plot = np.where(y == 0, -1, y)\n",
    "    plt.scatter(X[y_plot==-1, 0], X[y_plot==-1, 1], c='blue', marker='o', \n",
    "                s=50, label='Class -1', edgecolors='k')\n",
    "    plt.scatter(X[y_plot==1, 0], X[y_plot==1, 1], c='red', marker='s', \n",
    "                s=50, label='Class +1', edgecolors='k')\n",
    "    \n",
    "    # 支持向量\n",
    "    sv_X, sv_y, sv_idx = svm.get_support_vectors(X, y)\n",
    "    plt.scatter(sv_X[:, 0], sv_X[:, 1], s=200, facecolors='none', \n",
    "                edgecolors='green', linewidths=2, label=f'Support Vectors ({len(sv_idx)})')\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plot_svm_decision_boundary(svm, X, y, title='Linear SVM')\n",
    "plt.show()\n",
    "\n",
    "# 訓練曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(svm.history['loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss (Hinge + L2)')\n",
    "axes[0].set_title('訓練 Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(svm.history['accuracy'])\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('訓練準確率')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: C 參數的影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C 參數的影響\n",
    "print(\"=== C 參數的影響 ===\")\n",
    "\n",
    "# 生成有一些噪音的資料\n",
    "X_noisy, y_noisy = generate_linearly_separable_data(n_samples=100, margin=0.5, seed=42)\n",
    "\n",
    "# 加入一些錯分點\n",
    "np.random.seed(123)\n",
    "flip_idx = np.random.choice(100, 5, replace=False)\n",
    "y_noisy[flip_idx] *= -1\n",
    "\n",
    "# 不同的 C 值\n",
    "C_values = [0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, C in zip(axes, C_values):\n",
    "    svm = LinearSVM(C=C)\n",
    "    svm.fit(X_noisy, y_noisy, lr=0.01, n_iter=1000, verbose=False)\n",
    "    \n",
    "    # 繪製\n",
    "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
    "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=np.linspace(-3, 3, 13), cmap='RdYlBu', alpha=0.4)\n",
    "    ax.contour(xx, yy, Z, levels=[-1, 0, 1], colors=['blue', 'black', 'red'],\n",
    "               linestyles=['--', '-', '--'], linewidths=[1.5, 2, 1.5])\n",
    "    \n",
    "    y_plot = np.where(y_noisy == 0, -1, y_noisy)\n",
    "    ax.scatter(X_noisy[y_plot==-1, 0], X_noisy[y_plot==-1, 1], c='blue', marker='o', s=50)\n",
    "    ax.scatter(X_noisy[y_plot==1, 0], X_noisy[y_plot==1, 1], c='red', marker='s', s=50)\n",
    "    \n",
    "    sv_X, _, sv_idx = svm.get_support_vectors(X_noisy, y_noisy)\n",
    "    ax.scatter(sv_X[:, 0], sv_X[:, 1], s=200, facecolors='none', \n",
    "               edgecolors='green', linewidths=2)\n",
    "    \n",
    "    accuracy = svm.score(X_noisy, y_noisy)\n",
    "    ax.set_title(f'C = {C}\\nAcc: {accuracy:.2f}, SVs: {len(sv_idx)}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('不同 C 值的 SVM', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：\")\n",
    "print(\"- 較小的 C（強正則化）：間隔較寬，可能欠擬合\")\n",
    "print(\"- 較大的 C（弱正則化）：間隔較窄，更接近 hard-margin SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題\n",
    "\n",
    "### 練習 1：Multi-class SVM (One-vs-All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1 解答\n",
    "\n",
    "class MultiClassSVM:\n",
    "    \"\"\"\n",
    "    使用 One-vs-All 策略的多分類 SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, C=1.0):\n",
    "        self.n_classes = n_classes\n",
    "        self.C = C\n",
    "        self.classifiers = []\n",
    "    \n",
    "    def fit(self, X, y, lr=0.01, n_iter=1000, verbose=False):\n",
    "        \"\"\"\n",
    "        訓練 K 個二分類 SVM\n",
    "        \"\"\"\n",
    "        self.classifiers = []\n",
    "        \n",
    "        for k in range(self.n_classes):\n",
    "            if verbose:\n",
    "                print(f\"訓練分類器 {k}...\")\n",
    "            \n",
    "            # 建立二分類標籤：類別 k 為 +1，其他為 -1\n",
    "            y_binary = np.where(y == k, 1, -1)\n",
    "            \n",
    "            # 訓練 SVM\n",
    "            svm = LinearSVM(C=self.C)\n",
    "            svm.fit(X, y_binary, lr=lr, n_iter=n_iter, verbose=False)\n",
    "            \n",
    "            self.classifiers.append(svm)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測：選擇分數最高的類別\n",
    "        \"\"\"\n",
    "        scores = np.zeros((X.shape[0], self.n_classes))\n",
    "        \n",
    "        for k, svm in enumerate(self.classifiers):\n",
    "            scores[:, k] = svm.decision_function(X)\n",
    "        \n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"計算準確率\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "\n",
    "# 測試多分類 SVM\n",
    "print(\"=== 測試 Multi-class SVM ===\")\n",
    "\n",
    "# 生成 3 類資料\n",
    "def generate_3class_data(n_samples=150, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n_per_class = n_samples // 3\n",
    "    \n",
    "    X0 = np.random.randn(n_per_class, 2) * 0.6 + np.array([0, 2])\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.6 + np.array([-1.5, -1])\n",
    "    X2 = np.random.randn(n_per_class, 2) * 0.6 + np.array([1.5, -1])\n",
    "    \n",
    "    X = np.vstack([X0, X1, X2])\n",
    "    y = np.array([0] * n_per_class + [1] * n_per_class + [2] * n_per_class)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "X3, y3 = generate_3class_data(n_samples=150)\n",
    "\n",
    "mc_svm = MultiClassSVM(n_classes=3, C=1.0)\n",
    "mc_svm.fit(X3, y3, lr=0.01, n_iter=1000, verbose=True)\n",
    "\n",
    "print(f\"\\n準確率: {mc_svm.score(X3, y3):.4f}\")\n",
    "\n",
    "# 視覺化\n",
    "x_min, x_max = X3[:, 0].min() - 1, X3[:, 0].max() + 1\n",
    "y_min, y_max = X3[:, 1].min() - 1, X3[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "Z = mc_svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "plt.contour(xx, yy, Z, colors='black', linewidths=0.5)\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for k in range(3):\n",
    "    mask = y3 == k\n",
    "    plt.scatter(X3[mask, 0], X3[mask, 1], c=colors[k], s=50, \n",
    "               label=f'Class {k}', edgecolors='k')\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Multi-class SVM (One-vs-All)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的內容\n",
    "\n",
    "1. **SVM 幾何直覺**：\n",
    "   - 最大化間隔\n",
    "   - 支持向量\n",
    "\n",
    "2. **Hinge Loss**：\n",
    "   - $L = \\max(0, 1 - y \\cdot f(x))$\n",
    "   - 分段線性，是 0-1 loss 的凸上界\n",
    "\n",
    "3. **Subgradient Descent**：\n",
    "   - 處理不可微分的 loss\n",
    "   - 梯度選擇\n",
    "\n",
    "4. **C 參數**：\n",
    "   - C = 1/λ\n",
    "   - 控制正則化強度\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. SVM 的優點：最大化間隔有理論保證\n",
    "2. Hinge Loss 的優點：sparse solutions（很多點的梯度為 0）\n",
    "3. 支持向量：決定決策邊界的關鍵點\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個 notebook 中，我們將學習 **K-Means 聚類**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
