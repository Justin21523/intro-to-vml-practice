{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.3: Softmax Regression - 多分類\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "完成這個 notebook 後，你將能夠：\n",
    "\n",
    "1. 理解 Softmax 函數的數學定義與性質\n",
    "2. 理解 Cross-Entropy Loss 用於多分類\n",
    "3. 推導並實作 Softmax Regression 的梯度\n",
    "4. 處理多分類問題\n",
    "\n",
    "## 背景知識\n",
    "\n",
    "Softmax Regression 是 Logistic Regression 的多分類推廣。將 K 個線性分數轉換成 K 個機率。\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- Bishop PRML Ch. 4.3: Probabilistic Discriminative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境設定完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Softmax 函數\n",
    "\n",
    "### 定義\n",
    "\n",
    "給定 K 個分數 $z = [z_1, z_2, ..., z_K]$，Softmax 輸出 K 個機率：\n",
    "\n",
    "$$\\text{softmax}(z)_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "### 性質\n",
    "\n",
    "1. 輸出和為 1：$\\sum_k \\text{softmax}(z)_k = 1$\n",
    "2. 所有輸出都是正的\n",
    "3. 放大差異：最大值對應的機率會很大\n",
    "\n",
    "### 數值穩定性\n",
    "\n",
    "$e^{z_k}$ 可能溢出。解法：減去最大值\n",
    "\n",
    "$$\\text{softmax}(z)_k = \\frac{e^{z_k - \\max(z)}}{\\sum_j e^{z_j - \\max(z)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    數值穩定的 Softmax 函數\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray, shape (N, K) or (K,)\n",
    "        輸入分數\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        機率分布，和輸入形狀相同\n",
    "    \"\"\"\n",
    "    z = np.atleast_2d(z)\n",
    "    \n",
    "    # 減去最大值（數值穩定性）\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    \n",
    "    # 計算 exp\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    \n",
    "    # 正規化\n",
    "    probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    return probs.squeeze()\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 Softmax ===\")\n",
    "\n",
    "z_test = np.array([2.0, 1.0, 0.1])\n",
    "probs = softmax(z_test)\n",
    "\n",
    "print(f\"輸入分數: {z_test}\")\n",
    "print(f\"Softmax 輸出: {probs}\")\n",
    "print(f\"輸出和: {probs.sum():.6f} (應該是 1)\")\n",
    "\n",
    "# 數值穩定性測試\n",
    "z_large = np.array([1000, 1001, 1002])\n",
    "probs_large = softmax(z_large)\n",
    "print(f\"\\n大數值測試: {z_large}\")\n",
    "print(f\"Softmax 輸出: {probs_large} (不應該有 nan 或 inf)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Softmax 的「放大差異」效果\n",
    "\n",
    "# 固定兩個分數，改變第三個\n",
    "z1, z2 = 1.0, 0.0\n",
    "z3_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "probs = np.array([softmax([z1, z2, z3]) for z3 in z3_range])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z3_range, probs[:, 0], 'r-', label='P(class 0) [z=1]')\n",
    "plt.plot(z3_range, probs[:, 1], 'g-', label='P(class 1) [z=0]')\n",
    "plt.plot(z3_range, probs[:, 2], 'b-', label='P(class 2) [z=z3]')\n",
    "plt.xlabel('$z_3$')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Softmax: 改變 $z_3$ 對各類機率的影響')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Cross-Entropy Loss（多分類）\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "標籤 $y = k$ 轉換為 one-hot 向量：\n",
    "\n",
    "$$y_{\\text{one-hot}} = [0, ..., 0, 1, 0, ..., 0]$$ （第 k 位是 1）\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$L = -\\sum_{k=1}^K y_k \\log(\\hat{y}_k)$$\n",
    "\n",
    "由於 $y$ 是 one-hot，只有一項非零：\n",
    "\n",
    "$$L = -\\log(\\hat{y}_{\\text{true class}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, n_classes):\n",
    "    \"\"\"\n",
    "    將標籤轉換為 one-hot 編碼\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.ndarray, shape (N,)\n",
    "        類別標籤（整數 0 到 K-1）\n",
    "    n_classes : int\n",
    "        類別數 K\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, shape (N, K)\n",
    "        One-hot 編碼\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    one_hot = np.zeros((N, n_classes))\n",
    "    one_hot[np.arange(N), y.astype(int)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"\n",
    "    計算 Cross-Entropy Loss（多分類）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray, shape (N,) or (N, K)\n",
    "        真實標籤（整數或 one-hot）\n",
    "    y_pred : np.ndarray, shape (N, K)\n",
    "        預測機率\n",
    "    eps : float\n",
    "        避免 log(0)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        平均 Cross-Entropy Loss\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    if y_true.ndim == 1:\n",
    "        # 標籤是整數，直接索引\n",
    "        N = len(y_true)\n",
    "        return -np.mean(np.log(y_pred[np.arange(N), y_true.astype(int)]))\n",
    "    else:\n",
    "        # 標籤是 one-hot\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 Cross-Entropy Loss ===\")\n",
    "\n",
    "y_true = np.array([0, 1, 2])\n",
    "y_pred = np.array([[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1],\n",
    "                   [0.2, 0.2, 0.6]])\n",
    "\n",
    "loss = cross_entropy_loss(y_true, y_pred)\n",
    "print(f\"真實標籤: {y_true}\")\n",
    "print(f\"預測機率:\\n{y_pred}\")\n",
    "print(f\"Cross-Entropy Loss: {loss:.4f}\")\n",
    "\n",
    "# 驗證：手動計算\n",
    "manual_loss = -np.mean([np.log(0.7), np.log(0.8), np.log(0.6)])\n",
    "print(f\"手動計算: {manual_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 梯度推導\n",
    "\n",
    "### 模型\n",
    "\n",
    "$$z = XW + b$$\n",
    "$$\\hat{y} = \\text{softmax}(z)$$\n",
    "\n",
    "其中 $W \\in \\mathbb{R}^{D \\times K}$，$b \\in \\mathbb{R}^K$\n",
    "\n",
    "### 漂亮的結果\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n",
    "\n",
    "這和 Logistic Regression 一樣！\n",
    "\n",
    "因此：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{1}{N} X^T (\\hat{y} - y)$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_i (\\hat{y}_i - y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_softmax_gradients(X, y, W, b):\n",
    "    \"\"\"\n",
    "    計算 Softmax Regression 的梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "        類別標籤（整數）\n",
    "    W : np.ndarray, shape (D, K)\n",
    "    b : np.ndarray, shape (K,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dW : np.ndarray, shape (D, K)\n",
    "    db : np.ndarray, shape (K,)\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    K = W.shape[1]\n",
    "    \n",
    "    # 前向傳播\n",
    "    z = X @ W + b\n",
    "    y_pred = softmax(z)  # shape (N, K)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape(1, -1)\n",
    "    \n",
    "    # One-hot 編碼\n",
    "    y_one_hot = one_hot_encode(y, K)  # shape (N, K)\n",
    "    \n",
    "    # 梯度\n",
    "    error = y_pred - y_one_hot  # shape (N, K)\n",
    "    \n",
    "    dW = (1 / N) * (X.T @ error)  # shape (D, K)\n",
    "    db = (1 / N) * np.sum(error, axis=0)  # shape (K,)\n",
    "    \n",
    "    return dW, db\n",
    "\n",
    "\n",
    "# 數值梯度檢查\n",
    "def numerical_gradient_matrix(f, x, eps=1e-5):\n",
    "    \"\"\"計算矩陣的數值梯度\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_value = x[idx]\n",
    "        \n",
    "        x[idx] = old_value + eps\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        x[idx] = old_value - eps\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        x[idx] = old_value\n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * eps)\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_check_softmax(X, y, W, b, eps=1e-5):\n",
    "    \"\"\"檢查 Softmax 梯度\"\"\"\n",
    "    K = W.shape[1]\n",
    "    \n",
    "    # 解析梯度\n",
    "    dW_analytic, db_analytic = compute_softmax_gradients(X, y, W, b)\n",
    "    \n",
    "    # 數值梯度 for W\n",
    "    def loss_W(W_test):\n",
    "        z = X @ W_test + b\n",
    "        y_pred = softmax(z)\n",
    "        if y_pred.ndim == 1:\n",
    "            y_pred = y_pred.reshape(1, -1)\n",
    "        return cross_entropy_loss(y, y_pred)\n",
    "    \n",
    "    dW_numeric = numerical_gradient_matrix(loss_W, W.copy(), eps)\n",
    "    \n",
    "    # 數值梯度 for b\n",
    "    def loss_b(b_test):\n",
    "        z = X @ W + b_test\n",
    "        y_pred = softmax(z)\n",
    "        if y_pred.ndim == 1:\n",
    "            y_pred = y_pred.reshape(1, -1)\n",
    "        return cross_entropy_loss(y, y_pred)\n",
    "    \n",
    "    db_numeric = numerical_gradient_matrix(loss_b, b.copy(), eps)\n",
    "    \n",
    "    # 比較\n",
    "    print(\"=== 梯度檢查 ===\")\n",
    "    rel_err_W = np.linalg.norm(dW_analytic - dW_numeric) / (np.linalg.norm(dW_analytic) + np.linalg.norm(dW_numeric) + 1e-8)\n",
    "    rel_err_b = np.linalg.norm(db_analytic - db_numeric) / (np.linalg.norm(db_analytic) + np.linalg.norm(db_numeric) + 1e-8)\n",
    "    \n",
    "    print(f\"dW 相對誤差: {rel_err_W:.2e}\")\n",
    "    print(f\"db 相對誤差: {rel_err_b:.2e}\")\n",
    "    \n",
    "    passed = rel_err_W < 1e-4 and rel_err_b < 1e-4\n",
    "    print(f\"梯度檢查通過: {passed}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "# 測試\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(20, 5)\n",
    "y_test = np.random.randint(0, 3, 20)\n",
    "W_test = np.random.randn(5, 3) * 0.1\n",
    "b_test = np.random.randn(3) * 0.1\n",
    "\n",
    "gradient_check_softmax(X_test, y_test, W_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 完整 Softmax Regression 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Softmax Regression 多分類模型\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int\n",
    "        類別數\n",
    "    regularization : float\n",
    "        L2 正則化強度\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, regularization=0.0):\n",
    "        self.n_classes = n_classes\n",
    "        self.reg = regularization\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.history = None\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        \"\"\"數值穩定的 Softmax\"\"\"\n",
    "        z = np.atleast_2d(z)\n",
    "        z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y, lr=0.1, n_iter=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        訓練模型\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "        y : np.ndarray, shape (N,)\n",
    "            類別標籤（整數 0 到 K-1）\n",
    "        lr : float\n",
    "            學習率\n",
    "        n_iter : int\n",
    "            迭代次數\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        K = self.n_classes\n",
    "        \n",
    "        # 初始化\n",
    "        self.W = np.random.randn(D, K) * 0.01\n",
    "        self.b = np.zeros(K)\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        # One-hot 編碼\n",
    "        y_one_hot = one_hot_encode(y, K)\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # 前向傳播\n",
    "            z = X @ self.W + self.b\n",
    "            y_pred = self._softmax(z)\n",
    "            \n",
    "            # Loss\n",
    "            loss = cross_entropy_loss(y, y_pred) + 0.5 * self.reg * np.sum(self.W ** 2)\n",
    "            \n",
    "            # Accuracy\n",
    "            predictions = np.argmax(y_pred, axis=1)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # 梯度\n",
    "            error = y_pred - y_one_hot\n",
    "            dW = (1 / N) * (X.T @ error) + self.reg * self.W\n",
    "            db = (1 / N) * np.sum(error, axis=0)\n",
    "            \n",
    "            # 更新\n",
    "            self.W = self.W - lr * dW\n",
    "            self.b = self.b - lr * db\n",
    "            \n",
    "            if verbose and (i + 1) % (n_iter // 10) == 0:\n",
    "                print(f\"Iter {i+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"預測機率\"\"\"\n",
    "        z = X @ self.W + self.b\n",
    "        return self._softmax(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"預測類別\"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"計算準確率\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "\n",
    "# 生成多分類資料\n",
    "def generate_multiclass_data(n_samples=300, n_classes=3, seed=42):\n",
    "    \"\"\"\n",
    "    生成多分類測試資料\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_per_class = n_samples // n_classes\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    \n",
    "    # 每個類別的中心（等間隔分布在圓上）\n",
    "    angles = np.linspace(0, 2 * np.pi, n_classes, endpoint=False)\n",
    "    radius = 2.0\n",
    "    \n",
    "    for k in range(n_classes):\n",
    "        center = np.array([radius * np.cos(angles[k]), radius * np.sin(angles[k])])\n",
    "        X_k = np.random.randn(n_per_class, 2) * 0.7 + center\n",
    "        X_list.append(X_k)\n",
    "        y_list.append(np.full(n_per_class, k))\n",
    "    \n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 Softmax Regression ===\")\n",
    "\n",
    "X, y = generate_multiclass_data(n_samples=300, n_classes=3)\n",
    "\n",
    "print(f\"資料大小: {X.shape}\")\n",
    "print(f\"類別分布: {np.bincount(y.astype(int))}\")\n",
    "\n",
    "# 訓練\n",
    "model = SoftmaxRegression(n_classes=3, regularization=0.01)\n",
    "model.fit(X, y, lr=0.5, n_iter=500, verbose=True)\n",
    "\n",
    "print(f\"\\n最終準確率: {model.score(X, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化\n",
    "\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    \"\"\"繪製多分類決策邊界\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    plt.contour(xx, yy, Z, colors='black', linewidths=0.5)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    for k in range(model.n_classes):\n",
    "        mask = y == k\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=colors[k % len(colors)], \n",
    "                   marker=markers[k % len(markers)], s=50, \n",
    "                   label=f'Class {k}', edgecolors='k')\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title('Softmax Regression 決策邊界')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plot_multiclass_decision_boundary(model, X, y)\n",
    "plt.show()\n",
    "\n",
    "# 訓練曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(model.history['loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('訓練 Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(model.history['accuracy'])\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('訓練準確率')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題\n",
    "\n",
    "### 練習 1：更多類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1 解答：5 類別分類\n",
    "\n",
    "print(\"=== 5 類別分類測試 ===\")\n",
    "\n",
    "X5, y5 = generate_multiclass_data(n_samples=500, n_classes=5)\n",
    "\n",
    "model5 = SoftmaxRegression(n_classes=5, regularization=0.01)\n",
    "model5.fit(X5, y5, lr=0.3, n_iter=1000, verbose=True)\n",
    "\n",
    "print(f\"\\n最終準確率: {model5.score(X5, y5):.4f}\")\n",
    "\n",
    "plot_multiclass_decision_boundary(model5, X5, y5)\n",
    "plt.title('5 類別 Softmax Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2 解答\n",
    "\n",
    "def compute_confusion_matrix(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    計算混淆矩陣\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, shape (n_classes, n_classes)\n",
    "        confusion[i, j] = 真實類別 i 被預測為 j 的次數\n",
    "    \"\"\"\n",
    "    confusion = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    \n",
    "    for true, pred in zip(y_true.astype(int), y_pred.astype(int)):\n",
    "        confusion[true, pred] += 1\n",
    "    \n",
    "    return confusion\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(confusion, class_names=None):\n",
    "    \"\"\"視覺化混淆矩陣\"\"\"\n",
    "    n_classes = confusion.shape[0]\n",
    "    \n",
    "    if class_names is None:\n",
    "        class_names = [f'Class {i}' for i in range(n_classes)]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(confusion, cmap='Blues')\n",
    "    plt.colorbar(label='Count')\n",
    "    \n",
    "    # 標籤\n",
    "    plt.xticks(range(n_classes), class_names)\n",
    "    plt.yticks(range(n_classes), class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    # 數值\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            plt.text(j, i, str(confusion[i, j]), \n",
    "                    ha='center', va='center', fontsize=12,\n",
    "                    color='white' if confusion[i, j] > confusion.max() / 2 else 'black')\n",
    "\n",
    "\n",
    "# 計算混淆矩陣\n",
    "y_pred = model.predict(X)\n",
    "confusion = compute_confusion_matrix(y, y_pred, 3)\n",
    "\n",
    "print(\"混淆矩陣:\")\n",
    "print(confusion)\n",
    "\n",
    "plot_confusion_matrix(confusion)\n",
    "plt.show()\n",
    "\n",
    "# 計算每類準確率\n",
    "print(\"\\n每類準確率:\")\n",
    "for i in range(3):\n",
    "    class_acc = confusion[i, i] / np.sum(confusion[i, :])\n",
    "    print(f\"  Class {i}: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的內容\n",
    "\n",
    "1. **Softmax 函數**：\n",
    "   - 將 K 個分數轉為 K 個機率\n",
    "   - 數值穩定：減去最大值\n",
    "\n",
    "2. **Cross-Entropy Loss**：\n",
    "   - $L = -\\log(\\hat{y}_{\\text{true class}})$\n",
    "   - One-hot 編碼\n",
    "\n",
    "3. **梯度**：\n",
    "   - $\\frac{\\partial L}{\\partial z} = \\hat{y} - y$\n",
    "   - 和 Logistic Regression 形式相同\n",
    "\n",
    "4. **決策邊界**：\n",
    "   - 線性邊界\n",
    "   - K 類會有 K 個區域\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. Softmax = Sigmoid 的多分類推廣\n",
    "2. 數值穩定性很重要\n",
    "3. 梯度形式統一簡潔\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個 notebook 中，我們將學習 **SVM（支持向量機）**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
