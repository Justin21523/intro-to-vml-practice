{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.2: Logistic Regression - 邏輯回歸\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "完成這個 notebook 後，你將能夠：\n",
    "\n",
    "1. 理解 Sigmoid 函數的性質與意義\n",
    "2. 理解 Binary Cross-Entropy 損失函數\n",
    "3. 推導並實作 Logistic Regression 的梯度\n",
    "4. 使用數值梯度檢查驗證實作\n",
    "5. 視覺化決策邊界\n",
    "\n",
    "## 背景知識\n",
    "\n",
    "Logistic Regression 是最基礎的**分類**模型。雖然名字有 \"Regression\"，但它用於二分類問題。\n",
    "\n",
    "### 從線性回歸到邏輯回歸\n",
    "\n",
    "線性回歸輸出連續值，但分類問題需要輸出類別標籤。我們需要：\n",
    "\n",
    "1. 將線性組合 $z = w^T x + b$ 映射到 $[0, 1]$ 區間\n",
    "2. 解釋為 \"屬於類別 1 的機率\"\n",
    "\n",
    "**Sigmoid 函數** 完成這個映射。\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- Bishop PRML Ch. 4: Linear Models for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定 matplotlib\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境設定完成！\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Sigmoid 函數\n",
    "\n",
    "### 定義\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "### 性質\n",
    "\n",
    "1. **值域**：$\\sigma(z) \\in (0, 1)$\n",
    "2. **單調遞增**：$z$ 越大，$\\sigma(z)$ 越大\n",
    "3. **對稱性**：$\\sigma(-z) = 1 - \\sigma(z)$\n",
    "4. **導數**：$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "### 數值穩定性\n",
    "\n",
    "直接計算 $e^{-z}$ 在 $z$ 很負時會溢出。使用以下技巧：\n",
    "\n",
    "$$\\sigma(z) = \\begin{cases} \n",
    "\\frac{1}{1 + e^{-z}} & \\text{if } z \\geq 0 \\\\\n",
    "\\frac{e^z}{1 + e^z} & \\text{if } z < 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_naive(z):\n",
    "    \"\"\"\n",
    "    Sigmoid 函數（簡單版，可能有數值問題）\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid 函數（數值穩定版）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray or float\n",
    "        輸入值\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray or float\n",
    "        Sigmoid 輸出，範圍 (0, 1)\n",
    "    \"\"\"\n",
    "    z = np.asarray(z, dtype=np.float64)\n",
    "    result = np.zeros_like(z)\n",
    "    \n",
    "    # z >= 0: 用標準公式\n",
    "    pos_mask = z >= 0\n",
    "    result[pos_mask] = 1 / (1 + np.exp(-z[pos_mask]))\n",
    "    \n",
    "    # z < 0: 用等價形式避免 exp 溢出\n",
    "    neg_mask = z < 0\n",
    "    exp_z = np.exp(z[neg_mask])\n",
    "    result[neg_mask] = exp_z / (1 + exp_z)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# 測試 Sigmoid\n",
    "print(\"=== 測試 Sigmoid 函數 ===\")\n",
    "\n",
    "# 基本測試\n",
    "test_values = [-10, -5, -1, 0, 1, 5, 10]\n",
    "print(f\"{'z':>6} | {'σ(z)':>10}\")\n",
    "print(\"-\" * 20)\n",
    "for z in test_values:\n",
    "    print(f\"{z:>6} | {sigmoid(z):>10.6f}\")\n",
    "\n",
    "# 驗證性質\n",
    "print(\"\\n驗證性質：\")\n",
    "print(f\"σ(0) = {sigmoid(0):.6f} (應該是 0.5)\")\n",
    "print(f\"σ(-5) + σ(5) = {sigmoid(-5) + sigmoid(5):.6f} (應該是 1.0)\")\n",
    "\n",
    "# 導數驗證\n",
    "z = 2.0\n",
    "s = sigmoid(z)\n",
    "dsigmoid_analytic = s * (1 - s)\n",
    "\n",
    "# 數值導數\n",
    "eps = 1e-5\n",
    "dsigmoid_numeric = (sigmoid(z + eps) - sigmoid(z - eps)) / (2 * eps)\n",
    "\n",
    "print(f\"\\n導數驗證 (z={z}):\")\n",
    "print(f\"解析導數: {dsigmoid_analytic:.8f}\")\n",
    "print(f\"數值導數: {dsigmoid_numeric:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Sigmoid\n",
    "z = np.linspace(-10, 10, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sigmoid 函數\n",
    "axes[0].plot(z, sigmoid(z), 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].set_ylabel('σ(z)')\n",
    "axes[0].set_title('Sigmoid 函數')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Sigmoid 導數\n",
    "s = sigmoid(z)\n",
    "ds = s * (1 - s)\n",
    "axes[1].plot(z, ds, 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].set_ylabel(\"σ'(z)\")\n",
    "axes[1].set_title('Sigmoid 導數: σ(z)(1-σ(z))')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：導數在 z=0 時最大 (0.25)，在兩端趨近於 0\")\n",
    "print(\"這意味著梯度在預測很確定時會變小（梯度消失問題）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Binary Cross-Entropy Loss\n",
    "\n",
    "### 為什麼不用 MSE？\n",
    "\n",
    "如果用 MSE：$L = (\\sigma(z) - y)^2$，損失函數會是**非凸**的，有多個局部最小值。\n",
    "\n",
    "### Cross-Entropy 定義\n",
    "\n",
    "對單一樣本：\n",
    "\n",
    "$$L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
    "\n",
    "其中 $\\hat{y} = \\sigma(w^T x + b) \\in (0, 1)$，$y \\in \\{0, 1\\}$\n",
    "\n",
    "### 直覺理解\n",
    "\n",
    "- 當 $y = 1$ 時：$L = -\\log(\\hat{y})$\n",
    "  - $\\hat{y}$ 越接近 1，loss 越小\n",
    "  - $\\hat{y}$ 接近 0 時，$-\\log(\\hat{y}) \\to \\infty$\n",
    "\n",
    "- 當 $y = 0$ 時：$L = -\\log(1 - \\hat{y})$\n",
    "  - $\\hat{y}$ 越接近 0，loss 越小\n",
    "\n",
    "### 批次形式\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"\n",
    "    計算 Binary Cross-Entropy Loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray, shape (N,)\n",
    "        真實標籤，值為 0 或 1\n",
    "    y_pred : np.ndarray, shape (N,)\n",
    "        預測機率，值在 (0, 1)\n",
    "    eps : float\n",
    "        避免 log(0) 的小常數\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        平均 BCE loss\n",
    "    \"\"\"\n",
    "    # Clip 預測值避免 log(0)\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    # BCE\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# 視覺化 BCE\n",
    "print(\"=== 視覺化 Binary Cross-Entropy ===\")\n",
    "\n",
    "y_pred_range = np.linspace(0.001, 0.999, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# y = 1 的情況\n",
    "loss_y1 = -np.log(y_pred_range)\n",
    "axes[0].plot(y_pred_range, loss_y1, 'b-', linewidth=2, label='BCE when y=1')\n",
    "axes[0].set_xlabel('Predicted probability ŷ')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('BCE Loss when y=1: -log(ŷ)')\n",
    "axes[0].set_ylim(0, 5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "# y = 0 的情況\n",
    "loss_y0 = -np.log(1 - y_pred_range)\n",
    "axes[1].plot(y_pred_range, loss_y0, 'r-', linewidth=2, label='BCE when y=0')\n",
    "axes[1].set_xlabel('Predicted probability ŷ')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('BCE Loss when y=0: -log(1-ŷ)')\n",
    "axes[1].set_ylim(0, 5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 測試\n",
    "print(\"\\n測試 BCE：\")\n",
    "print(f\"y=1, ŷ=0.9: Loss = {binary_cross_entropy(np.array([1]), np.array([0.9])):.4f}\")\n",
    "print(f\"y=1, ŷ=0.5: Loss = {binary_cross_entropy(np.array([1]), np.array([0.5])):.4f}\")\n",
    "print(f\"y=1, ŷ=0.1: Loss = {binary_cross_entropy(np.array([1]), np.array([0.1])):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 梯度推導\n",
    "\n",
    "### 目標\n",
    "\n",
    "計算 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "### 推導過程\n",
    "\n",
    "令 $z = w^T x + b$，$\\hat{y} = \\sigma(z)$\n",
    "\n",
    "$$L = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
    "\n",
    "使用鏈式法則：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "**Step 1**: $\\frac{\\partial L}{\\partial \\hat{y}}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}$$\n",
    "\n",
    "**Step 2**: $\\frac{\\partial \\hat{y}}{\\partial z} = \\sigma(z)(1-\\sigma(z)) = \\hat{y}(1-\\hat{y})$\n",
    "\n",
    "**Step 3**: $\\frac{\\partial z}{\\partial w} = x$\n",
    "\n",
    "### 漂亮的結果\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} \\cdot \\hat{y}(1-\\hat{y}) \\cdot x = (\\hat{y} - y) \\cdot x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\hat{y} - y$$\n",
    "\n",
    "**批次形式**：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{N} X^T (\\hat{y} - y)$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_i (\\hat{y}_i - y_i)$$\n",
    "\n",
    "這和線性回歸的梯度形式一樣！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    計算 Logistic Regression 的梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "        特徵矩陣\n",
    "    y : np.ndarray, shape (N,)\n",
    "        真實標籤（0 或 1）\n",
    "    w : np.ndarray, shape (D,)\n",
    "        權重\n",
    "    b : float\n",
    "        偏置\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dw : np.ndarray, shape (D,)\n",
    "        ∂L/∂w\n",
    "    db : float\n",
    "        ∂L/∂b\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # 前向傳播\n",
    "    z = X @ w + b\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # 梯度（漂亮的結果！）\n",
    "    error = y_pred - y  # shape (N,)\n",
    "    \n",
    "    dw = (1 / N) * (X.T @ error)\n",
    "    db = (1 / N) * np.sum(error)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "\n",
    "# 數值梯度檢查\n",
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"計算數值梯度\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_value = x[idx]\n",
    "        \n",
    "        x[idx] = old_value + eps\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        x[idx] = old_value - eps\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        x[idx] = old_value\n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * eps)\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_check_logistic(X, y, w, b, eps=1e-5):\n",
    "    \"\"\"\n",
    "    檢查 Logistic Regression 的梯度\n",
    "    \"\"\"\n",
    "    # 解析梯度\n",
    "    dw_analytic, db_analytic = compute_logistic_gradients(X, y, w, b)\n",
    "    \n",
    "    # 數值梯度 for w\n",
    "    def loss_w(w_test):\n",
    "        z = X @ w_test + b\n",
    "        y_pred = sigmoid(z)\n",
    "        return binary_cross_entropy(y, y_pred)\n",
    "    \n",
    "    dw_numeric = numerical_gradient(loss_w, w.copy(), eps)\n",
    "    \n",
    "    # 數值梯度 for b\n",
    "    def loss_b(b_test):\n",
    "        z = X @ w + b_test[0]\n",
    "        y_pred = sigmoid(z)\n",
    "        return binary_cross_entropy(y, y_pred)\n",
    "    \n",
    "    db_numeric = numerical_gradient(loss_b, np.array([b]), eps)[0]\n",
    "    \n",
    "    # 比較\n",
    "    print(\"=== 梯度檢查 ===\")\n",
    "    print(f\"dw 解析梯度: {dw_analytic}\")\n",
    "    print(f\"dw 數值梯度: {dw_numeric}\")\n",
    "    rel_err_w = np.linalg.norm(dw_analytic - dw_numeric) / (np.linalg.norm(dw_analytic) + np.linalg.norm(dw_numeric) + 1e-8)\n",
    "    print(f\"dw 相對誤差: {rel_err_w:.2e}\")\n",
    "    print()\n",
    "    print(f\"db 解析梯度: {db_analytic:.8f}\")\n",
    "    print(f\"db 數值梯度: {db_numeric:.8f}\")\n",
    "    rel_err_b = abs(db_analytic - db_numeric) / (abs(db_analytic) + abs(db_numeric) + 1e-8)\n",
    "    print(f\"db 相對誤差: {rel_err_b:.2e}\")\n",
    "    \n",
    "    passed = rel_err_w < 1e-4 and rel_err_b < 1e-4\n",
    "    print(f\"\\n梯度檢查通過: {passed}\")\n",
    "    return passed\n",
    "\n",
    "\n",
    "# 生成測試資料\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(50, 3)\n",
    "y_test = (np.random.rand(50) > 0.5).astype(float)\n",
    "w_test = np.random.randn(3)\n",
    "b_test = np.random.randn()\n",
    "\n",
    "gradient_check_logistic(X_test, y_test, w_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 完整的 Logistic Regression 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression 二分類模型\n",
    "    \n",
    "    使用梯度下降優化 Binary Cross-Entropy Loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regularization : float\n",
    "        L2 正則化強度\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w : np.ndarray\n",
    "        權重向量\n",
    "    b : float\n",
    "        偏置\n",
    "    history : dict\n",
    "        訓練歷史\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, regularization=0.0):\n",
    "        self.reg = regularization\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.history = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"數值穩定的 Sigmoid\"\"\"\n",
    "        return sigmoid(z)\n",
    "    \n",
    "    def fit(self, X, y, lr=0.1, n_iter=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        使用梯度下降訓練模型\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "            特徵矩陣\n",
    "        y : np.ndarray, shape (N,)\n",
    "            標籤（0 或 1）\n",
    "        lr : float\n",
    "            學習率\n",
    "        n_iter : int\n",
    "            迭代次數\n",
    "        verbose : bool\n",
    "            是否輸出訓練過程\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # 初始化參數\n",
    "        self.w = np.zeros(D)\n",
    "        self.b = 0.0\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # 前向傳播\n",
    "            z = X @ self.w + self.b\n",
    "            y_pred = self._sigmoid(z)\n",
    "            \n",
    "            # 計算 loss（含正則化）\n",
    "            loss = binary_cross_entropy(y, y_pred) + 0.5 * self.reg * np.sum(self.w ** 2)\n",
    "            \n",
    "            # 計算準確率\n",
    "            predictions = (y_pred >= 0.5).astype(float)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            # 記錄\n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # 計算梯度\n",
    "            error = y_pred - y\n",
    "            dw = (1 / N) * (X.T @ error) + self.reg * self.w\n",
    "            db = (1 / N) * np.sum(error)\n",
    "            \n",
    "            # 更新參數\n",
    "            self.w = self.w - lr * dw\n",
    "            self.b = self.b - lr * db\n",
    "            \n",
    "            if verbose and (i + 1) % (n_iter // 10) == 0:\n",
    "                print(f\"Iter {i+1:4d}: Loss = {loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        預測屬於類別 1 的機率\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray, shape (N,)\n",
    "            P(y=1|x)\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        z = X @ self.w + self.b\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        預測類別標籤\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray, shape (N,)\n",
    "            預測標籤（0 或 1）\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        計算準確率\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "\n",
    "# 生成二分類資料\n",
    "def generate_binary_classification_data(n_samples=200, seed=42):\n",
    "    \"\"\"\n",
    "    生成二分類測試資料\n",
    "    \n",
    "    兩個高斯分布的混合\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # 類別 0\n",
    "    X0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-1, -1])\n",
    "    y0 = np.zeros(n_per_class)\n",
    "    \n",
    "    # 類別 1\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([1, 1])\n",
    "    y1 = np.ones(n_per_class)\n",
    "    \n",
    "    # 合併\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.concatenate([y0, y1])\n",
    "    \n",
    "    # 打亂\n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 Logistic Regression ===\")\n",
    "\n",
    "X, y = generate_binary_classification_data(n_samples=200)\n",
    "\n",
    "print(f\"資料大小: {X.shape}\")\n",
    "print(f\"類別分布: {np.bincount(y.astype(int))}\")\n",
    "\n",
    "# 訓練模型\n",
    "model = LogisticRegression(regularization=0.01)\n",
    "model.fit(X, y, lr=0.5, n_iter=500, verbose=True)\n",
    "\n",
    "print(f\"\\n最終準確率: {model.score(X, y):.4f}\")\n",
    "print(f\"學習到的參數: w = {model.w}, b = {model.b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化結果\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    繪製決策邊界\n",
    "    \"\"\"\n",
    "    # 建立網格\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # 預測網格上每個點\n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # 機率等高線\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap='RdYlBu_r', alpha=0.6)\n",
    "    plt.colorbar(label='P(y=1|x)')\n",
    "    \n",
    "    # 決策邊界（P=0.5）\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # 資料點\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', marker='o', s=50, label='Class 0', edgecolors='k')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s', s=50, label='Class 1', edgecolors='k')\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "# 決策邊界\n",
    "plot_decision_boundary(model, X, y, title='Logistic Regression 決策邊界')\n",
    "plt.show()\n",
    "\n",
    "# 訓練曲線\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(model.history['loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Loss (BCE + L2)')\n",
    "axes[0].set_title('訓練 Loss 曲線')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(model.history['accuracy'])\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('訓練準確率曲線')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: 決策邊界的幾何意義\n",
    "\n",
    "### 決策邊界方程\n",
    "\n",
    "決策邊界是 $P(y=1|x) = 0.5$ 的點集，即：\n",
    "\n",
    "$$\\sigma(w^T x + b) = 0.5 \\Rightarrow w^T x + b = 0$$\n",
    "\n",
    "這是一個**超平面**（2D 中是直線）。\n",
    "\n",
    "### 法向量\n",
    "\n",
    "$w$ 是超平面的法向量，指向類別 1 的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化決策邊界的幾何\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 資料點\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', marker='o', s=50, label='Class 0', edgecolors='k')\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s', s=50, label='Class 1', edgecolors='k')\n",
    "\n",
    "# 決策邊界直線\n",
    "# w1*x1 + w2*x2 + b = 0 => x2 = -(w1*x1 + b) / w2\n",
    "w1, w2 = model.w\n",
    "b = model.b\n",
    "\n",
    "x1_range = np.linspace(-4, 4, 100)\n",
    "x2_boundary = -(w1 * x1_range + b) / w2\n",
    "\n",
    "plt.plot(x1_range, x2_boundary, 'k-', linewidth=2, label='Decision boundary')\n",
    "\n",
    "# 法向量 w（從原點指向）\n",
    "origin = np.array([0, 0])\n",
    "w_normalized = model.w / np.linalg.norm(model.w) * 2  # 縮放顯示\n",
    "plt.arrow(origin[0], origin[1], w_normalized[0], w_normalized[1], \n",
    "          head_width=0.15, head_length=0.1, fc='green', ec='green', linewidth=2)\n",
    "plt.text(w_normalized[0] + 0.2, w_normalized[1] + 0.2, 'w (normal)', fontsize=12, color='green')\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('決策邊界的幾何意義')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n決策邊界方程: {w1:.4f}*x1 + {w2:.4f}*x2 + {b:.4f} = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題\n",
    "\n",
    "### 練習 1：非線性可分資料\n",
    "\n",
    "**任務**：在非線性可分資料上測試 Logistic Regression\n",
    "\n",
    "**觀察**：線性模型的局限性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1 解答\n",
    "\n",
    "def generate_xor_data(n_samples=200, noise=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    生成 XOR 模式的資料（非線性可分）\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_per_class = n_samples // 4\n",
    "    \n",
    "    # 四個象限\n",
    "    X1 = np.random.randn(n_per_class, 2) * 0.5 + np.array([1, 1])\n",
    "    X2 = np.random.randn(n_per_class, 2) * 0.5 + np.array([-1, -1])\n",
    "    X3 = np.random.randn(n_per_class, 2) * 0.5 + np.array([1, -1])\n",
    "    X4 = np.random.randn(n_per_class, 2) * 0.5 + np.array([-1, 1])\n",
    "    \n",
    "    X = np.vstack([X1, X2, X3, X4])\n",
    "    y = np.array([1] * n_per_class + [1] * n_per_class + \n",
    "                 [0] * n_per_class + [0] * n_per_class)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx].astype(float)\n",
    "\n",
    "\n",
    "# 生成 XOR 資料\n",
    "X_xor, y_xor = generate_xor_data(n_samples=200)\n",
    "\n",
    "# 訓練線性 Logistic Regression\n",
    "model_linear = LogisticRegression(regularization=0.01)\n",
    "model_linear.fit(X_xor, y_xor, lr=0.5, n_iter=500, verbose=False)\n",
    "\n",
    "print(f\"線性模型在 XOR 資料上的準確率: {model_linear.score(X_xor, y_xor):.4f}\")\n",
    "print(\"(接近 0.5 表示模型無法學習)\")\n",
    "\n",
    "# 視覺化\n",
    "plot_decision_boundary(model_linear, X_xor, y_xor, \n",
    "                       title='XOR 資料 - 線性 Logistic Regression（失敗）')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：特徵工程解決非線性問題\n",
    "\n",
    "**任務**：通過添加非線性特徵來解決 XOR 問題\n",
    "\n",
    "**提示**：加入 $x_1 \\cdot x_2$ 特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2 解答\n",
    "\n",
    "def add_interaction_feature(X):\n",
    "    \"\"\"\n",
    "    添加交互特徵 x1 * x2\n",
    "    \"\"\"\n",
    "    x1_x2 = (X[:, 0] * X[:, 1]).reshape(-1, 1)\n",
    "    return np.hstack([X, x1_x2])\n",
    "\n",
    "\n",
    "# 添加交互特徵\n",
    "X_xor_extended = add_interaction_feature(X_xor)\n",
    "print(f\"原始特徵維度: {X_xor.shape[1]}\")\n",
    "print(f\"擴展後特徵維度: {X_xor_extended.shape[1]}\")\n",
    "\n",
    "# 重新訓練\n",
    "model_extended = LogisticRegression(regularization=0.01)\n",
    "model_extended.fit(X_xor_extended, y_xor, lr=0.5, n_iter=500, verbose=False)\n",
    "\n",
    "print(f\"\\n加入交互特徵後的準確率: {model_extended.score(X_xor_extended, y_xor):.4f}\")\n",
    "\n",
    "# 視覺化決策邊界\n",
    "# 這需要在原始 2D 空間中繪製\n",
    "x_min, x_max = X_xor[:, 0].min() - 1, X_xor[:, 0].max() + 1\n",
    "y_min, y_max = X_xor[:, 1].min() - 1, X_xor[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# 加入交互特徵後預測\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_extended = add_interaction_feature(grid_points)\n",
    "Z = model_extended.predict_proba(grid_extended)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(0, 1, 11), cmap='RdYlBu_r', alpha=0.6)\n",
    "plt.colorbar(label='P(y=1|x)')\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "plt.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', marker='o', s=50, label='Class 0', edgecolors='k')\n",
    "plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', marker='s', s=50, label='Class 1', edgecolors='k')\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('XOR 資料 - 加入 $x_1 \\cdot x_2$ 特徵後的決策邊界')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n學習到的參數: w = {model_extended.w}, b = {model_extended.b:.4f}\")\n",
    "print(\"決策邊界: w1*x1 + w2*x2 + w3*(x1*x2) + b = 0\")\n",
    "print(\"這是一個雙曲線！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：多項式特徵\n",
    "\n",
    "**任務**：用多項式特徵處理圓形分佈的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3 解答\n",
    "\n",
    "def generate_circle_data(n_samples=200, seed=42):\n",
    "    \"\"\"\n",
    "    生成圓形分佈資料\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # 內圈（類別 0）\n",
    "    r_inner = np.random.uniform(0, 1, n_per_class)\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    X_inner = np.column_stack([r_inner * np.cos(theta_inner), \n",
    "                               r_inner * np.sin(theta_inner)])\n",
    "    \n",
    "    # 外圈（類別 1）\n",
    "    r_outer = np.random.uniform(2, 3, n_per_class)\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_per_class)\n",
    "    X_outer = np.column_stack([r_outer * np.cos(theta_outer), \n",
    "                               r_outer * np.sin(theta_outer)])\n",
    "    \n",
    "    X = np.vstack([X_inner, X_outer])\n",
    "    y = np.array([0] * n_per_class + [1] * n_per_class).astype(float)\n",
    "    \n",
    "    idx = np.random.permutation(n_samples)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "def polynomial_features_2d(X, degree=2):\n",
    "    \"\"\"\n",
    "    生成 2D 多項式特徵\n",
    "    \n",
    "    degree=2: [1, x1, x2, x1^2, x1*x2, x2^2]\n",
    "    \"\"\"\n",
    "    x1, x2 = X[:, 0], X[:, 1]\n",
    "    features = [np.ones(len(x1))]  # bias（會被模型加，這裡先不加）\n",
    "    \n",
    "    for d in range(1, degree + 1):\n",
    "        for i in range(d + 1):\n",
    "            features.append((x1 ** (d - i)) * (x2 ** i))\n",
    "    \n",
    "    return np.column_stack(features[1:])  # 去掉常數項\n",
    "\n",
    "\n",
    "# 生成圓形資料\n",
    "X_circle, y_circle = generate_circle_data(n_samples=200)\n",
    "\n",
    "# 線性模型\n",
    "model_linear = LogisticRegression(regularization=0.01)\n",
    "model_linear.fit(X_circle, y_circle, lr=0.5, n_iter=500, verbose=False)\n",
    "print(f\"線性模型準確率: {model_linear.score(X_circle, y_circle):.4f}\")\n",
    "\n",
    "# 多項式特徵模型\n",
    "X_circle_poly = polynomial_features_2d(X_circle, degree=2)\n",
    "print(f\"多項式特徵維度: {X_circle_poly.shape[1]}\")\n",
    "\n",
    "model_poly = LogisticRegression(regularization=0.01)\n",
    "model_poly.fit(X_circle_poly, y_circle, lr=0.1, n_iter=1000, verbose=False)\n",
    "print(f\"多項式模型準確率: {model_poly.score(X_circle_poly, y_circle):.4f}\")\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 線性模型\n",
    "x_min, x_max = X_circle[:, 0].min() - 0.5, X_circle[:, 0].max() + 0.5\n",
    "y_min, y_max = X_circle[:, 1].min() - 0.5, X_circle[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "Z_linear = model_linear.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z_linear = Z_linear.reshape(xx.shape)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z_linear, levels=np.linspace(0, 1, 11), cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[0].contour(xx, yy, Z_linear, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[0].scatter(X_circle[y_circle==0, 0], X_circle[y_circle==0, 1], c='blue', s=30, label='Class 0')\n",
    "axes[0].scatter(X_circle[y_circle==1, 0], X_circle[y_circle==1, 1], c='red', s=30, label='Class 1')\n",
    "axes[0].set_title(f'線性模型 (Acc: {model_linear.score(X_circle, y_circle):.2f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# 多項式模型\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid_poly = polynomial_features_2d(grid_points, degree=2)\n",
    "Z_poly = model_poly.predict_proba(grid_poly)\n",
    "Z_poly = Z_poly.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z_poly, levels=np.linspace(0, 1, 11), cmap='RdYlBu_r', alpha=0.6)\n",
    "axes[1].contour(xx, yy, Z_poly, levels=[0.5], colors='black', linewidths=2)\n",
    "axes[1].scatter(X_circle[y_circle==0, 0], X_circle[y_circle==0, 1], c='blue', s=30, label='Class 0')\n",
    "axes[1].scatter(X_circle[y_circle==1, 0], X_circle[y_circle==1, 1], c='red', s=30, label='Class 1')\n",
    "axes[1].set_title(f'二次多項式模型 (Acc: {model_poly.score(X_circle_poly, y_circle):.2f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：加入 x1², x1*x2, x2² 特徵後，可以學習圓形邊界\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的內容\n",
    "\n",
    "1. **Sigmoid 函數**：\n",
    "   - $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "   - 將任意值映射到 $(0, 1)$\n",
    "   - 數值穩定實作\n",
    "\n",
    "2. **Binary Cross-Entropy**：\n",
    "   - $L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$\n",
    "   - 比 MSE 更適合分類\n",
    "\n",
    "3. **梯度推導**：\n",
    "   - $\\frac{\\partial L}{\\partial w} = \\frac{1}{N} X^T (\\hat{y} - y)$\n",
    "   - 形式和線性回歸一樣漂亮！\n",
    "\n",
    "4. **決策邊界**：\n",
    "   - $w^T x + b = 0$ 定義超平面\n",
    "   - 線性模型只能學習線性邊界\n",
    "\n",
    "5. **特徵工程**：\n",
    "   - 添加非線性特徵可以處理非線性可分資料\n",
    "   - 這是「核方法」的前身\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. **梯度檢查**：永遠驗證你的梯度實作\n",
    "2. **數值穩定性**：處理 exp 和 log 時要小心\n",
    "3. **線性模型的局限**：無法處理非線性可分資料\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個 notebook 中，我們將學習 **Softmax Regression**，將二分類推廣到多分類問題。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
