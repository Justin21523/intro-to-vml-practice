{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.1: Linear Regression - 線性回歸\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "完成這個 notebook 後，你將能夠：\n",
    "\n",
    "1. 理解線性回歸的數學原理\n",
    "2. 使用閉式解（Normal Equation）求解線性回歸\n",
    "3. 使用梯度下降法求解線性回歸\n",
    "4. 實作 L2 正則化（Ridge Regression）\n",
    "5. 使用數值梯度檢查驗證你的梯度推導\n",
    "\n",
    "## 背景知識\n",
    "\n",
    "線性回歸是最基礎的監督式學習模型，用於預測連續值。雖然簡單，但理解它的數學推導對後續學習非常重要。\n",
    "\n",
    "### 問題設定\n",
    "\n",
    "給定訓練資料 $\\{(x_i, y_i)\\}_{i=1}^N$，其中：\n",
    "- $x_i \\in \\mathbb{R}^D$ 是特徵向量\n",
    "- $y_i \\in \\mathbb{R}$ 是目標值\n",
    "\n",
    "我們假設 $y$ 與 $x$ 有線性關係：\n",
    "\n",
    "$$\\hat{y} = w^T x + b = \\sum_{j=1}^D w_j x_j + b$$\n",
    "\n",
    "目標是找到最佳的 $w$ 和 $b$ 使預測誤差最小。\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- Bishop PRML Ch. 3: Linear Models for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定 matplotlib\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# 設定隨機種子\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境設定完成！\")\n",
    "print(f\"NumPy 版本: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: 損失函數 - Mean Squared Error (MSE)\n",
    "\n",
    "### MSE 定義\n",
    "\n",
    "均方誤差（Mean Squared Error）是最常用的回歸損失函數：\n",
    "\n",
    "$$L(w, b) = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 = \\frac{1}{N} \\sum_{i=1}^N (w^T x_i + b - y_i)^2$$\n",
    "\n",
    "### 為什麼用 MSE？\n",
    "\n",
    "1. **統計意義**：假設雜訊服從高斯分布時，最大化似然等價於最小化 MSE\n",
    "2. **凸函數**：MSE 是凸函數，有唯一全域最小值\n",
    "3. **可微分**：處處可微，方便優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    計算 Mean Squared Error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : np.ndarray, shape (N,)\n",
    "        預測值\n",
    "    y_true : np.ndarray, shape (N,)\n",
    "        真實值\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        MSE 損失值\n",
    "    \"\"\"\n",
    "    N = len(y_true)\n",
    "    return np.sum((y_pred - y_true) ** 2) / N\n",
    "\n",
    "\n",
    "# 建立簡單測試資料\n",
    "def generate_linear_data(n_samples=100, n_features=1, noise_std=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    生成線性回歸測試資料\n",
    "    \n",
    "    y = w^T x + b + noise\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 真實參數\n",
    "    w_true = np.random.randn(n_features) * 2\n",
    "    b_true = np.random.randn() * 2\n",
    "    \n",
    "    # 生成資料\n",
    "    X = np.random.randn(n_samples, n_features) * 3\n",
    "    y = X @ w_true + b_true + np.random.randn(n_samples) * noise_std\n",
    "    \n",
    "    return X, y, w_true, b_true\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 MSE Loss ===\")\n",
    "\n",
    "X, y, w_true, b_true = generate_linear_data(n_samples=100, n_features=1)\n",
    "\n",
    "print(f\"真實參數: w = {w_true}, b = {b_true:.4f}\")\n",
    "\n",
    "# 用真實參數預測\n",
    "y_pred_true = X @ w_true + b_true\n",
    "print(f\"用真實參數的 MSE: {mse_loss(y_pred_true, y):.4f}\")\n",
    "\n",
    "# 用隨機參數預測\n",
    "y_pred_random = X @ np.random.randn(1) + np.random.randn()\n",
    "print(f\"用隨機參數的 MSE: {mse_loss(y_pred_random, y):.4f}\")\n",
    "\n",
    "# 視覺化（1D 情況）\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, label='Data')\n",
    "plt.plot(X, y_pred_true, 'r-', linewidth=2, label='True model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('線性回歸測試資料')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: 閉式解（Normal Equation）\n",
    "\n",
    "### 矩陣形式\n",
    "\n",
    "為了方便推導，我們把 bias 併入 weight 中。令 $\\tilde{X}$ 是在 $X$ 前面加一行 1：\n",
    "\n",
    "$$\\tilde{X} = \\begin{bmatrix} 1 & x_1^T \\\\ 1 & x_2^T \\\\ \\vdots \\\\ 1 & x_N^T \\end{bmatrix} \\in \\mathbb{R}^{N \\times (D+1)}, \\quad \\tilde{w} = \\begin{bmatrix} b \\\\ w \\end{bmatrix} \\in \\mathbb{R}^{D+1}$$\n",
    "\n",
    "則：\n",
    "$$\\hat{y} = \\tilde{X} \\tilde{w}$$\n",
    "\n",
    "$$L(\\tilde{w}) = \\frac{1}{N} \\|\\tilde{X}\\tilde{w} - y\\|^2$$\n",
    "\n",
    "### 求解\n",
    "\n",
    "對 $\\tilde{w}$ 微分並設為 0：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\tilde{w}} = \\frac{2}{N} \\tilde{X}^T (\\tilde{X}\\tilde{w} - y) = 0$$\n",
    "\n",
    "$$\\tilde{X}^T \\tilde{X} \\tilde{w} = \\tilde{X}^T y$$\n",
    "\n",
    "$$\\tilde{w}^* = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T y$$\n",
    "\n",
    "這就是 **Normal Equation**（正規方程）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_closed_form(X, y):\n",
    "    \"\"\"\n",
    "    使用閉式解（Normal Equation）求解線性回歸\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "        特徵矩陣\n",
    "    y : np.ndarray, shape (N,)\n",
    "        目標值\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : np.ndarray, shape (D,)\n",
    "        權重\n",
    "    b : float\n",
    "        偏置\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # Step 1: 在 X 前面加一行 1（用於 bias）\n",
    "    # X_tilde shape: (N, D+1)\n",
    "    X_tilde = np.hstack([np.ones((N, 1)), X])\n",
    "    \n",
    "    # Step 2: 計算 (X^T X)^{-1} X^T y\n",
    "    # 使用 np.linalg.solve 比直接求逆更數值穩定\n",
    "    # X^T X * w = X^T y\n",
    "    XTX = X_tilde.T @ X_tilde\n",
    "    XTy = X_tilde.T @ y\n",
    "    \n",
    "    # 解線性系統（比 np.linalg.inv 更穩定）\n",
    "    w_tilde = np.linalg.solve(XTX, XTy)\n",
    "    \n",
    "    # Step 3: 分離 b 和 w\n",
    "    b = w_tilde[0]\n",
    "    w = w_tilde[1:]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "\n",
    "# 測試閉式解\n",
    "print(\"=== 測試閉式解 ===\")\n",
    "\n",
    "X, y, w_true, b_true = generate_linear_data(n_samples=100, n_features=3, noise_std=0.5)\n",
    "\n",
    "print(f\"真實參數:\")\n",
    "print(f\"  w = {w_true}\")\n",
    "print(f\"  b = {b_true:.4f}\")\n",
    "\n",
    "w_pred, b_pred = linear_regression_closed_form(X, y)\n",
    "\n",
    "print(f\"\\n閉式解:\")\n",
    "print(f\"  w = {w_pred}\")\n",
    "print(f\"  b = {b_pred:.4f}\")\n",
    "\n",
    "# 計算預測誤差\n",
    "y_pred = X @ w_pred + b_pred\n",
    "print(f\"\\n訓練 MSE: {mse_loss(y_pred, y):.6f}\")\n",
    "\n",
    "# 參數誤差\n",
    "print(f\"\\n參數誤差:\")\n",
    "print(f\"  |w - w_true| = {np.linalg.norm(w_pred - w_true):.6f}\")\n",
    "print(f\"  |b - b_true| = {abs(b_pred - b_true):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閉式解什麼時候會失敗？\n",
    "\n",
    "$X^T X$ 不可逆的情況：\n",
    "\n",
    "1. **N < D**：樣本數小於特徵數（欠定系統）\n",
    "2. **共線性**：特徵之間有線性相關（例如 $x_2 = 2x_1$）\n",
    "\n",
    "解決方法：**正則化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範 X^T X 不可逆的情況\n",
    "print(\"=== 示範不可逆情況 ===\")\n",
    "\n",
    "# Case 1: N < D\n",
    "print(\"\\nCase 1: 樣本數 < 特徵數\")\n",
    "X_bad1 = np.random.randn(5, 10)  # 5 samples, 10 features\n",
    "y_bad1 = np.random.randn(5)\n",
    "\n",
    "try:\n",
    "    w, b = linear_regression_closed_form(X_bad1, y_bad1)\n",
    "    print(f\"  成功，但結果可能不穩定\")\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"  失敗: {e}\")\n",
    "\n",
    "# 檢查條件數\n",
    "X_tilde = np.hstack([np.ones((5, 1)), X_bad1])\n",
    "XTX = X_tilde.T @ X_tilde\n",
    "cond_num = np.linalg.cond(XTX)\n",
    "print(f\"  X^T X 的條件數: {cond_num:.2e} (太大表示接近奇異)\")\n",
    "\n",
    "# Case 2: 共線性\n",
    "print(\"\\nCase 2: 共線性特徵\")\n",
    "X_base = np.random.randn(100, 2)\n",
    "X_bad2 = np.hstack([X_base, X_base[:, 0:1] * 2])  # 第三個特徵是第一個的 2 倍\n",
    "y_bad2 = np.random.randn(100)\n",
    "\n",
    "X_tilde2 = np.hstack([np.ones((100, 1)), X_bad2])\n",
    "XTX2 = X_tilde2.T @ X_tilde2\n",
    "print(f\"  X^T X 的行列式: {np.linalg.det(XTX2):.2e}\")\n",
    "print(f\"  X^T X 的條件數: {np.linalg.cond(XTX2):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: 梯度下降法\n",
    "\n",
    "### MSE 的梯度推導\n",
    "\n",
    "$$L(w, b) = \\frac{1}{N} \\sum_{i=1}^N (w^T x_i + b - y_i)^2$$\n",
    "\n",
    "令 $e_i = w^T x_i + b - y_i$（殘差），則：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{2}{N} \\sum_{i=1}^N e_i \\cdot x_i = \\frac{2}{N} X^T (Xw + b\\mathbf{1} - y)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^N e_i = \\frac{2}{N} \\mathbf{1}^T (Xw + b\\mathbf{1} - y)$$\n",
    "\n",
    "其中 $\\mathbf{1}$ 是全 1 向量。\n",
    "\n",
    "### 梯度下降更新規則\n",
    "\n",
    "$$w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}$$\n",
    "$$b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    計算 MSE 對 w 和 b 的梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "    w : np.ndarray, shape (D,)\n",
    "    b : float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dw : np.ndarray, shape (D,)\n",
    "        ∂L/∂w\n",
    "    db : float\n",
    "        ∂L/∂b\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # 計算殘差 e = y_pred - y\n",
    "    y_pred = X @ w + b\n",
    "    error = y_pred - y  # shape (N,)\n",
    "    \n",
    "    # 梯度\n",
    "    dw = (2 / N) * (X.T @ error)\n",
    "    db = (2 / N) * np.sum(error)\n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "\n",
    "def linear_regression_gradient_descent(X, y, lr=0.01, n_iter=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    使用梯度下降求解線性回歸\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "    lr : float\n",
    "        學習率\n",
    "    n_iter : int\n",
    "        迭代次數\n",
    "    verbose : bool\n",
    "        是否輸出訓練過程\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : np.ndarray, shape (D,)\n",
    "    b : float\n",
    "    history : dict\n",
    "        訓練歷史（loss, w, b）\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # 初始化參數\n",
    "    w = np.zeros(D)\n",
    "    b = 0.0\n",
    "    \n",
    "    # 記錄歷史\n",
    "    history = {'loss': [], 'w': [], 'b': []}\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # 計算梯度\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "        \n",
    "        # 更新參數\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "        # 計算 loss\n",
    "        y_pred = X @ w + b\n",
    "        loss = mse_loss(y_pred, y)\n",
    "        \n",
    "        # 記錄\n",
    "        history['loss'].append(loss)\n",
    "        history['w'].append(w.copy())\n",
    "        history['b'].append(b)\n",
    "        \n",
    "        if verbose and (i + 1) % (n_iter // 10) == 0:\n",
    "            print(f\"Iter {i+1:4d}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# 測試梯度下降\n",
    "print(\"=== 測試梯度下降 ===\")\n",
    "\n",
    "X, y, w_true, b_true = generate_linear_data(n_samples=100, n_features=3, noise_std=0.5)\n",
    "\n",
    "print(f\"真實參數: w = {w_true}, b = {b_true:.4f}\\n\")\n",
    "\n",
    "w_gd, b_gd, history = linear_regression_gradient_descent(X, y, lr=0.01, n_iter=1000)\n",
    "\n",
    "print(f\"\\n梯度下降解:\")\n",
    "print(f\"  w = {w_gd}\")\n",
    "print(f\"  b = {b_gd:.4f}\")\n",
    "\n",
    "# 與閉式解比較\n",
    "w_closed, b_closed = linear_regression_closed_form(X, y)\n",
    "print(f\"\\n閉式解:\")\n",
    "print(f\"  w = {w_closed}\")\n",
    "print(f\"  b = {b_closed:.4f}\")\n",
    "\n",
    "print(f\"\\n梯度下降與閉式解的差異:\")\n",
    "print(f\"  |w_gd - w_closed| = {np.linalg.norm(w_gd - w_closed):.8f}\")\n",
    "print(f\"  |b_gd - b_closed| = {abs(b_gd - b_closed):.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss 曲線\n",
    "axes[0].plot(history['loss'])\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('訓練 Loss 曲線')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# 參數收斂\n",
    "w_history = np.array(history['w'])\n",
    "for j in range(w_history.shape[1]):\n",
    "    axes[1].plot(w_history[:, j], label=f'w[{j}]')\n",
    "axes[1].axhline(y=w_true[0], color='r', linestyle='--', alpha=0.5, label='True w[0]')\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Parameter Value')\n",
    "axes[1].set_title('參數收斂過程')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: 數值梯度檢查\n",
    "\n",
    "確認我們推導的解析梯度是正確的。使用有限差分近似：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\theta_j} \\approx \\frac{L(\\theta + \\epsilon e_j) - L(\\theta - \\epsilon e_j)}{2\\epsilon}$$\n",
    "\n",
    "其中 $e_j$ 是第 $j$ 個基向量，$\\epsilon$ 是小量（如 $10^{-5}$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    計算數值梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        損失函數 f(x) -> scalar\n",
    "    x : np.ndarray\n",
    "        參數\n",
    "    eps : float\n",
    "        有限差分的步長\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : np.ndarray\n",
    "        數值梯度，與 x 形狀相同\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    # 對每個維度計算數值梯度\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_value = x[idx]\n",
    "        \n",
    "        # f(x + eps)\n",
    "        x[idx] = old_value + eps\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        # f(x - eps)\n",
    "        x[idx] = old_value - eps\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        # 還原\n",
    "        x[idx] = old_value\n",
    "        \n",
    "        # 中心差分\n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * eps)\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_check(X, y, w, b, eps=1e-5):\n",
    "    \"\"\"\n",
    "    檢查解析梯度與數值梯度是否一致\n",
    "    \"\"\"\n",
    "    # 解析梯度\n",
    "    dw_analytic, db_analytic = compute_gradients(X, y, w, b)\n",
    "    \n",
    "    # 數值梯度 for w\n",
    "    def loss_w(w_test):\n",
    "        y_pred = X @ w_test + b\n",
    "        return mse_loss(y_pred, y)\n",
    "    \n",
    "    dw_numeric = numerical_gradient(loss_w, w.copy(), eps)\n",
    "    \n",
    "    # 數值梯度 for b\n",
    "    def loss_b(b_test):\n",
    "        y_pred = X @ w + b_test[0]\n",
    "        return mse_loss(y_pred, y)\n",
    "    \n",
    "    db_numeric = numerical_gradient(loss_b, np.array([b]), eps)[0]\n",
    "    \n",
    "    # 比較\n",
    "    print(\"=== 梯度檢查 ===\")\n",
    "    print(f\"dw 解析梯度: {dw_analytic}\")\n",
    "    print(f\"dw 數值梯度: {dw_numeric}\")\n",
    "    print(f\"dw 相對誤差: {np.linalg.norm(dw_analytic - dw_numeric) / (np.linalg.norm(dw_analytic) + np.linalg.norm(dw_numeric) + 1e-8):.2e}\")\n",
    "    print()\n",
    "    print(f\"db 解析梯度: {db_analytic:.6f}\")\n",
    "    print(f\"db 數值梯度: {db_numeric:.6f}\")\n",
    "    print(f\"db 相對誤差: {abs(db_analytic - db_numeric) / (abs(db_analytic) + abs(db_numeric) + 1e-8):.2e}\")\n",
    "    \n",
    "    # 判定\n",
    "    w_ok = np.allclose(dw_analytic, dw_numeric, rtol=1e-4, atol=1e-6)\n",
    "    b_ok = np.allclose(db_analytic, db_numeric, rtol=1e-4, atol=1e-6)\n",
    "    \n",
    "    print(f\"\\n梯度檢查通過: {w_ok and b_ok}\")\n",
    "    return w_ok and b_ok\n",
    "\n",
    "\n",
    "# 執行梯度檢查\n",
    "X, y, _, _ = generate_linear_data(n_samples=50, n_features=3)\n",
    "w_test = np.random.randn(3)\n",
    "b_test = np.random.randn()\n",
    "\n",
    "gradient_check(X, y, w_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: 正則化（Ridge Regression）\n",
    "\n",
    "### L2 正則化\n",
    "\n",
    "在損失函數中加入權重的 L2 範數：\n",
    "\n",
    "$$L_{\\text{ridge}}(w, b) = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2 + \\lambda \\|w\\|^2$$\n",
    "\n",
    "其中 $\\lambda > 0$ 是正則化強度。\n",
    "\n",
    "### 為什麼需要正則化？\n",
    "\n",
    "1. **防止過擬合**：限制權重大小，減少模型複雜度\n",
    "2. **解決不可逆問題**：$X^T X + \\lambda I$ 一定可逆（$\\lambda > 0$ 時）\n",
    "\n",
    "### 閉式解\n",
    "\n",
    "$$\\tilde{w}^* = (\\tilde{X}^T \\tilde{X} + \\lambda I')^{-1} \\tilde{X}^T y$$\n",
    "\n",
    "注意：$I'$ 的第一個對角元素是 0（不對 bias 正則化）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_closed_form(X, y, reg=0.0):\n",
    "    \"\"\"\n",
    "    使用閉式解求解 Ridge 回歸\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "    reg : float\n",
    "        正則化強度 λ\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : np.ndarray, shape (D,)\n",
    "    b : float\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # 加入 bias 項\n",
    "    X_tilde = np.hstack([np.ones((N, 1)), X])\n",
    "    \n",
    "    # 建立正則化矩陣\n",
    "    # 不對 bias 正則化，所以第一個對角元素是 0\n",
    "    reg_matrix = reg * np.eye(D + 1)\n",
    "    reg_matrix[0, 0] = 0  # 不正則化 bias\n",
    "    \n",
    "    # 閉式解\n",
    "    XTX = X_tilde.T @ X_tilde + reg_matrix\n",
    "    XTy = X_tilde.T @ y\n",
    "    \n",
    "    w_tilde = np.linalg.solve(XTX, XTy)\n",
    "    \n",
    "    b = w_tilde[0]\n",
    "    w = w_tilde[1:]\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "\n",
    "def ridge_regression_gradient_descent(X, y, reg=0.0, lr=0.01, n_iter=1000):\n",
    "    \"\"\"\n",
    "    使用梯度下降求解 Ridge 回歸\n",
    "    \n",
    "    梯度：\n",
    "    ∂L/∂w = (2/N) X^T (Xw + b - y) + 2λw\n",
    "    ∂L/∂b = (2/N) 1^T (Xw + b - y)  (bias 不正則化)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    w = np.zeros(D)\n",
    "    b = 0.0\n",
    "    \n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # 預測\n",
    "        y_pred = X @ w + b\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # 梯度\n",
    "        dw = (2 / N) * (X.T @ error) + 2 * reg * w\n",
    "        db = (2 / N) * np.sum(error)\n",
    "        \n",
    "        # 更新\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "        # Loss（含正則化項）\n",
    "        loss = mse_loss(y_pred, y) + reg * np.sum(w ** 2)\n",
    "        history['loss'].append(loss)\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# 測試正則化\n",
    "print(\"=== 測試 Ridge Regression ===\")\n",
    "\n",
    "# 建立有共線性的資料\n",
    "np.random.seed(42)\n",
    "X_base = np.random.randn(100, 5)\n",
    "X_collinear = np.hstack([X_base, X_base[:, 0:1] * 2 + 0.01 * np.random.randn(100, 1)])  # 近似共線\n",
    "y_collinear = X_base @ np.array([1, 2, 3, 4, 5]) + np.random.randn(100) * 0.5\n",
    "\n",
    "print(\"資料有近似共線性...\")\n",
    "\n",
    "# 不同正則化強度\n",
    "regs = [0, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "print(f\"\\n{'λ':>10} {'||w||':>12} {'Train MSE':>12}\")\n",
    "print(\"-\" * 36)\n",
    "\n",
    "for reg in regs:\n",
    "    w, b = ridge_regression_closed_form(X_collinear, y_collinear, reg)\n",
    "    y_pred = X_collinear @ w + b\n",
    "    mse = mse_loss(y_pred, y_collinear)\n",
    "    w_norm = np.linalg.norm(w)\n",
    "    print(f\"{reg:>10.2f} {w_norm:>12.4f} {mse:>12.6f}\")\n",
    "\n",
    "print(\"\\n觀察：正則化越強，||w|| 越小，但 MSE 可能增加（bias-variance trade-off）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化正則化效果\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 權重大小 vs 正則化強度\n",
    "regs = np.logspace(-3, 2, 50)\n",
    "w_norms = []\n",
    "mses = []\n",
    "\n",
    "for reg in regs:\n",
    "    w, b = ridge_regression_closed_form(X_collinear, y_collinear, reg)\n",
    "    w_norms.append(np.linalg.norm(w))\n",
    "    y_pred = X_collinear @ w + b\n",
    "    mses.append(mse_loss(y_pred, y_collinear))\n",
    "\n",
    "axes[0].semilogx(regs, w_norms)\n",
    "axes[0].set_xlabel('λ (正則化強度)')\n",
    "axes[0].set_ylabel('||w||')\n",
    "axes[0].set_title('正則化強度 vs 權重大小')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogx(regs, mses)\n",
    "axes[1].set_xlabel('λ (正則化強度)')\n",
    "axes[1].set_ylabel('Training MSE')\n",
    "axes[1].set_title('正則化強度 vs 訓練誤差')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題\n",
    "\n",
    "### 練習 1：實作多項式回歸\n",
    "\n",
    "**任務**：使用特徵轉換實作多項式回歸\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x + w_2 x^2 + ... + w_d x^d$$\n",
    "\n",
    "**提示**：\n",
    "- 建立特徵矩陣 $[1, x, x^2, ..., x^d]$\n",
    "- 然後套用一般的線性回歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1 解答\n",
    "\n",
    "def polynomial_features(x, degree):\n",
    "    \"\"\"\n",
    "    建立多項式特徵\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape (N,)\n",
    "        原始特徵\n",
    "    degree : int\n",
    "        多項式次數\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_poly : np.ndarray, shape (N, degree+1)\n",
    "        多項式特徵 [1, x, x^2, ..., x^degree]\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    X_poly = np.zeros((N, degree + 1))\n",
    "    \n",
    "    for d in range(degree + 1):\n",
    "        X_poly[:, d] = x ** d\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "\n",
    "def polynomial_regression(x, y, degree, reg=0.0):\n",
    "    \"\"\"\n",
    "    多項式回歸\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape (N,)\n",
    "    y : np.ndarray, shape (N,)\n",
    "    degree : int\n",
    "    reg : float\n",
    "        正則化強度\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    coefficients : np.ndarray, shape (degree+1,)\n",
    "        多項式係數 [w_0, w_1, ..., w_d]\n",
    "    \"\"\"\n",
    "    # 建立多項式特徵（不包含常數項，因為 ridge_regression 會加）\n",
    "    X_poly = polynomial_features(x, degree)[:, 1:]  # 去掉常數項\n",
    "    \n",
    "    # 用 Ridge regression 擬合\n",
    "    w, b = ridge_regression_closed_form(X_poly, y, reg)\n",
    "    \n",
    "    # 組合係數\n",
    "    coefficients = np.concatenate([[b], w])\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "\n",
    "# 測試多項式回歸\n",
    "print(\"=== 測試多項式回歸 ===\")\n",
    "\n",
    "# 生成非線性資料\n",
    "np.random.seed(42)\n",
    "x_train = np.sort(np.random.uniform(-3, 3, 30))\n",
    "y_train = np.sin(x_train) + np.random.randn(30) * 0.2  # 真實函數是 sin(x)\n",
    "\n",
    "# 測試不同次數的多項式\n",
    "x_test = np.linspace(-3.5, 3.5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "degrees = [1, 2, 3, 5, 9, 15]\n",
    "\n",
    "for ax, degree in zip(axes, degrees):\n",
    "    # 擬合\n",
    "    coeffs = polynomial_regression(x_train, y_train, degree, reg=0.0)\n",
    "    \n",
    "    # 預測\n",
    "    X_test_poly = polynomial_features(x_test, degree)\n",
    "    y_pred = X_test_poly @ coeffs\n",
    "    \n",
    "    # 訓練 MSE\n",
    "    X_train_poly = polynomial_features(x_train, degree)\n",
    "    y_train_pred = X_train_poly @ coeffs\n",
    "    mse = mse_loss(y_train_pred, y_train)\n",
    "    \n",
    "    # 繪圖\n",
    "    ax.scatter(x_train, y_train, c='blue', alpha=0.6, label='Training data')\n",
    "    ax.plot(x_test, np.sin(x_test), 'g--', linewidth=2, label='True function')\n",
    "    ax.plot(x_test, y_pred, 'r-', linewidth=2, label=f'Degree {degree}')\n",
    "    ax.set_title(f'Degree {degree}, MSE={mse:.4f}')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('多項式回歸：不同次數的擬合效果', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：次數太高會過擬合（在訓練範圍外預測不穩定）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：正則化對多項式回歸的影響\n",
    "\n",
    "**任務**：觀察正則化如何幫助減少高次多項式的過擬合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2 解答\n",
    "\n",
    "print(\"=== 正則化對高次多項式的影響 ===\")\n",
    "\n",
    "degree = 15  # 高次多項式容易過擬合\n",
    "regs = [0, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, reg in zip(axes, regs):\n",
    "    # 擬合\n",
    "    coeffs = polynomial_regression(x_train, y_train, degree, reg=reg)\n",
    "    \n",
    "    # 預測\n",
    "    X_test_poly = polynomial_features(x_test, degree)\n",
    "    y_pred = X_test_poly @ coeffs\n",
    "    \n",
    "    # 訓練 MSE\n",
    "    X_train_poly = polynomial_features(x_train, degree)\n",
    "    y_train_pred = X_train_poly @ coeffs\n",
    "    mse = mse_loss(y_train_pred, y_train)\n",
    "    \n",
    "    # 係數大小\n",
    "    coeff_norm = np.linalg.norm(coeffs)\n",
    "    \n",
    "    # 繪圖\n",
    "    ax.scatter(x_train, y_train, c='blue', alpha=0.6, label='Training data')\n",
    "    ax.plot(x_test, np.sin(x_test), 'g--', linewidth=2, label='True function')\n",
    "    ax.plot(x_test, y_pred, 'r-', linewidth=2, label=f'λ={reg}')\n",
    "    ax.set_title(f'λ={reg}\\nMSE={mse:.4f}, ||w||={coeff_norm:.2f}')\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Degree {degree} 多項式回歸：正則化的影響', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：適當的正則化可以讓高次多項式也有好的泛化能力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：Mini-batch 梯度下降\n",
    "\n",
    "**任務**：實作 Mini-batch SGD 來加速大資料集的訓練\n",
    "\n",
    "**提示**：\n",
    "- 每次迭代只用一個 batch 的資料計算梯度\n",
    "- Batch size 通常取 32, 64, 128 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3 解答\n",
    "\n",
    "def linear_regression_minibatch_sgd(X, y, lr=0.01, n_epochs=100, batch_size=32, verbose=True):\n",
    "    \"\"\"\n",
    "    使用 Mini-batch SGD 求解線性回歸\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    y : np.ndarray, shape (N,)\n",
    "    lr : float\n",
    "        學習率\n",
    "    n_epochs : int\n",
    "        訓練 epochs 數\n",
    "    batch_size : int\n",
    "        Batch 大小\n",
    "    verbose : bool\n",
    "        是否輸出訓練過程\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : np.ndarray, shape (D,)\n",
    "    b : float\n",
    "    history : dict\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # 初始化參數\n",
    "    w = np.zeros(D)\n",
    "    b = 0.0\n",
    "    \n",
    "    history = {'loss': []}\n",
    "    \n",
    "    # 每個 epoch 的 batch 數\n",
    "    n_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # 打亂資料順序\n",
    "        indices = np.random.permutation(N)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            # 取出 batch\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # 計算梯度（在 batch 上）\n",
    "            dw, db = compute_gradients(X_batch, y_batch, w, b)\n",
    "            \n",
    "            # 更新參數\n",
    "            w = w - lr * dw\n",
    "            b = b - lr * db\n",
    "        \n",
    "        # 計算整個訓練集的 loss（用於監控）\n",
    "        y_pred = X @ w + b\n",
    "        loss = mse_loss(y_pred, y)\n",
    "        history['loss'].append(loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % (n_epochs // 10) == 0:\n",
    "            print(f\"Epoch {epoch+1:4d}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# 測試 Mini-batch SGD\n",
    "print(\"=== 測試 Mini-batch SGD ===\")\n",
    "\n",
    "# 生成較大的資料集\n",
    "X_large, y_large, w_true, b_true = generate_linear_data(n_samples=1000, n_features=10, noise_std=0.5)\n",
    "\n",
    "print(f\"資料大小: {X_large.shape}\")\n",
    "print(f\"真實參數: w = {w_true[:3]}..., b = {b_true:.4f}\\n\")\n",
    "\n",
    "# Mini-batch SGD\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "w_sgd, b_sgd, history_sgd = linear_regression_minibatch_sgd(\n",
    "    X_large, y_large, lr=0.01, n_epochs=100, batch_size=32\n",
    ")\n",
    "time_sgd = time.time() - start\n",
    "\n",
    "print(f\"\\nMini-batch SGD 完成 (耗時: {time_sgd:.3f}s)\")\n",
    "\n",
    "# 與閉式解比較\n",
    "start = time.time()\n",
    "w_closed, b_closed = linear_regression_closed_form(X_large, y_large)\n",
    "time_closed = time.time() - start\n",
    "\n",
    "print(f\"閉式解完成 (耗時: {time_closed:.3f}s)\")\n",
    "\n",
    "# 比較結果\n",
    "y_pred_sgd = X_large @ w_sgd + b_sgd\n",
    "y_pred_closed = X_large @ w_closed + b_closed\n",
    "\n",
    "print(f\"\\nMini-batch SGD MSE: {mse_loss(y_pred_sgd, y_large):.6f}\")\n",
    "print(f\"閉式解 MSE: {mse_loss(y_pred_closed, y_large):.6f}\")\n",
    "\n",
    "# 視覺化訓練過程\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_sgd['loss'])\n",
    "plt.axhline(y=mse_loss(y_pred_closed, y_large), color='r', linestyle='--', label='Closed-form solution')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Mini-batch SGD 訓練曲線')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 整合練習：完整的線性回歸類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    線性回歸模型\n",
    "    \n",
    "    支援閉式解和梯度下降兩種方法，以及 L2 正則化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    regularization : float\n",
    "        L2 正則化強度（預設 0，即不正則化）\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w : np.ndarray\n",
    "        權重向量\n",
    "    b : float\n",
    "        偏置\n",
    "    history : dict\n",
    "        訓練歷史（僅梯度下降法）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, regularization=0.0):\n",
    "        self.reg = regularization\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.history = None\n",
    "    \n",
    "    def fit_closed_form(self, X, y):\n",
    "        \"\"\"\n",
    "        使用閉式解（Normal Equation）擬合\n",
    "        \n",
    "        w* = (X^T X + λI)^{-1} X^T y\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # 加入 bias 項\n",
    "        X_tilde = np.hstack([np.ones((N, 1)), X])\n",
    "        \n",
    "        # 正則化矩陣（不對 bias 正則化）\n",
    "        reg_matrix = self.reg * np.eye(D + 1)\n",
    "        reg_matrix[0, 0] = 0\n",
    "        \n",
    "        # 閉式解\n",
    "        XTX = X_tilde.T @ X_tilde + reg_matrix\n",
    "        XTy = X_tilde.T @ y\n",
    "        \n",
    "        w_tilde = np.linalg.solve(XTX, XTy)\n",
    "        \n",
    "        self.b = w_tilde[0]\n",
    "        self.w = w_tilde[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_gradient_descent(self, X, y, lr=0.01, n_iter=1000, verbose=False):\n",
    "        \"\"\"\n",
    "        使用梯度下降擬合\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # 初始化\n",
    "        self.w = np.zeros(D)\n",
    "        self.b = 0.0\n",
    "        self.history = {'loss': []}\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # 預測\n",
    "            y_pred = X @ self.w + self.b\n",
    "            error = y_pred - y\n",
    "            \n",
    "            # 梯度（含正則化）\n",
    "            dw = (2 / N) * (X.T @ error) + 2 * self.reg * self.w\n",
    "            db = (2 / N) * np.sum(error)\n",
    "            \n",
    "            # 更新\n",
    "            self.w = self.w - lr * dw\n",
    "            self.b = self.b - lr * db\n",
    "            \n",
    "            # 記錄 loss\n",
    "            loss = np.mean(error ** 2) + self.reg * np.sum(self.w ** 2)\n",
    "            self.history['loss'].append(loss)\n",
    "            \n",
    "            if verbose and (i + 1) % (n_iter // 10) == 0:\n",
    "                print(f\"Iter {i+1}: Loss = {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測\n",
    "        \"\"\"\n",
    "        if self.w is None:\n",
    "            raise ValueError(\"Model not fitted. Call fit_* first.\")\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        計算 R² 分數\n",
    "        \n",
    "        R² = 1 - SS_res / SS_tot\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "# 測試完整類別\n",
    "print(\"=== 測試 LinearRegression 類別 ===\")\n",
    "\n",
    "X, y, w_true, b_true = generate_linear_data(n_samples=100, n_features=5)\n",
    "\n",
    "# 閉式解\n",
    "model_closed = LinearRegression(regularization=0.01)\n",
    "model_closed.fit_closed_form(X, y)\n",
    "\n",
    "print(f\"閉式解 R²: {model_closed.score(X, y):.6f}\")\n",
    "\n",
    "# 梯度下降\n",
    "model_gd = LinearRegression(regularization=0.01)\n",
    "model_gd.fit_gradient_descent(X, y, lr=0.01, n_iter=2000, verbose=False)\n",
    "\n",
    "print(f\"梯度下降 R²: {model_gd.score(X, y):.6f}\")\n",
    "\n",
    "# 參數比較\n",
    "print(f\"\\n參數差異: |w_closed - w_gd| = {np.linalg.norm(model_closed.w - model_gd.w):.8f}\")\n",
    "\n",
    "print(\"\\nLinearRegression 類別測試完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的內容\n",
    "\n",
    "1. **MSE 損失函數**：$L = \\frac{1}{N} \\sum (\\hat{y} - y)^2$\n",
    "\n",
    "2. **閉式解（Normal Equation）**：\n",
    "   - $w^* = (X^T X)^{-1} X^T y$\n",
    "   - 一次計算得到最優解\n",
    "   - 但 $X^T X$ 可能不可逆\n",
    "\n",
    "3. **梯度下降**：\n",
    "   - 梯度：$\\frac{\\partial L}{\\partial w} = \\frac{2}{N} X^T (Xw + b - y)$\n",
    "   - 迭代更新：$w \\leftarrow w - \\eta \\nabla_w L$\n",
    "   - Mini-batch SGD 加速大資料訓練\n",
    "\n",
    "4. **正則化（Ridge）**：\n",
    "   - $L_{ridge} = L_{MSE} + \\lambda \\|w\\|^2$\n",
    "   - 閉式解：$(X^T X + \\lambda I)^{-1} X^T y$\n",
    "   - 解決不可逆和過擬合問題\n",
    "\n",
    "5. **數值梯度檢查**：\n",
    "   - 用有限差分驗證解析梯度\n",
    "   - 確保實作正確\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. **公式 → 程式碼**：理解數學推導，然後用向量化實作\n",
    "2. **梯度檢查**：永遠驗證你的梯度實作\n",
    "3. **正則化**：在實際應用中幾乎總是需要的\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個 notebook 中，我們將學習 **Logistic Regression**（邏輯回歸），用於分類問題。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
