{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.6: GMM + EM 演算法（進階）\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "完成這個 notebook 後，你將能夠：\n",
    "\n",
    "1. 理解 Gaussian Mixture Model (GMM) 的概念\n",
    "2. 理解 Expectation-Maximization (EM) 演算法\n",
    "3. 實作 GMM 的 E-step 和 M-step\n",
    "4. 比較 GMM（軟聚類）與 K-Means（硬聚類）\n",
    "\n",
    "## 背景知識\n",
    "\n",
    "GMM 假設資料是由 K 個高斯分布混合而成。與 K-Means 不同，GMM 給出的是「機率性」的聚類結果。\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- Bishop PRML Ch. 9.2-9.3: Mixtures of Gaussians and EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"環境設定完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Gaussian Mixture Model\n",
    "\n",
    "### 模型定義\n",
    "\n",
    "GMM 假設資料點 $x$ 來自 K 個高斯分布的混合：\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "其中：\n",
    "- $\\pi_k$：第 k 個成分的混合係數（權重），$\\sum_k \\pi_k = 1$\n",
    "- $\\mu_k$：第 k 個高斯的均值\n",
    "- $\\Sigma_k$：第 k 個高斯的協方差矩陣\n",
    "\n",
    "### 多元高斯分布\n",
    "\n",
    "$$\\mathcal{N}(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_gaussian(x, mean, cov):\n",
    "    \"\"\"\n",
    "    計算多元高斯分布的機率密度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape (N, D) or (D,)\n",
    "        資料點\n",
    "    mean : np.ndarray, shape (D,)\n",
    "        均值\n",
    "    cov : np.ndarray, shape (D, D)\n",
    "        協方差矩陣\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, shape (N,) or float\n",
    "        機率密度值\n",
    "    \"\"\"\n",
    "    x = np.atleast_2d(x)\n",
    "    N, D = x.shape\n",
    "    \n",
    "    # 計算常數項\n",
    "    det_cov = np.linalg.det(cov)\n",
    "    norm_const = 1.0 / (np.power(2 * np.pi, D/2) * np.sqrt(det_cov))\n",
    "    \n",
    "    # 計算指數項\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    diff = x - mean  # shape (N, D)\n",
    "    \n",
    "    # (x-μ)^T Σ^{-1} (x-μ)\n",
    "    exponent = np.sum(diff @ cov_inv * diff, axis=1)  # shape (N,)\n",
    "    \n",
    "    pdf = norm_const * np.exp(-0.5 * exponent)\n",
    "    \n",
    "    return pdf.squeeze()\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試多元高斯分布 ===\")\n",
    "\n",
    "mean = np.array([0, 0])\n",
    "cov = np.array([[1, 0.5], [0.5, 1]])\n",
    "\n",
    "# 在均值處的密度應該最大\n",
    "print(f\"p([0, 0]) = {multivariate_gaussian([0, 0], mean, cov):.4f}\")\n",
    "print(f\"p([1, 1]) = {multivariate_gaussian([1, 1], mean, cov):.4f}\")\n",
    "print(f\"p([3, 3]) = {multivariate_gaussian([3, 3], mean, cov):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 GMM 測試資料\n",
    "def generate_gmm_data(n_samples=300, seed=42):\n",
    "    \"\"\"\n",
    "    生成 GMM 測試資料\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # 3 個高斯成分\n",
    "    means = [\n",
    "        np.array([0, 0]),\n",
    "        np.array([4, 4]),\n",
    "        np.array([8, 0])\n",
    "    ]\n",
    "    \n",
    "    covs = [\n",
    "        np.array([[1, 0.5], [0.5, 1]]),\n",
    "        np.array([[1.5, -0.3], [-0.3, 0.5]]),\n",
    "        np.array([[0.8, 0], [0, 2]])\n",
    "    ]\n",
    "    \n",
    "    weights = [0.3, 0.4, 0.3]\n",
    "    \n",
    "    X_list = []\n",
    "    y_list = []  # 真實成分標籤\n",
    "    \n",
    "    for k in range(3):\n",
    "        n_k = int(n_samples * weights[k])\n",
    "        X_k = np.random.multivariate_normal(means[k], covs[k], n_k)\n",
    "        X_list.append(X_k)\n",
    "        y_list.append(np.full(n_k, k))\n",
    "    \n",
    "    X = np.vstack(X_list)\n",
    "    y = np.concatenate(y_list)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    return X[idx], y[idx], means, covs, weights\n",
    "\n",
    "\n",
    "# 視覺化\n",
    "X, y_true, true_means, true_covs, true_weights = generate_gmm_data(n_samples=300)\n",
    "\n",
    "def plot_ellipse(mean, cov, ax, n_std=2, **kwargs):\n",
    "    \"\"\"繪製高斯分布的等高線橢圓\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    width, height = 2 * n_std * np.sqrt(eigenvalues)\n",
    "    \n",
    "    ellipse = Ellipse(xy=mean, width=width, height=height, angle=angle, **kwargs)\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for k in range(3):\n",
    "    mask = y_true == k\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[k], s=30, alpha=0.6, label=f'Component {k}')\n",
    "    plot_ellipse(true_means[k], true_covs[k], ax, n_std=2, \n",
    "                fill=False, edgecolor=colors[k], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('GMM 測試資料（真實分布）')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: EM 演算法\n",
    "\n",
    "### 問題\n",
    "\n",
    "GMM 參數的最大似然估計沒有閉式解。\n",
    "\n",
    "### EM 演算法\n",
    "\n",
    "**E-step (Expectation)**：計算每個點屬於各成分的「責任」(responsibility)\n",
    "\n",
    "$$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**M-step (Maximization)**：用責任更新參數\n",
    "\n",
    "$$N_k = \\sum_{i=1}^N \\gamma_{ik}$$\n",
    "\n",
    "$$\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} x_i$$\n",
    "\n",
    "$$\\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T$$\n",
    "\n",
    "$$\\pi_k = \\frac{N_k}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture:\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model with EM algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int\n",
    "        混合成分數量 K\n",
    "    max_iter : int\n",
    "        最大迭代次數\n",
    "    tol : float\n",
    "        收斂容差（log-likelihood 變化）\n",
    "    reg_covar : float\n",
    "        協方差正則化（避免奇異）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=3, max_iter=100, tol=1e-4, reg_covar=1e-6):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.reg_covar = reg_covar\n",
    "        \n",
    "        self.weights_ = None  # π_k\n",
    "        self.means_ = None    # μ_k\n",
    "        self.covariances_ = None  # Σ_k\n",
    "        self.history = None\n",
    "    \n",
    "    def _initialize(self, X):\n",
    "        \"\"\"\n",
    "        初始化參數\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        K = self.n_components\n",
    "        \n",
    "        # 權重：均勻\n",
    "        self.weights_ = np.ones(K) / K\n",
    "        \n",
    "        # 均值：隨機選 K 個點\n",
    "        indices = np.random.choice(N, K, replace=False)\n",
    "        self.means_ = X[indices].copy()\n",
    "        \n",
    "        # 協方差：初始化為單位矩陣\n",
    "        self.covariances_ = np.array([np.eye(D) for _ in range(K)])\n",
    "    \n",
    "    def _compute_responsibilities(self, X):\n",
    "        \"\"\"\n",
    "        E-step：計算 responsibilities γ_{ik}\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        responsibilities : np.ndarray, shape (N, K)\n",
    "            γ_{ik} = P(z_i = k | x_i)\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        K = self.n_components\n",
    "        \n",
    "        # 計算每個成分對每個點的加權機率密度\n",
    "        weighted_probs = np.zeros((N, K))\n",
    "        \n",
    "        for k in range(K):\n",
    "            weighted_probs[:, k] = self.weights_[k] * multivariate_gaussian(\n",
    "                X, self.means_[k], self.covariances_[k]\n",
    "            )\n",
    "        \n",
    "        # 正規化\n",
    "        sum_probs = np.sum(weighted_probs, axis=1, keepdims=True) + 1e-10\n",
    "        responsibilities = weighted_probs / sum_probs\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step：更新參數\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        K = self.n_components\n",
    "        \n",
    "        # N_k: 有效樣本數\n",
    "        N_k = np.sum(responsibilities, axis=0)  # shape (K,)\n",
    "        \n",
    "        # 更新權重\n",
    "        self.weights_ = N_k / N\n",
    "        \n",
    "        # 更新均值\n",
    "        for k in range(K):\n",
    "            self.means_[k] = np.sum(responsibilities[:, k:k+1] * X, axis=0) / N_k[k]\n",
    "        \n",
    "        # 更新協方差\n",
    "        for k in range(K):\n",
    "            diff = X - self.means_[k]  # shape (N, D)\n",
    "            # 加權外積和\n",
    "            weighted_diff = responsibilities[:, k:k+1] * diff  # shape (N, D)\n",
    "            self.covariances_[k] = (weighted_diff.T @ diff) / N_k[k]\n",
    "            \n",
    "            # 正則化（避免奇異）\n",
    "            self.covariances_[k] += self.reg_covar * np.eye(D)\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        計算 log-likelihood\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        K = self.n_components\n",
    "        \n",
    "        # 計算混合機率\n",
    "        probs = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            probs[:, k] = self.weights_[k] * multivariate_gaussian(\n",
    "                X, self.means_[k], self.covariances_[k]\n",
    "            )\n",
    "        \n",
    "        # log-likelihood = Σ log(Σ_k π_k N(x|μ_k, Σ_k))\n",
    "        return np.sum(np.log(np.sum(probs, axis=1) + 1e-10))\n",
    "    \n",
    "    def fit(self, X, verbose=True):\n",
    "        \"\"\"\n",
    "        使用 EM 演算法擬合 GMM\n",
    "        \"\"\"\n",
    "        self._initialize(X)\n",
    "        self.history = {'log_likelihood': []}\n",
    "        \n",
    "        prev_ll = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step\n",
    "            responsibilities = self._compute_responsibilities(X)\n",
    "            \n",
    "            # M-step\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # 計算 log-likelihood\n",
    "            ll = self._compute_log_likelihood(X)\n",
    "            self.history['log_likelihood'].append(ll)\n",
    "            \n",
    "            if verbose and (iteration + 1) % 10 == 0:\n",
    "                print(f\"Iter {iteration+1}: Log-likelihood = {ll:.4f}\")\n",
    "            \n",
    "            # 檢查收斂\n",
    "            if abs(ll - prev_ll) < self.tol:\n",
    "                if verbose:\n",
    "                    print(f\"收斂於 iteration {iteration+1}\")\n",
    "                break\n",
    "            \n",
    "            prev_ll = ll\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測每個點最可能的成分（硬分配）\n",
    "        \"\"\"\n",
    "        responsibilities = self._compute_responsibilities(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        預測每個點屬於各成分的機率（軟分配）\n",
    "        \"\"\"\n",
    "        return self._compute_responsibilities(X)\n",
    "\n",
    "\n",
    "# 測試\n",
    "print(\"=== 測試 GMM ===\")\n",
    "\n",
    "gmm = GaussianMixture(n_components=3, max_iter=100)\n",
    "gmm.fit(X, verbose=True)\n",
    "\n",
    "print(f\"\\n學習到的權重: {gmm.weights_}\")\n",
    "print(f\"真實權重: {true_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 GMM 結果\n",
    "\n",
    "def plot_gmm_result(X, gmm, title=\"GMM Result\"):\n",
    "    \"\"\"繪製 GMM 聚類結果\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 硬分配\n",
    "    labels = gmm.predict(X)\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    for k in range(gmm.n_components):\n",
    "        mask = labels == k\n",
    "        axes[0].scatter(X[mask, 0], X[mask, 1], c=colors[k], s=30, alpha=0.6)\n",
    "        plot_ellipse(gmm.means_[k], gmm.covariances_[k], axes[0], n_std=2,\n",
    "                    fill=False, edgecolor=colors[k], linewidth=2)\n",
    "        axes[0].scatter(gmm.means_[k][0], gmm.means_[k][1], c='black', marker='X', s=150)\n",
    "    \n",
    "    axes[0].set_title('Hard Assignment (argmax γ)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_aspect('equal')\n",
    "    \n",
    "    # 軟分配（用顏色混合表示）\n",
    "    responsibilities = gmm.predict_proba(X)\n",
    "    \n",
    "    # RGB 顏色混合\n",
    "    color_matrix = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])  # blue, red, green\n",
    "    point_colors = responsibilities @ color_matrix\n",
    "    \n",
    "    axes[1].scatter(X[:, 0], X[:, 1], c=point_colors, s=30, alpha=0.6)\n",
    "    \n",
    "    for k in range(gmm.n_components):\n",
    "        plot_ellipse(gmm.means_[k], gmm.covariances_[k], axes[1], n_std=2,\n",
    "                    fill=False, edgecolor=colors[k], linewidth=2)\n",
    "    \n",
    "    axes[1].set_title('Soft Assignment (color ∝ γ)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_aspect('equal')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "plot_gmm_result(X, gmm, title='GMM 聚類結果')\n",
    "plt.show()\n",
    "\n",
    "# Log-likelihood 曲線\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gmm.history['log_likelihood'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.title('EM 演算法收斂曲線')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: GMM vs K-Means 比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 GMM 和 K-Means\n",
    "print(\"=== GMM vs K-Means 比較 ===\")\n",
    "\n",
    "# 使用前面實作的 K-Means\n",
    "class KMeansSimple:\n",
    "    def __init__(self, n_clusters=3, max_iter=100):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        N = X.shape[0]\n",
    "        indices = np.random.choice(N, self.n_clusters, replace=False)\n",
    "        self.centroids = X[indices].copy()\n",
    "        \n",
    "        for _ in range(self.max_iter):\n",
    "            distances = np.sum((X[:, np.newaxis, :] - self.centroids[np.newaxis, :, :]) ** 2, axis=2)\n",
    "            self.labels_ = np.argmin(distances, axis=1)\n",
    "            \n",
    "            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) if np.sum(self.labels_ == k) > 0 \n",
    "                                      else self.centroids[k] for k in range(self.n_clusters)])\n",
    "            \n",
    "            if np.allclose(new_centroids, self.centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = np.sum((X[:, np.newaxis, :] - self.centroids[np.newaxis, :, :]) ** 2, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "# 生成橢圓形 cluster 資料\n",
    "np.random.seed(42)\n",
    "X_ellipse, y_ellipse, _, _, _ = generate_gmm_data(n_samples=300)\n",
    "\n",
    "# 訓練\n",
    "kmeans = KMeansSimple(n_clusters=3)\n",
    "kmeans.fit(X_ellipse)\n",
    "\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(X_ellipse, verbose=False)\n",
    "\n",
    "# 視覺化比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "# K-Means\n",
    "for k in range(3):\n",
    "    mask = kmeans.labels_ == k\n",
    "    axes[0].scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=colors[k], s=30, alpha=0.6)\n",
    "axes[0].scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c='black', marker='X', s=200)\n",
    "axes[0].set_title('K-Means (Hard, Spherical)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# GMM\n",
    "labels_gmm = gmm.predict(X_ellipse)\n",
    "for k in range(3):\n",
    "    mask = labels_gmm == k\n",
    "    axes[1].scatter(X_ellipse[mask, 0], X_ellipse[mask, 1], c=colors[k], s=30, alpha=0.6)\n",
    "    plot_ellipse(gmm.means_[k], gmm.covariances_[k], axes[1], n_std=2,\n",
    "                fill=False, edgecolor=colors[k], linewidth=2)\n",
    "axes[1].set_title('GMM (Soft, Elliptical)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('K-Means vs GMM', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"- K-Means 假設 cluster 是球形的\")\n",
    "print(\"- GMM 可以學習橢圓形的 cluster\")\n",
    "print(\"- GMM 提供機率性的軟分配\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題\n",
    "\n",
    "### 練習 1：BIC/AIC 選擇成分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1 解答：使用 BIC 選擇 K\n",
    "\n",
    "def compute_bic(gmm, X):\n",
    "    \"\"\"\n",
    "    計算 BIC (Bayesian Information Criterion)\n",
    "    \n",
    "    BIC = -2 * log-likelihood + k * log(N)\n",
    "    \n",
    "    其中 k 是參數數量\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = gmm.n_components\n",
    "    \n",
    "    # 參數數量\n",
    "    # K-1 weights (sum to 1)\n",
    "    # K * D means\n",
    "    # K * D*(D+1)/2 covariance parameters (symmetric)\n",
    "    n_params = (K - 1) + K * D + K * D * (D + 1) / 2\n",
    "    \n",
    "    # Log-likelihood\n",
    "    ll = gmm._compute_log_likelihood(X)\n",
    "    \n",
    "    # BIC（越小越好）\n",
    "    bic = -2 * ll + n_params * np.log(N)\n",
    "    \n",
    "    return bic\n",
    "\n",
    "\n",
    "# 測試不同的 K\n",
    "print(\"=== 使用 BIC 選擇成分數 ===\")\n",
    "\n",
    "K_range = range(1, 7)\n",
    "bics = []\n",
    "lls = []\n",
    "\n",
    "for K in K_range:\n",
    "    gmm = GaussianMixture(n_components=K, max_iter=100)\n",
    "    gmm.fit(X, verbose=False)\n",
    "    \n",
    "    bic = compute_bic(gmm, X)\n",
    "    ll = gmm._compute_log_likelihood(X)\n",
    "    \n",
    "    bics.append(bic)\n",
    "    lls.append(ll)\n",
    "    print(f\"K={K}: BIC = {bic:.2f}, Log-likelihood = {ll:.2f}\")\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range, lls, 'bo-', linewidth=2)\n",
    "axes[0].set_xlabel('Number of components (K)')\n",
    "axes[0].set_ylabel('Log-likelihood')\n",
    "axes[0].set_title('Log-likelihood vs K')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range, bics, 'ro-', linewidth=2)\n",
    "axes[1].set_xlabel('Number of components (K)')\n",
    "axes[1].set_ylabel('BIC (lower is better)')\n",
    "axes[1].set_title('BIC vs K')\n",
    "axes[1].axvline(x=K_range[np.argmin(bics)], color='green', linestyle='--', label=f'Best K={K_range[np.argmin(bics)]}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n最佳 K (BIC): {K_range[np.argmin(bics)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的內容\n",
    "\n",
    "1. **Gaussian Mixture Model**：\n",
    "   - K 個高斯分布的混合\n",
    "   - 參數：$\\{\\pi_k, \\mu_k, \\Sigma_k\\}$\n",
    "\n",
    "2. **EM 演算法**：\n",
    "   - E-step：計算 responsibilities\n",
    "   - M-step：更新參數\n",
    "   - 保證 log-likelihood 單調增加\n",
    "\n",
    "3. **GMM vs K-Means**：\n",
    "   - 軟分配 vs 硬分配\n",
    "   - 橢圓 vs 球形\n",
    "   - 機率模型 vs 距離模型\n",
    "\n",
    "4. **模型選擇**：\n",
    "   - BIC/AIC 選擇成分數\n",
    "\n",
    "### 關鍵要點\n",
    "\n",
    "1. GMM 是生成模型，可以用於密度估計和取樣\n",
    "2. EM 演算法是通用的隱變數模型優化方法\n",
    "3. 初始化很重要，多次運行選最佳結果\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個 notebook 中，我們將整合 HOG + SVM 建立完整的影像分類器。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
