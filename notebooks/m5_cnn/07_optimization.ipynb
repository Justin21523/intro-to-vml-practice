{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 優化技巧 Optimization Techniques\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 深入理解 Weight Initialization 的重要性\n",
    "2. 實作 Momentum SGD 和 Adam 優化器\n",
    "3. 實作 Learning Rate Schedule\n",
    "4. 比較不同優化策略的效果\n",
    "\n",
    "## 為什麼優化技巧重要？\n",
    "\n",
    "深度神經網路的訓練是一個複雜的非凸優化問題。好的優化策略可以：\n",
    "- 加速收斂\n",
    "- 避免陷入局部極小值\n",
    "- 提高最終模型的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Optimization Techniques module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：Weight Initialization\n",
    "\n",
    "### 為什麼初始化很重要？\n",
    "\n",
    "不當的初始化會導致：\n",
    "1. **梯度消失**：激活值太小，梯度接近零\n",
    "2. **梯度爆炸**：激活值太大，梯度暴增\n",
    "3. **對稱性問題**：如果所有權重相同，每個神經元學到相同的東西\n",
    "\n",
    "### 常見的初始化方法\n",
    "\n",
    "1. **零初始化**：$W = 0$（壞的！對稱性問題）\n",
    "2. **隨機初始化**：$W \\sim \\mathcal{N}(0, \\sigma^2)$（需要選擇合適的 $\\sigma$）\n",
    "3. **Xavier 初始化**：$W \\sim \\mathcal{N}(0, \\frac{2}{n_{in} + n_{out}})$（適合 tanh/sigmoid）\n",
    "4. **He 初始化**：$W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})$（適合 ReLU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_activation_distribution(init_method, activation_fn, num_layers=10, hidden_dim=256):\n",
    "    \"\"\"\n",
    "    分析不同初始化方式在深度網路中的激活值分佈\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    init_method : str\n",
    "        初始化方法：'zero', 'small', 'normal', 'xavier', 'he'\n",
    "    activation_fn : str\n",
    "        激活函數：'tanh', 'relu'\n",
    "    \"\"\"\n",
    "    x = np.random.randn(32, hidden_dim)\n",
    "    activations = [x]\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        # 初始化權重\n",
    "        if init_method == 'zero':\n",
    "            W = np.zeros((hidden_dim, hidden_dim))\n",
    "        elif init_method == 'small':\n",
    "            W = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        elif init_method == 'normal':\n",
    "            W = np.random.randn(hidden_dim, hidden_dim) * 1.0\n",
    "        elif init_method == 'xavier':\n",
    "            std = np.sqrt(2.0 / (hidden_dim + hidden_dim))\n",
    "            W = np.random.randn(hidden_dim, hidden_dim) * std\n",
    "        elif init_method == 'he':\n",
    "            std = np.sqrt(2.0 / hidden_dim)\n",
    "            W = np.random.randn(hidden_dim, hidden_dim) * std\n",
    "        \n",
    "        # 線性變換\n",
    "        x = x @ W\n",
    "        \n",
    "        # 激活函數\n",
    "        if activation_fn == 'tanh':\n",
    "            x = np.tanh(x)\n",
    "        elif activation_fn == 'relu':\n",
    "            x = np.maximum(0, x)\n",
    "        \n",
    "        activations.append(x)\n",
    "    \n",
    "    return activations\n",
    "\n",
    "# 比較不同初始化方式\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "init_methods = ['small', 'normal', 'xavier', 'he']\n",
    "activation_fns = ['tanh', 'relu']\n",
    "\n",
    "for i, act_fn in enumerate(activation_fns):\n",
    "    for j, init_method in enumerate(init_methods):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        activations = analyze_activation_distribution(init_method, act_fn)\n",
    "        \n",
    "        # 畫每層的激活值分佈\n",
    "        means = [np.mean(a) for a in activations]\n",
    "        stds = [np.std(a) for a in activations]\n",
    "        \n",
    "        layers = list(range(len(means)))\n",
    "        ax.fill_between(layers, \n",
    "                        [m - s for m, s in zip(means, stds)],\n",
    "                        [m + s for m, s in zip(means, stds)],\n",
    "                        alpha=0.3)\n",
    "        ax.plot(layers, means, 'o-')\n",
    "        ax.plot(layers, stds, 's--', label='std')\n",
    "        \n",
    "        ax.set_xlabel('Layer')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title(f'{init_method.capitalize()} Init + {act_fn.upper()}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"- small + tanh/relu: 激活值迅速趨近於零（梯度消失）\")\n",
    "print(\"- normal + tanh: 激活值飽和在 ±1（梯度消失）\")\n",
    "print(\"- normal + relu: 激活值爆炸\")\n",
    "print(\"- xavier + tanh: 激活值保持穩定\")\n",
    "print(\"- he + relu: 激活值保持穩定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：優化器實作\n",
    "\n",
    "### 2.1 Vanilla SGD\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla L(\\theta_t)$$\n",
    "\n",
    "### 2.2 Momentum SGD\n",
    "\n",
    "$$v_{t+1} = \\beta v_t + \\nabla L(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha v_{t+1}$$\n",
    "\n",
    "**直觀理解**：像一個有動量的球在滾動，不容易被小的梯度波動影響\n",
    "\n",
    "### 2.3 Adam (Adaptive Moment Estimation)\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "**直觀理解**：結合了 Momentum（一階動量）和 RMSProp（二階動量）的優點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Vanilla SGD 優化器\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        for param, grad in params_and_grads:\n",
    "            param -= self.lr * grad\n",
    "\n",
    "\n",
    "class MomentumSGD:\n",
    "    \"\"\"\n",
    "    Momentum SGD 優化器\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocities = {}\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        for i, (param, grad) in enumerate(params_and_grads):\n",
    "            if i not in self.velocities:\n",
    "                self.velocities[i] = np.zeros_like(param)\n",
    "            \n",
    "            self.velocities[i] = self.momentum * self.velocities[i] + grad\n",
    "            param -= self.lr * self.velocities[i]\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam 優化器\n",
    "    \n",
    "    結合了 Momentum 和 RMSProp 的優點\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.m = {}  # 一階動量\n",
    "        self.v = {}  # 二階動量\n",
    "        self.t = 0   # 時間步\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, (param, grad) in enumerate(params_and_grads):\n",
    "            if i not in self.m:\n",
    "                self.m[i] = np.zeros_like(param)\n",
    "                self.v[i] = np.zeros_like(param)\n",
    "            \n",
    "            # 更新一階動量\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            \n",
    "            # 更新二階動量\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            # 偏差修正\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # 更新參數\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "print(\"優化器已定義！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化不同優化器在 2D 函數上的行為\n",
    "\n",
    "def rosenbrock(x, y, a=1, b=100):\n",
    "    \"\"\"Rosenbrock 函數（香蕉函數）\"\"\"\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y, a=1, b=100):\n",
    "    \"\"\"Rosenbrock 函數的梯度\"\"\"\n",
    "    dx = -2 * (a - x) - 4 * b * x * (y - x**2)\n",
    "    dy = 2 * b * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "def optimize_rosenbrock(optimizer_class, optimizer_kwargs, start, max_iters=1000):\n",
    "    \"\"\"優化 Rosenbrock 函數\"\"\"\n",
    "    pos = np.array(start, dtype=float)\n",
    "    trajectory = [pos.copy()]\n",
    "    losses = [rosenbrock(pos[0], pos[1])]\n",
    "    \n",
    "    optimizer = optimizer_class(**optimizer_kwargs)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        grad = rosenbrock_grad(pos[0], pos[1])\n",
    "        \n",
    "        # 模擬 params_and_grads 的格式\n",
    "        optimizer.update([(pos, grad)])\n",
    "        \n",
    "        trajectory.append(pos.copy())\n",
    "        losses.append(rosenbrock(pos[0], pos[1]))\n",
    "        \n",
    "        if losses[-1] < 1e-8:\n",
    "            break\n",
    "    \n",
    "    return np.array(trajectory), np.array(losses)\n",
    "\n",
    "# 比較不同優化器\n",
    "start_point = [-1.0, 2.0]\n",
    "\n",
    "optimizers = [\n",
    "    ('SGD', SGD, {'learning_rate': 0.001}),\n",
    "    ('Momentum', MomentumSGD, {'learning_rate': 0.001, 'momentum': 0.9}),\n",
    "    ('Adam', Adam, {'learning_rate': 0.01}),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 等高線圖\n",
    "ax = axes[0]\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.5)\n",
    "ax.contourf(X, Y, Z, levels=np.logspace(-1, 3, 20), cmap='viridis', alpha=0.3)\n",
    "\n",
    "# 最優點\n",
    "ax.plot(1, 1, 'r*', markersize=15, label='Optimum (1, 1)')\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "for (name, opt_class, opt_kwargs), color in zip(optimizers, colors):\n",
    "    traj, losses = optimize_rosenbrock(opt_class, opt_kwargs, start_point)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'o-', color=color, label=name, \n",
    "            markersize=2, alpha=0.7, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimization Trajectories on Rosenbrock Function')\n",
    "ax.legend()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 3)\n",
    "\n",
    "# Loss 曲線\n",
    "ax = axes[1]\n",
    "for (name, opt_class, opt_kwargs), color in zip(optimizers, colors):\n",
    "    traj, losses = optimize_rosenbrock(opt_class, opt_kwargs, start_point)\n",
    "    ax.plot(losses[:200], color=color, label=name)\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Convergence Comparison')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：Learning Rate Schedule\n",
    "\n",
    "學習率的選擇對訓練至關重要：\n",
    "- **太大**：震盪，可能無法收斂\n",
    "- **太小**：收斂太慢\n",
    "\n",
    "常見的策略是從較大的學習率開始，隨著訓練進行逐漸減小。\n",
    "\n",
    "### 常見的 Schedule\n",
    "\n",
    "1. **Step Decay**：每隔 N 個 epoch 減少一定比例\n",
    "2. **Exponential Decay**：$\\alpha_t = \\alpha_0 \\cdot \\gamma^t$\n",
    "3. **Cosine Annealing**：$\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t}{T}\\pi))$\n",
    "4. **Warmup + Decay**：先增加後減少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRScheduler:\n",
    "    \"\"\"Learning Rate Scheduler 基類\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return self.current_lr\n",
    "\n",
    "\n",
    "class StepLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Step Decay: 每 step_size 個 epoch 減少 gamma 倍\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, step_size=30, gamma=0.1):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        self.current_lr = self.initial_lr * (self.gamma ** (epoch // self.step_size))\n",
    "        self.optimizer.lr = self.current_lr\n",
    "\n",
    "\n",
    "class ExponentialLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Exponential Decay: lr = initial_lr * gamma^epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, gamma=0.95):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        self.current_lr = self.initial_lr * (self.gamma ** epoch)\n",
    "        self.optimizer.lr = self.current_lr\n",
    "\n",
    "\n",
    "class CosineAnnealingLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Cosine Annealing: 學習率按餘弦曲線衰減\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, T_max, min_lr=0):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.T_max = T_max\n",
    "        self.min_lr = min_lr\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        self.current_lr = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * (\n",
    "            1 + np.cos(np.pi * epoch / self.T_max)\n",
    "        )\n",
    "        self.optimizer.lr = self.current_lr\n",
    "\n",
    "\n",
    "class WarmupCosineAnnealingLR(LRScheduler):\n",
    "    \"\"\"\n",
    "    Warmup + Cosine Annealing\n",
    "    先線性增加學習率，再用餘弦衰減\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, warmup_epochs, T_max, min_lr=0):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.T_max = T_max\n",
    "        self.min_lr = min_lr\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Warmup: 線性增加\n",
    "            self.current_lr = self.initial_lr * epoch / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (epoch - self.warmup_epochs) / (self.T_max - self.warmup_epochs)\n",
    "            self.current_lr = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * (\n",
    "                1 + np.cos(np.pi * progress)\n",
    "            )\n",
    "        self.optimizer.lr = self.current_lr\n",
    "\n",
    "# 視覺化不同 Schedule\n",
    "epochs = 100\n",
    "initial_lr = 0.1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Step LR\n",
    "optimizer = SGD(learning_rate=initial_lr)\n",
    "scheduler = StepLR(optimizer, initial_lr, step_size=30, gamma=0.1)\n",
    "lrs = []\n",
    "for epoch in range(epochs):\n",
    "    scheduler.step(epoch)\n",
    "    lrs.append(scheduler.get_lr())\n",
    "ax.plot(lrs, label='Step (step=30, gamma=0.1)')\n",
    "\n",
    "# Exponential LR\n",
    "optimizer = SGD(learning_rate=initial_lr)\n",
    "scheduler = ExponentialLR(optimizer, initial_lr, gamma=0.95)\n",
    "lrs = []\n",
    "for epoch in range(epochs):\n",
    "    scheduler.step(epoch)\n",
    "    lrs.append(scheduler.get_lr())\n",
    "ax.plot(lrs, label='Exponential (gamma=0.95)')\n",
    "\n",
    "# Cosine Annealing\n",
    "optimizer = SGD(learning_rate=initial_lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, initial_lr, T_max=epochs)\n",
    "lrs = []\n",
    "for epoch in range(epochs):\n",
    "    scheduler.step(epoch)\n",
    "    lrs.append(scheduler.get_lr())\n",
    "ax.plot(lrs, label='Cosine Annealing')\n",
    "\n",
    "# Warmup + Cosine\n",
    "optimizer = SGD(learning_rate=initial_lr)\n",
    "scheduler = WarmupCosineAnnealingLR(optimizer, initial_lr, warmup_epochs=10, T_max=epochs)\n",
    "lrs = []\n",
    "for epoch in range(epochs):\n",
    "    scheduler.step(epoch)\n",
    "    lrs.append(scheduler.get_lr())\n",
    "ax.plot(lrs, label='Warmup (10 epochs) + Cosine')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedules')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：實驗比較\n",
    "\n",
    "讓我們在實際的分類任務上比較不同的優化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡化的網路用於實驗\n",
    "class SimpleNet:\n",
    "    \"\"\"簡單的兩層網路\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Xavier 初始化\n",
    "        std1 = np.sqrt(2.0 / (input_dim + hidden_dim))\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * std1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        \n",
    "        std2 = np.sqrt(2.0 / (hidden_dim + output_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * std2\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dW1 = None\n",
    "        self.db1 = None\n",
    "        self.dW2 = None\n",
    "        self.db2 = None\n",
    "        \n",
    "        # 快取\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Layer 1\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        \n",
    "        self.cache = (X, z1, a1)\n",
    "        return z2\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        \n",
    "        # Softmax + CE\n",
    "        z_shifted = logits - np.max(logits, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z_shifted)\n",
    "        probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        N = len(y)\n",
    "        loss = -np.mean(np.log(probs[np.arange(N), y] + 1e-10))\n",
    "        \n",
    "        self.probs = probs\n",
    "        self.y = y\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        X, z1, a1 = self.cache\n",
    "        N = len(self.y)\n",
    "        \n",
    "        # Softmax + CE gradient\n",
    "        dz2 = self.probs.copy()\n",
    "        dz2[np.arange(N), self.y] -= 1\n",
    "        dz2 /= N\n",
    "        \n",
    "        # Layer 2 gradients\n",
    "        self.dW2 = a1.T @ dz2\n",
    "        self.db2 = np.sum(dz2, axis=0)\n",
    "        \n",
    "        # Backprop through ReLU\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * (z1 > 0)\n",
    "        \n",
    "        # Layer 1 gradients\n",
    "        self.dW1 = X.T @ dz1\n",
    "        self.db1 = np.sum(dz1, axis=0)\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        return [\n",
    "            (self.W1, self.dW1),\n",
    "            (self.b1, self.db1),\n",
    "            (self.W2, self.dW2),\n",
    "            (self.b2, self.db2),\n",
    "        ]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# 產生資料\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_classes = 5\n",
    "n_features = 20\n",
    "\n",
    "# 產生分類資料\n",
    "X_data = np.random.randn(n_samples, n_features)\n",
    "true_W = np.random.randn(n_features, n_classes)\n",
    "logits = X_data @ true_W\n",
    "y_data = np.argmax(logits, axis=1)\n",
    "\n",
    "# 分割訓練/驗證\n",
    "X_train, X_val = X_data[:800], X_data[800:]\n",
    "y_train, y_val = y_data[:800], y_data[800:]\n",
    "\n",
    "print(f\"訓練資料: {X_train.shape}\")\n",
    "print(f\"驗證資料: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_optimizer(optimizer_class, optimizer_kwargs, epochs=100, use_scheduler=False):\n",
    "    \"\"\"使用指定的優化器訓練\"\"\"\n",
    "    np.random.seed(42)\n",
    "    net = SimpleNet(n_features, 64, n_classes)\n",
    "    optimizer = optimizer_class(**optimizer_kwargs)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = CosineAnnealingLR(optimizer, optimizer_kwargs.get('learning_rate', 0.01), T_max=epochs)\n",
    "    \n",
    "    history = {'loss': [], 'acc': [], 'lr': []}\n",
    "    batch_size = 32\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if use_scheduler:\n",
    "            scheduler.step(epoch)\n",
    "        \n",
    "        # 訓練一個 epoch\n",
    "        perm = np.random.permutation(len(y_train))\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, len(y_train), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            X_batch = X_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "            \n",
    "            loss = net.loss(X_batch, y_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            net.backward()\n",
    "            optimizer.update(net.get_params_and_grads())\n",
    "        \n",
    "        # 記錄\n",
    "        val_pred = net.predict(X_val)\n",
    "        val_acc = np.mean(val_pred == y_val)\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.lr)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 比較不同優化器\n",
    "results = {}\n",
    "\n",
    "print(\"訓練中...\")\n",
    "\n",
    "# SGD\n",
    "results['SGD'] = train_with_optimizer(SGD, {'learning_rate': 0.1})\n",
    "print(f\"SGD: 最終 Acc = {results['SGD']['acc'][-1]:.4f}\")\n",
    "\n",
    "# Momentum SGD\n",
    "results['Momentum'] = train_with_optimizer(MomentumSGD, {'learning_rate': 0.1, 'momentum': 0.9})\n",
    "print(f\"Momentum: 最終 Acc = {results['Momentum']['acc'][-1]:.4f}\")\n",
    "\n",
    "# Adam\n",
    "results['Adam'] = train_with_optimizer(Adam, {'learning_rate': 0.01})\n",
    "print(f\"Adam: 最終 Acc = {results['Adam']['acc'][-1]:.4f}\")\n",
    "\n",
    "# SGD + Cosine LR\n",
    "results['SGD + Cosine'] = train_with_optimizer(SGD, {'learning_rate': 0.1}, use_scheduler=True)\n",
    "print(f\"SGD + Cosine: 最終 Acc = {results['SGD + Cosine']['acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化比較\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "for name, hist in results.items():\n",
    "    ax.plot(hist['loss'], label=name)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "for name, hist in results.items():\n",
    "    ax.plot(hist['acc'], label=name)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate\n",
    "ax = axes[2]\n",
    "for name, hist in results.items():\n",
    "    ax.plot(hist['lr'], label=name)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作 RMSProp\n",
    "\n",
    "RMSProp 使用指數移動平均來調整學習率：\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + (1 - \\beta) g_t^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} g_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    \"\"\"\n",
    "    RMSProp 優化器\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, beta=0.99, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.v = {}  # 二階動量\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        for i, (param, grad) in enumerate(params_and_grads):\n",
    "            # 解答：\n",
    "            if i not in self.v:\n",
    "                self.v[i] = np.zeros_like(param)\n",
    "            \n",
    "            # 更新二階動量\n",
    "            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * (grad ** 2)\n",
    "            \n",
    "            # 更新參數\n",
    "            param -= self.lr * grad / (np.sqrt(self.v[i]) + self.epsilon)\n",
    "\n",
    "# 測試 RMSProp\n",
    "results['RMSProp'] = train_with_optimizer(RMSProp, {'learning_rate': 0.01})\n",
    "print(f\"RMSProp: 最終 Acc = {results['RMSProp']['acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：實作 Gradient Clipping\n",
    "\n",
    "梯度裁剪可以防止梯度爆炸：\n",
    "\n",
    "$$g' = \\frac{g}{\\max(1, \\frac{\\|g\\|}{\\text{threshold}})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params_and_grads, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    對梯度進行裁剪\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params_and_grads : list of (param, grad)\n",
    "    max_norm : float\n",
    "        最大梯度範數\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    clipped : list of (param, clipped_grad)\n",
    "    \"\"\"\n",
    "    # 解答：\n",
    "    # 計算所有梯度的總範數\n",
    "    total_norm = 0\n",
    "    for param, grad in params_and_grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # 計算縮放因子\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    if clip_coef < 1:\n",
    "        # 需要裁剪\n",
    "        clipped = []\n",
    "        for param, grad in params_and_grads:\n",
    "            clipped.append((param, grad * clip_coef))\n",
    "        return clipped\n",
    "    else:\n",
    "        return params_and_grads\n",
    "\n",
    "# 測試\n",
    "test_grads = [\n",
    "    (np.zeros((3, 3)), np.random.randn(3, 3) * 10),\n",
    "    (np.zeros((3,)), np.random.randn(3) * 10),\n",
    "]\n",
    "\n",
    "print(\"裁剪前的梯度範數:\")\n",
    "for _, grad in test_grads:\n",
    "    print(f\"  {np.linalg.norm(grad):.4f}\")\n",
    "\n",
    "clipped = clip_gradients(test_grads, max_norm=1.0)\n",
    "\n",
    "print(\"\\n裁剪後的梯度範數:\")\n",
    "for _, grad in clipped:\n",
    "    print(f\"  {np.linalg.norm(grad):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：權重衰減 (Weight Decay / L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW:\n",
    "    \"\"\"\n",
    "    AdamW: Adam 加上 decoupled weight decay\n",
    "    \n",
    "    與 L2 正則化的區別：\n",
    "    - L2 正則化：將 λ*w 加到梯度中\n",
    "    - Weight Decay：直接在更新步驟中減去 λ*w\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, \n",
    "                 epsilon=1e-8, weight_decay=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        self.t += 1\n",
    "        \n",
    "        for i, (param, grad) in enumerate(params_and_grads):\n",
    "            if i not in self.m:\n",
    "                self.m[i] = np.zeros_like(param)\n",
    "                self.v[i] = np.zeros_like(param)\n",
    "            \n",
    "            # Adam 更新\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # 解答：結合 Adam 更新和 weight decay\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            param -= self.lr * self.weight_decay * param  # decoupled weight decay\n",
    "\n",
    "# 測試 AdamW\n",
    "results['AdamW'] = train_with_optimizer(AdamW, {'learning_rate': 0.01, 'weight_decay': 0.01})\n",
    "print(f\"AdamW: 最終 Acc = {results['AdamW']['acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終比較\n",
    "print(\"\\n=== 優化器比較總結 ===\")\n",
    "print(f\"{'優化器':<15} {'最終 Loss':<12} {'最終 Acc':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for name in ['SGD', 'Momentum', 'RMSProp', 'Adam', 'AdamW', 'SGD + Cosine']:\n",
    "    if name in results:\n",
    "        print(f\"{name:<15} {results[name]['loss'][-1]:<12.4f} {results[name]['acc'][-1]:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們學習了：\n",
    "\n",
    "### 權重初始化\n",
    "\n",
    "| 方法 | 公式 | 適用場景 |\n",
    "|------|------|----------|\n",
    "| Xavier | $\\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}}$ | Sigmoid, Tanh |\n",
    "| He | $\\sigma = \\sqrt{\\frac{2}{n_{in}}}$ | ReLU |\n",
    "\n",
    "### 優化器\n",
    "\n",
    "| 優化器 | 特點 | 超參數 |\n",
    "|--------|------|--------|\n",
    "| SGD | 最基本 | lr |\n",
    "| Momentum | 累積動量，加速收斂 | lr, β |\n",
    "| RMSProp | 自適應學習率 | lr, β |\n",
    "| Adam | 結合 Momentum 和 RMSProp | lr, β1, β2 |\n",
    "| AdamW | Adam + Decoupled Weight Decay | lr, β1, β2, wd |\n",
    "\n",
    "### Learning Rate Schedule\n",
    "\n",
    "| 方法 | 特點 |\n",
    "|------|------|\n",
    "| Step Decay | 每隔固定 epoch 減少 |\n",
    "| Exponential | 指數衰減 |\n",
    "| Cosine Annealing | 平滑的餘弦衰減 |\n",
    "| Warmup | 先增加後減少 |\n",
    "\n",
    "### 實用建議\n",
    "\n",
    "1. **初學者**：使用 Adam，lr=0.001\n",
    "2. **追求最佳性能**：SGD + Momentum + Cosine Annealing\n",
    "3. **有正則化需求**：AdamW\n",
    "4. **防止梯度爆炸**：使用 Gradient Clipping\n",
    "\n",
    "### 完成 Module 5！\n",
    "\n",
    "至此，我們已經從零實作了：\n",
    "1. 反向傳播基礎\n",
    "2. 全連接層\n",
    "3. 激活函數\n",
    "4. 卷積層\n",
    "5. 池化層\n",
    "6. 完整的 LeNet 網路\n",
    "7. 各種優化技巧\n",
    "\n",
    "接下來的 Module 6 將學習更進階的架構，如 BatchNorm、ResNet、U-Net 等！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
