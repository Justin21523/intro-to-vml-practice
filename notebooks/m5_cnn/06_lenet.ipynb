{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 組裝 LeNet 網路\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 將所有組件組裝成完整的 LeNet-like CNN\n",
    "2. 實作完整的訓練迴圈\n",
    "3. 在簡單資料集上訓練並驗證 loss 下降\n",
    "4. 理解各層的維度變化\n",
    "\n",
    "## LeNet 架構回顧\n",
    "\n",
    "原始 LeNet-5 (LeCun 1998) 的架構：\n",
    "\n",
    "```\n",
    "Input (1, 32, 32)\n",
    "    ↓\n",
    "Conv2D(6, 5x5) → ReLU → MaxPool(2x2)\n",
    "    ↓ (6, 14, 14)\n",
    "Conv2D(16, 5x5) → ReLU → MaxPool(2x2)\n",
    "    ↓ (16, 5, 5)\n",
    "Flatten\n",
    "    ↓ (400,)\n",
    "FC(120) → ReLU\n",
    "    ↓ (120,)\n",
    "FC(84) → ReLU\n",
    "    ↓ (84,)\n",
    "FC(10) → Softmax\n",
    "    ↓ (10,)\n",
    "Output\n",
    "```\n",
    "\n",
    "我們將實作一個簡化版本，適用於 28x28 輸入（如 MNIST 風格的資料）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"LeNet module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：收集所有組件\n",
    "\n",
    "將前面實作的所有層整合在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 層定義 ==================\n",
    "\n",
    "class Conv2D:\n",
    "    \"\"\"2D 卷積層\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        kH, kW = self.kernel_size\n",
    "        std = np.sqrt(2.0 / (in_channels * kH * kW))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kH, kW) * std\n",
    "        self.b = np.zeros(out_channels)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        N, C_in, H, W_in = X.shape\n",
    "        C_out = self.out_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        S, P = self.stride, self.padding\n",
    "        \n",
    "        if P > 0:\n",
    "            X_pad = np.pad(X, ((0,0), (0,0), (P,P), (P,P)), mode='constant')\n",
    "        else:\n",
    "            X_pad = X\n",
    "        \n",
    "        _, _, H_pad, W_pad = X_pad.shape\n",
    "        H_out = (H_pad - kH) // S + 1\n",
    "        W_out = (W_pad - kW) // S + 1\n",
    "        \n",
    "        Y = np.zeros((N, C_out, H_out, W_out))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_s, w_s = i * S, j * S\n",
    "                        Y[n, c_out, i, j] = np.sum(\n",
    "                            X_pad[n, :, h_s:h_s+kH, w_s:w_s+kW] * self.W[c_out]\n",
    "                        ) + self.b[c_out]\n",
    "        \n",
    "        self.cache = (X, X_pad)\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        X, X_pad = self.cache\n",
    "        N, C_in, H, W_in = X.shape\n",
    "        C_out = self.out_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        S, P = self.stride, self.padding\n",
    "        _, _, H_out, W_out = dY.shape\n",
    "        \n",
    "        dX_pad = np.zeros_like(X_pad)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.sum(dY, axis=(0, 2, 3))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_s, w_s = i * S, j * S\n",
    "                        self.dW[c_out] += dY[n, c_out, i, j] * X_pad[n, :, h_s:h_s+kH, w_s:w_s+kW]\n",
    "                        dX_pad[n, :, h_s:h_s+kH, w_s:w_s+kW] += dY[n, c_out, i, j] * self.W[c_out]\n",
    "        \n",
    "        if P > 0:\n",
    "            dX = dX_pad[:, :, P:-P, P:-P]\n",
    "        else:\n",
    "            dX = dX_pad\n",
    "        return dX\n",
    "\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"Max Pooling 層\"\"\"\n",
    "    def __init__(self, pool_size, stride=None):\n",
    "        self.pool_size = pool_size if isinstance(pool_size, tuple) else (pool_size, pool_size)\n",
    "        self.stride = stride if stride else self.pool_size[0]\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        N, C, H, W = X.shape\n",
    "        kH, kW = self.pool_size\n",
    "        S = self.stride\n",
    "        H_out = (H - kH) // S + 1\n",
    "        W_out = (W - kW) // S + 1\n",
    "        \n",
    "        Y = np.zeros((N, C, H_out, W_out))\n",
    "        max_idx = np.zeros((N, C, H_out, W_out, 2), dtype=int)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_s, w_s = i * S, j * S\n",
    "                        window = X[n, c, h_s:h_s+kH, w_s:w_s+kW]\n",
    "                        Y[n, c, i, j] = np.max(window)\n",
    "                        pos = np.unravel_index(np.argmax(window), window.shape)\n",
    "                        max_idx[n, c, i, j] = [h_s + pos[0], w_s + pos[1]]\n",
    "        \n",
    "        self.cache = (X.shape, max_idx)\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        X_shape, max_idx = self.cache\n",
    "        N, C, H_out, W_out = dY.shape\n",
    "        dX = np.zeros(X_shape)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_idx, w_idx = max_idx[n, c, i, j]\n",
    "                        dX[n, c, h_idx, w_idx] += dY[n, c, i, j]\n",
    "        return dX\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"ReLU 激活函數\"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.cache > 0)\n",
    "\n",
    "\n",
    "class Flatten:\n",
    "    \"\"\"展平層\"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = X.shape\n",
    "        return X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        return dY.reshape(self.cache)\n",
    "\n",
    "\n",
    "class FullyConnected:\n",
    "    \"\"\"全連接層\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        std = np.sqrt(2.0 / in_features)\n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        X = self.cache\n",
    "        self.dW = X.T @ dY\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        return dY @ self.W.T\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"數值穩定的 Softmax\"\"\"\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"Softmax + Cross-Entropy Loss\"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, z, y):\n",
    "        N = z.shape[0]\n",
    "        p = softmax(z)\n",
    "        eps = 1e-10\n",
    "        loss = -np.mean(np.log(p[np.arange(N), y] + eps))\n",
    "        self.cache = (p, y)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        p, y = self.cache\n",
    "        N = p.shape[0]\n",
    "        dz = p.copy()\n",
    "        dz[np.arange(N), y] -= 1\n",
    "        return dz / N\n",
    "\n",
    "print(\"所有組件已定義完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：組裝 LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet:\n",
    "    \"\"\"\n",
    "    簡化版 LeNet CNN\n",
    "    \n",
    "    架構（針對 28x28 輸入）：\n",
    "    - Conv(6, 5x5, padding=2) → ReLU → MaxPool(2x2)  # 28→28→14\n",
    "    - Conv(16, 5x5) → ReLU → MaxPool(2x2)            # 14→10→5\n",
    "    - Flatten → FC(120) → ReLU → FC(84) → ReLU → FC(num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape=(1, 28, 28), num_classes=10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple (C, H, W)\n",
    "            輸入影像的形狀\n",
    "        num_classes : int\n",
    "            類別數\n",
    "        \"\"\"\n",
    "        C_in, H, W = input_shape\n",
    "        \n",
    "        # 卷積層\n",
    "        self.conv1 = Conv2D(C_in, 6, 5, padding=2)  # 保持大小\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D(2, stride=2)          # H/2, W/2\n",
    "        \n",
    "        self.conv2 = Conv2D(6, 16, 5, padding=0)     # 縮小 4\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D(2, stride=2)          # H/2, W/2\n",
    "        \n",
    "        # 計算 flatten 後的維度\n",
    "        # 28 → conv1(p=2) → 28 → pool1 → 14 → conv2(p=0) → 10 → pool2 → 5\n",
    "        # 所以 flatten 大小 = 16 * 5 * 5 = 400\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        # 計算實際的 flatten 大小\n",
    "        h1 = (H + 2*2 - 5) // 1 + 1  # conv1 with padding=2\n",
    "        h2 = h1 // 2                  # pool1\n",
    "        h3 = (h2 - 5) // 1 + 1        # conv2\n",
    "        h4 = h3 // 2                  # pool2\n",
    "        flatten_size = 16 * h4 * h4\n",
    "        \n",
    "        # 全連接層\n",
    "        self.fc1 = FullyConnected(flatten_size, 120)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = FullyConnected(120, 84)\n",
    "        self.relu4 = ReLU()\n",
    "        self.fc3 = FullyConnected(84, num_classes)\n",
    "        \n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "        \n",
    "        # 所有層的列表（方便遍歷）\n",
    "        self.layers = [\n",
    "            self.conv1, self.relu1, self.pool1,\n",
    "            self.conv2, self.relu2, self.pool2,\n",
    "            self.flatten,\n",
    "            self.fc1, self.relu3,\n",
    "            self.fc2, self.relu4,\n",
    "            self.fc3\n",
    "        ]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播（返回 logits）\n",
    "        \"\"\"\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        計算損失\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)\n",
    "        return self.loss_fn.forward(logits, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \"\"\"\n",
    "        dout = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"\n",
    "        獲取所有可訓練參數及其梯度\n",
    "        \"\"\"\n",
    "        params_and_grads = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                params_and_grads.append((layer.W, layer.dW))\n",
    "                params_and_grads.append((layer.b, layer.db))\n",
    "        return params_and_grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測類別\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# 測試網路結構\n",
    "net = LeNet(input_shape=(1, 28, 28), num_classes=10)\n",
    "\n",
    "# 前向傳播測試\n",
    "X_test = np.random.randn(2, 1, 28, 28)\n",
    "logits = net.forward(X_test)\n",
    "\n",
    "print(\"=== LeNet 網路結構 ===\")\n",
    "print(f\"輸入: (batch, 1, 28, 28)\")\n",
    "print(f\"\\n各層輸出形狀:\")\n",
    "\n",
    "out = X_test\n",
    "layer_names = [\n",
    "    \"Conv1 (6, 5x5, p=2)\", \"ReLU1\", \"MaxPool1 (2x2)\",\n",
    "    \"Conv2 (16, 5x5)\", \"ReLU2\", \"MaxPool2 (2x2)\",\n",
    "    \"Flatten\",\n",
    "    \"FC1 (120)\", \"ReLU3\",\n",
    "    \"FC2 (84)\", \"ReLU4\",\n",
    "    \"FC3 (10)\"\n",
    "]\n",
    "\n",
    "for layer, name in zip(net.layers, layer_names):\n",
    "    out = layer.forward(out)\n",
    "    print(f\"  {name:25s} → {str(out.shape)}\")\n",
    "\n",
    "print(f\"\\n輸出 (logits) 形狀: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：產生訓練資料\n",
    "\n",
    "為了測試，我們產生一些簡單的合成資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset(n_samples=500, img_size=28, num_classes=4):\n",
    "    \"\"\"\n",
    "    產生簡單的合成分類資料集\n",
    "    \n",
    "    類別:\n",
    "    0: 水平線\n",
    "    1: 垂直線\n",
    "    2: 對角線（左上到右下）\n",
    "    3: 對角線（右上到左下）\n",
    "    \"\"\"\n",
    "    X = np.zeros((n_samples, 1, img_size, img_size))\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        label = i % num_classes\n",
    "        y[i] = label\n",
    "        \n",
    "        # 隨機位置和寬度\n",
    "        pos = np.random.randint(5, img_size - 5)\n",
    "        width = np.random.randint(2, 4)\n",
    "        \n",
    "        if label == 0:  # 水平線\n",
    "            X[i, 0, pos:pos+width, 3:-3] = 1\n",
    "        elif label == 1:  # 垂直線\n",
    "            X[i, 0, 3:-3, pos:pos+width] = 1\n",
    "        elif label == 2:  # 對角線\n",
    "            for j in range(3, img_size - 3):\n",
    "                X[i, 0, j, j] = 1\n",
    "                if width > 1:\n",
    "                    X[i, 0, j, min(j+1, img_size-1)] = 1\n",
    "        else:  # 反對角線\n",
    "            for j in range(3, img_size - 3):\n",
    "                X[i, 0, j, img_size - 1 - j] = 1\n",
    "                if width > 1:\n",
    "                    X[i, 0, j, max(img_size - 2 - j, 0)] = 1\n",
    "        \n",
    "        # 加入雜訊\n",
    "        X[i, 0] += np.random.randn(img_size, img_size) * 0.1\n",
    "    \n",
    "    # 打亂\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 產生資料\n",
    "np.random.seed(42)\n",
    "X_train, y_train = create_synthetic_dataset(n_samples=400, num_classes=4)\n",
    "X_val, y_val = create_synthetic_dataset(n_samples=100, num_classes=4)\n",
    "\n",
    "print(f\"訓練資料: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"驗證資料: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"類別分佈: {np.bincount(y_train)}\")\n",
    "\n",
    "# 視覺化樣本\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "class_names = ['水平線', '垂直線', '對角線', '反對角線']\n",
    "\n",
    "for i in range(4):\n",
    "    # 找到該類別的樣本\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[0, i].imshow(X_train[idx, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Class {i}: {class_names[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # 另一個樣本\n",
    "    idx = np.where(y_train == i)[0][1]\n",
    "    axes[1, i].imshow(X_train[idx, 0], cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Synthetic Dataset Samples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：訓練迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lenet(model, X_train, y_train, X_val, y_val, \n",
    "                epochs=50, batch_size=32, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    訓練 LeNet\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LeNet\n",
    "    X_train, y_train : 訓練資料\n",
    "    X_val, y_val : 驗證資料\n",
    "    epochs : int\n",
    "    batch_size : int\n",
    "    learning_rate : float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history : dict\n",
    "        訓練歷史\n",
    "    \"\"\"\n",
    "    n_samples = len(y_train)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 打亂訓練資料\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Mini-batch 訓練\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Forward\n",
    "            loss = model.loss(X_batch, y_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            # Backward\n",
    "            model.backward()\n",
    "            \n",
    "            # Update (SGD)\n",
    "            for param, grad in model.get_params_and_grads():\n",
    "                param -= learning_rate * grad\n",
    "        \n",
    "        # 計算訓練指標\n",
    "        train_loss = epoch_loss / n_batches\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_acc = np.mean(train_pred == y_train)\n",
    "        \n",
    "        # 計算驗證指標\n",
    "        val_loss = model.loss(X_val, y_val)\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_acc = np.mean(val_pred == y_val)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d}: \"\n",
    "                  f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n",
    "                  f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練（使用較小的資料集以加快速度）\n",
    "print(\"開始訓練 LeNet...\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "model = LeNet(input_shape=(1, 28, 28), num_classes=4)\n",
    "\n",
    "history = train_lenet(\n",
    "    model, X_train, y_train, X_val, y_val,\n",
    "    epochs=50, batch_size=16, learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train')\n",
    "ax.plot(history['val_loss'], label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training & Validation Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_acc'], label='Train')\n",
    "ax.plot(history['val_acc'], label='Validation')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Training & Validation Accuracy')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n最終驗證準確率: {history['val_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：視覺化學到的特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化第一層卷積核\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    kernel = model.conv1.W[i, 0]  # 取第 i 個 filter 的第一個通道\n",
    "    ax.imshow(kernel, cmap='RdBu_r', vmin=-np.abs(kernel).max(), vmax=np.abs(kernel).max())\n",
    "    ax.set_title(f'Conv1 Filter {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Learned Convolutional Filters (Layer 1)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化特徵圖\n",
    "def visualize_feature_maps(model, X_sample):\n",
    "    \"\"\"\n",
    "    視覺化各層的特徵圖\n",
    "    \"\"\"\n",
    "    # 各層的輸出\n",
    "    feature_maps = []\n",
    "    out = X_sample\n",
    "    \n",
    "    for layer in model.layers[:6]:  # 只看卷積/池化層\n",
    "        out = layer.forward(out)\n",
    "        feature_maps.append(out.copy())\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 6, figsize=(15, 8))\n",
    "    \n",
    "    # 原始輸入\n",
    "    axes[0, 0].imshow(X_sample[0, 0], cmap='gray')\n",
    "    axes[0, 0].set_title('Input')\n",
    "    axes[0, 0].axis('off')\n",
    "    for j in range(1, 6):\n",
    "        axes[0, j].axis('off')\n",
    "    \n",
    "    # Conv1 後的特徵圖\n",
    "    for j in range(6):\n",
    "        axes[1, j].imshow(feature_maps[0][0, j], cmap='viridis')\n",
    "        axes[1, j].set_title(f'Conv1 ch{j}')\n",
    "        axes[1, j].axis('off')\n",
    "    \n",
    "    # Pool1 後的特徵圖\n",
    "    for j in range(6):\n",
    "        axes[2, j].imshow(feature_maps[2][0, j], cmap='viridis')\n",
    "        axes[2, j].set_title(f'Pool1 ch{j}')\n",
    "        axes[2, j].axis('off')\n",
    "    \n",
    "    plt.suptitle('Feature Maps at Different Layers', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 對每個類別視覺化一個樣本\n",
    "for class_idx in range(4):\n",
    "    sample_idx = np.where(y_val == class_idx)[0][0]\n",
    "    X_sample = X_val[sample_idx:sample_idx+1]\n",
    "    print(f\"Class {class_idx}: {class_names[class_idx]}\")\n",
    "    visualize_feature_maps(model, X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：加入 Momentum SGD\n",
    "\n",
    "實作 Momentum SGD 優化器，通常能比普通 SGD 更快收斂。\n",
    "\n",
    "$$v_t = \\beta \\cdot v_{t-1} + \\nabla L$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum:\n",
    "    \"\"\"\n",
    "    Momentum SGD 優化器\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocities = {}  # 儲存每個參數的動量\n",
    "    \n",
    "    def update(self, params_and_grads):\n",
    "        \"\"\"\n",
    "        更新參數\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params_and_grads : list of (param, grad) tuples\n",
    "        \"\"\"\n",
    "        for i, (param, grad) in enumerate(params_and_grads):\n",
    "            # 初始化動量\n",
    "            if i not in self.velocities:\n",
    "                self.velocities[i] = np.zeros_like(param)\n",
    "            \n",
    "            # 更新動量\n",
    "            # 解答：v = β * v + grad\n",
    "            self.velocities[i] = self.momentum * self.velocities[i] + grad\n",
    "            \n",
    "            # 更新參數\n",
    "            # 解答：param -= lr * v\n",
    "            param -= self.lr * self.velocities[i]\n",
    "\n",
    "def train_with_momentum(model, X_train, y_train, X_val, y_val,\n",
    "                        epochs=50, batch_size=32, learning_rate=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    使用 Momentum SGD 訓練\n",
    "    \"\"\"\n",
    "    optimizer = SGDMomentum(learning_rate, momentum)\n",
    "    n_samples = len(y_train)\n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            loss = model.loss(X_batch, y_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            model.backward()\n",
    "            optimizer.update(model.get_params_and_grads())\n",
    "        \n",
    "        train_loss = epoch_loss / n_batches\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_acc = np.mean(val_pred == y_val)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:3d}: Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練比較\n",
    "print(\"=== 使用 Momentum SGD 訓練 ===\")\n",
    "np.random.seed(42)\n",
    "model_momentum = LeNet(input_shape=(1, 28, 28), num_classes=4)\n",
    "history_momentum = train_with_momentum(\n",
    "    model_momentum, X_train, y_train, X_val, y_val,\n",
    "    epochs=50, batch_size=16, learning_rate=0.01, momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 SGD vs Momentum SGD\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='SGD')\n",
    "ax.plot(history_momentum['train_loss'], label='Momentum SGD')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history['val_acc'], label='SGD')\n",
    "ax.plot(history_momentum['val_acc'], label='Momentum SGD')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation Accuracy')\n",
    "ax.set_title('Accuracy Comparison')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：計算混淆矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    計算混淆矩陣\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "# 預測\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 混淆矩陣\n",
    "cm = compute_confusion_matrix(y_val, y_pred, num_classes=4)\n",
    "\n",
    "# 視覺化\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# 標籤\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_yticklabels(class_names)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 數值標註\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
    "                color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 每類的準確率\n",
    "print(\"\\n每類別的準確率:\")\n",
    "for i in range(4):\n",
    "    class_acc = cm[i, i] / np.sum(cm[i, :])\n",
    "    print(f\"  {class_names[i]}: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們成功地：\n",
    "\n",
    "1. **組裝了完整的 LeNet CNN**：\n",
    "   - 2 個卷積層 + ReLU + MaxPool\n",
    "   - 3 個全連接層\n",
    "   - Softmax + Cross-Entropy 損失\n",
    "\n",
    "2. **實作了完整的訓練流程**：\n",
    "   - Mini-batch 訓練\n",
    "   - 前向傳播 → 計算損失 → 反向傳播 → 更新參數\n",
    "   - 訓練/驗證指標追蹤\n",
    "\n",
    "3. **驗證了網路能夠學習**：\n",
    "   - Loss 下降\n",
    "   - Accuracy 上升\n",
    "   - 學到了有意義的特徵\n",
    "\n",
    "4. **實作了優化技巧**：\n",
    "   - Momentum SGD\n",
    "\n",
    "### LeNet 架構總結\n",
    "\n",
    "```\n",
    "Input (1, 28, 28)\n",
    "    ↓ Conv(6, 5x5, p=2)\n",
    "(6, 28, 28)\n",
    "    ↓ ReLU → MaxPool(2)\n",
    "(6, 14, 14)\n",
    "    ↓ Conv(16, 5x5)\n",
    "(16, 10, 10)\n",
    "    ↓ ReLU → MaxPool(2)\n",
    "(16, 5, 5)\n",
    "    ↓ Flatten\n",
    "(400,)\n",
    "    ↓ FC(120) → ReLU\n",
    "(120,)\n",
    "    ↓ FC(84) → ReLU\n",
    "(84,)\n",
    "    ↓ FC(num_classes)\n",
    "(num_classes,)\n",
    "```\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們將學習更多優化技巧，包括：\n",
    "- Weight Initialization 的重要性\n",
    "- Learning Rate Schedule\n",
    "- 更進階的優化器（Adam 等）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
