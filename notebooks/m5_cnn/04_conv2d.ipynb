{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Conv2D 卷積層\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解多通道卷積的前向傳播\n",
    "2. **詳細推導**卷積層的反向傳播（這是最難的部分！）\n",
    "3. 實作完整的 Conv2D 類別，包含 forward 和 backward\n",
    "4. 使用 im2col 技巧加速卷積（選做）\n",
    "5. 嚴格的梯度檢驗\n",
    "\n",
    "## 回顧：多通道卷積\n",
    "\n",
    "在 Module 1 我們實作過 2D 卷積。現在要處理**多 batch、多通道**的情況：\n",
    "\n",
    "- **輸入** $X$：形狀 $(N, C_{in}, H, W)$\n",
    "- **卷積核** $W$：形狀 $(C_{out}, C_{in}, k_H, k_W)$\n",
    "- **偏置** $b$：形狀 $(C_{out},)$\n",
    "- **輸出** $Y$：形狀 $(N, C_{out}, H', W')$\n",
    "\n",
    "其中輸出大小：\n",
    "$$H' = \\frac{H + 2P - k_H}{S} + 1$$\n",
    "$$W' = \\frac{W + 2P - k_W}{S} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Conv2D module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：前向傳播\n",
    "\n",
    "### 公式（index 展開）\n",
    "\n",
    "輸出的每個元素：\n",
    "\n",
    "$$Y[n, c_{out}, i, j] = \\sum_{c_{in}=0}^{C_{in}-1} \\sum_{p=0}^{k_H-1} \\sum_{q=0}^{k_W-1} X[n, c_{in}, i \\cdot S + p, j \\cdot S + q] \\cdot W[c_{out}, c_{in}, p, q] + b[c_{out}]$$\n",
    "\n",
    "這看起來很複雜，讓我們一步步理解：\n",
    "1. 對每個輸出位置 $(i, j)$\n",
    "2. 對所有輸入通道 $c_{in}$ 求和\n",
    "3. 對卷積核的每個位置 $(p, q)$ 做點積\n",
    "4. 加上該輸出通道的偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward_naive(X, W, b, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    卷積前向傳播（樸素版本，用於理解）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, C_in, H, W)\n",
    "        輸入\n",
    "    W : np.ndarray, shape (C_out, C_in, kH, kW)\n",
    "        卷積核\n",
    "    b : np.ndarray, shape (C_out,)\n",
    "        偏置\n",
    "    stride : int\n",
    "        步長\n",
    "    padding : int\n",
    "        零填充\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : np.ndarray, shape (N, C_out, H', W')\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = X.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    # 零填充\n",
    "    if padding > 0:\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        X_pad = X\n",
    "    \n",
    "    _, _, H_pad, W_pad = X_pad.shape\n",
    "    \n",
    "    # 輸出大小\n",
    "    H_out = (H_pad - kH) // stride + 1\n",
    "    W_out = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 初始化輸出\n",
    "    Y = np.zeros((N, C_out, H_out, W_out))\n",
    "    \n",
    "    # 6 層迴圈（很慢，但容易理解）\n",
    "    for n in range(N):                    # 對每個樣本\n",
    "        for c_out in range(C_out):        # 對每個輸出通道\n",
    "            for i in range(H_out):        # 對每個輸出高度位置\n",
    "                for j in range(W_out):    # 對每個輸出寬度位置\n",
    "                    # 取出感受野\n",
    "                    h_start = i * stride\n",
    "                    h_end = h_start + kH\n",
    "                    w_start = j * stride\n",
    "                    w_end = w_start + kW\n",
    "                    \n",
    "                    receptive_field = X_pad[n, :, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    # 點積（對所有輸入通道）\n",
    "                    Y[n, c_out, i, j] = np.sum(receptive_field * W[c_out]) + b[c_out]\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# 測試\n",
    "N, C_in, H, W = 2, 3, 5, 5\n",
    "C_out, kH, kW = 4, 3, 3\n",
    "\n",
    "X = np.random.randn(N, C_in, H, W)\n",
    "W_conv = np.random.randn(C_out, C_in, kH, kW)\n",
    "b_conv = np.random.randn(C_out)\n",
    "\n",
    "Y = conv2d_forward_naive(X, W_conv, b_conv, stride=1, padding=0)\n",
    "\n",
    "print(f\"輸入形狀: {X.shape}\")\n",
    "print(f\"卷積核形狀: {W_conv.shape}\")\n",
    "print(f\"偏置形狀: {b_conv.shape}\")\n",
    "print(f\"輸出形狀: {Y.shape}\")\n",
    "print(f\"預期輸出形狀: ({N}, {C_out}, {H-kH+1}, {W-kW+1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：反向傳播（詳細推導）\n",
    "\n",
    "這是 CNN 中最難理解的部分。我們需要計算：\n",
    "- $\\frac{\\partial L}{\\partial X}$（對輸入的梯度）\n",
    "- $\\frac{\\partial L}{\\partial W}$（對卷積核的梯度）\n",
    "- $\\frac{\\partial L}{\\partial b}$（對偏置的梯度）\n",
    "\n",
    "假設我們已知 $\\frac{\\partial L}{\\partial Y}$（記作 $dY$）。\n",
    "\n",
    "### 2.1 對偏置 $b$ 的梯度\n",
    "\n",
    "偏置 $b[c_{out}]$ 影響所有 $Y[:, c_{out}, :, :]$，且影響是直接加 1：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b[c_{out}]} = \\sum_n \\sum_i \\sum_j dY[n, c_{out}, i, j]$$\n",
    "\n",
    "向量形式：對 $dY$ 的 axis=(0, 2, 3) 求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 對卷積核 $W$ 的梯度\n",
    "\n",
    "考慮 $W[c_{out}, c_{in}, p, q]$ 如何影響 $Y$：\n",
    "\n",
    "$$Y[n, c_{out}, i, j] = \\ldots + X[n, c_{in}, i \\cdot S + p, j \\cdot S + q] \\cdot W[c_{out}, c_{in}, p, q] + \\ldots$$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\frac{\\partial Y[n, c_{out}, i, j]}{\\partial W[c_{out}, c_{in}, p, q]} = X[n, c_{in}, i \\cdot S + p, j \\cdot S + q]$$\n",
    "\n",
    "使用 chain rule：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W[c_{out}, c_{in}, p, q]} = \\sum_n \\sum_i \\sum_j dY[n, c_{out}, i, j] \\cdot X[n, c_{in}, i \\cdot S + p, j \\cdot S + q]$$\n",
    "\n",
    "**這其實就是 $dY$ 和 $X$ 之間的「互相關」運算！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 對輸入 $X$ 的梯度（最難的部分！）\n",
    "\n",
    "考慮 $X[n, c_{in}, h, w]$ 如何影響 $Y$。\n",
    "\n",
    "關鍵觀察：$X[n, c_{in}, h, w]$ 會影響**多個**輸出位置！\n",
    "\n",
    "具體來說，如果 $h = i \\cdot S + p$ 且 $w = j \\cdot S + q$，則 $X[n, c_{in}, h, w]$ 參與了 $Y[n, :, i, j]$ 的計算。\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X[n, c_{in}, h, w]} = \\sum_{c_{out}} \\sum_{i, j \\text{ s.t. } (h,w) \\in \\text{receptive field of } (i,j)} dY[n, c_{out}, i, j] \\cdot W[c_{out}, c_{in}, h - i \\cdot S, w - j \\cdot S]$$\n",
    "\n",
    "**這其實是將 $dY$ 與 翻轉後的 $W$ 做「full convolution」！**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = dY \\ast \\text{flip}(W)$$\n",
    "\n",
    "其中 flip 是將 $W$ 旋轉 180 度（沿 kH 和 kW 軸翻轉）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_backward_naive(dY, X, W, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    卷積反向傳播（樸素版本）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dY : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        對輸出的梯度\n",
    "    X : np.ndarray, shape (N, C_in, H, W)\n",
    "        前向傳播時的輸入\n",
    "    W : np.ndarray, shape (C_out, C_in, kH, kW)\n",
    "        卷積核\n",
    "    stride : int\n",
    "    padding : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dX : np.ndarray, shape (N, C_in, H, W)\n",
    "    dW : np.ndarray, shape (C_out, C_in, kH, kW)\n",
    "    db : np.ndarray, shape (C_out,)\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = X.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    _, _, H_out, W_out = dY.shape\n",
    "    \n",
    "    # 零填充輸入\n",
    "    if padding > 0:\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        X_pad = X\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dX_pad = np.zeros_like(X_pad)\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros(C_out)\n",
    "    \n",
    "    # 計算 db：對 dY 的 axis=(0, 2, 3) 求和\n",
    "    db = np.sum(dY, axis=(0, 2, 3))\n",
    "    \n",
    "    # 計算 dW 和 dX\n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    h_start = i * stride\n",
    "                    h_end = h_start + kH\n",
    "                    w_start = j * stride\n",
    "                    w_end = w_start + kW\n",
    "                    \n",
    "                    # dW: 累積 dY * X\n",
    "                    dW[c_out] += dY[n, c_out, i, j] * X_pad[n, :, h_start:h_end, w_start:w_end]\n",
    "                    \n",
    "                    # dX: 累積 dY * W\n",
    "                    dX_pad[n, :, h_start:h_end, w_start:w_end] += dY[n, c_out, i, j] * W[c_out]\n",
    "    \n",
    "    # 移除 padding\n",
    "    if padding > 0:\n",
    "        dX = dX_pad[:, :, padding:-padding, padding:-padding]\n",
    "    else:\n",
    "        dX = dX_pad\n",
    "    \n",
    "    return dX, dW, db\n",
    "\n",
    "# 測試\n",
    "dY = np.random.randn(*Y.shape)\n",
    "dX, dW, db = conv2d_backward_naive(dY, X, W_conv, stride=1, padding=0)\n",
    "\n",
    "print(f\"dY 形狀: {dY.shape}\")\n",
    "print(f\"dX 形狀: {dX.shape} (應與 X 相同: {X.shape})\")\n",
    "print(f\"dW 形狀: {dW.shape} (應與 W 相同: {W_conv.shape})\")\n",
    "print(f\"db 形狀: {db.shape} (應與 b 相同: {b_conv.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：梯度檢驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_conv2d(X, W, b, stride=1, padding=0, eps=1e-5):\n",
    "    \"\"\"\n",
    "    對 Conv2D 進行梯度檢驗\n",
    "    \n",
    "    使用 loss = sum(Y^2) 作為測試損失\n",
    "    \"\"\"\n",
    "    # 前向傳播\n",
    "    Y = conv2d_forward_naive(X, W, b, stride, padding)\n",
    "    \n",
    "    # dL/dY = 2Y\n",
    "    dY = 2 * Y\n",
    "    \n",
    "    # 解析梯度\n",
    "    dX, dW, db = conv2d_backward_naive(dY, X, W, stride, padding)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # === 檢驗 dW ===\n",
    "    print(\"=== 檢驗 dW ===\")\n",
    "    dW_numerical = np.zeros_like(W)\n",
    "    \n",
    "    # 隨機選幾個位置檢驗（全部檢驗太慢）\n",
    "    num_checks = min(20, W.size)\n",
    "    indices = np.random.choice(W.size, num_checks, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, W.shape)\n",
    "        old_val = W[multi_idx]\n",
    "        \n",
    "        W[multi_idx] = old_val + eps\n",
    "        Y_plus = conv2d_forward_naive(X, W, b, stride, padding)\n",
    "        loss_plus = np.sum(Y_plus ** 2)\n",
    "        \n",
    "        W[multi_idx] = old_val - eps\n",
    "        Y_minus = conv2d_forward_naive(X, W, b, stride, padding)\n",
    "        loss_minus = np.sum(Y_minus ** 2)\n",
    "        \n",
    "        W[multi_idx] = old_val\n",
    "        \n",
    "        dW_numerical[multi_idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    # 只比較檢驗過的位置\n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, W.shape)\n",
    "        analytic = dW[multi_idx]\n",
    "        numerical = dW_numerical[multi_idx]\n",
    "        rel_error = abs(analytic - numerical) / (abs(analytic) + abs(numerical) + 1e-8)\n",
    "        if rel_error > 1e-4:\n",
    "            print(f\"  位置 {multi_idx}: 解析={analytic:.6f}, 數值={numerical:.6f}, 誤差={rel_error:.2e} ❌\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"  抽查 {num_checks} 個位置全部通過 ✓\")\n",
    "    \n",
    "    # === 檢驗 db ===\n",
    "    print(\"\\n=== 檢驗 db ===\")\n",
    "    db_numerical = np.zeros_like(b)\n",
    "    \n",
    "    for c in range(len(b)):\n",
    "        old_val = b[c]\n",
    "        \n",
    "        b[c] = old_val + eps\n",
    "        Y_plus = conv2d_forward_naive(X, W, b, stride, padding)\n",
    "        loss_plus = np.sum(Y_plus ** 2)\n",
    "        \n",
    "        b[c] = old_val - eps\n",
    "        Y_minus = conv2d_forward_naive(X, W, b, stride, padding)\n",
    "        loss_minus = np.sum(Y_minus ** 2)\n",
    "        \n",
    "        b[c] = old_val\n",
    "        \n",
    "        db_numerical[c] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(db - db_numerical) / (np.abs(db) + np.abs(db_numerical) + 1e-8))\n",
    "    print(f\"  最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"  通過: {rel_error < 1e-4}\")\n",
    "    if rel_error > 1e-4:\n",
    "        all_passed = False\n",
    "    \n",
    "    # === 檢驗 dX ===\n",
    "    print(\"\\n=== 檢驗 dX ===\")\n",
    "    dX_numerical = np.zeros_like(X)\n",
    "    \n",
    "    num_checks = min(20, X.size)\n",
    "    indices = np.random.choice(X.size, num_checks, replace=False)\n",
    "    X_test = X.copy()\n",
    "    \n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, X.shape)\n",
    "        old_val = X_test[multi_idx]\n",
    "        \n",
    "        X_test[multi_idx] = old_val + eps\n",
    "        Y_plus = conv2d_forward_naive(X_test, W, b, stride, padding)\n",
    "        loss_plus = np.sum(Y_plus ** 2)\n",
    "        \n",
    "        X_test[multi_idx] = old_val - eps\n",
    "        Y_minus = conv2d_forward_naive(X_test, W, b, stride, padding)\n",
    "        loss_minus = np.sum(Y_minus ** 2)\n",
    "        \n",
    "        X_test[multi_idx] = old_val\n",
    "        \n",
    "        dX_numerical[multi_idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, X.shape)\n",
    "        analytic = dX[multi_idx]\n",
    "        numerical = dX_numerical[multi_idx]\n",
    "        rel_error = abs(analytic - numerical) / (abs(analytic) + abs(numerical) + 1e-8)\n",
    "        if rel_error > 1e-4:\n",
    "            print(f\"  位置 {multi_idx}: 解析={analytic:.6f}, 數值={numerical:.6f}, 誤差={rel_error:.2e} ❌\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"  抽查 {num_checks} 個位置全部通過 ✓\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# 使用小規模資料測試\n",
    "np.random.seed(42)\n",
    "X_small = np.random.randn(2, 2, 4, 4)\n",
    "W_small = np.random.randn(3, 2, 2, 2)\n",
    "b_small = np.random.randn(3)\n",
    "\n",
    "passed = gradient_check_conv2d(X_small, W_small, b_small, stride=1, padding=0)\n",
    "print(f\"\\n總體結果: {'全部通過 ✓' if passed else '有錯誤 ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：完整的 Conv2D 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D 卷積層\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        輸入通道數\n",
    "    out_channels : int\n",
    "        輸出通道數\n",
    "    kernel_size : int or tuple\n",
    "        卷積核大小\n",
    "    stride : int\n",
    "        步長\n",
    "    padding : int\n",
    "        零填充\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # He 初始化\n",
    "        kH, kW = self.kernel_size\n",
    "        std = np.sqrt(2.0 / (in_channels * kH * kW))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kH, kW) * std\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # 快取\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, C_in, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : np.ndarray, shape (N, C_out, H', W')\n",
    "        \"\"\"\n",
    "        N, C_in, H, W_in = X.shape\n",
    "        C_out = self.out_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        S = self.stride\n",
    "        P = self.padding\n",
    "        \n",
    "        # 零填充\n",
    "        if P > 0:\n",
    "            X_pad = np.pad(X, ((0, 0), (0, 0), (P, P), (P, P)), mode='constant')\n",
    "        else:\n",
    "            X_pad = X\n",
    "        \n",
    "        _, _, H_pad, W_pad = X_pad.shape\n",
    "        \n",
    "        H_out = (H_pad - kH) // S + 1\n",
    "        W_out = (W_pad - kW) // S + 1\n",
    "        \n",
    "        Y = np.zeros((N, C_out, H_out, W_out))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * S\n",
    "                        w_start = j * S\n",
    "                        receptive = X_pad[n, :, h_start:h_start+kH, w_start:w_start+kW]\n",
    "                        Y[n, c_out, i, j] = np.sum(receptive * self.W[c_out]) + self.b[c_out]\n",
    "        \n",
    "        # 儲存快取\n",
    "        self.cache = (X, X_pad)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dY : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : np.ndarray, shape (N, C_in, H, W)\n",
    "        \"\"\"\n",
    "        X, X_pad = self.cache\n",
    "        N, C_in, H, W_in = X.shape\n",
    "        C_out = self.out_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        S = self.stride\n",
    "        P = self.padding\n",
    "        _, _, H_out, W_out = dY.shape\n",
    "        \n",
    "        dX_pad = np.zeros_like(X_pad)\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.sum(dY, axis=(0, 2, 3))\n",
    "        \n",
    "        for n in range(N):\n",
    "            for c_out in range(C_out):\n",
    "                for i in range(H_out):\n",
    "                    for j in range(W_out):\n",
    "                        h_start = i * S\n",
    "                        w_start = j * S\n",
    "                        \n",
    "                        self.dW[c_out] += dY[n, c_out, i, j] * X_pad[n, :, h_start:h_start+kH, w_start:w_start+kW]\n",
    "                        dX_pad[n, :, h_start:h_start+kH, w_start:w_start+kW] += dY[n, c_out, i, j] * self.W[c_out]\n",
    "        \n",
    "        # 移除 padding\n",
    "        if P > 0:\n",
    "            dX = dX_pad[:, :, P:-P, P:-P]\n",
    "        else:\n",
    "            dX = dX_pad\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Conv2D({self.in_channels}, {self.out_channels}, kernel_size={self.kernel_size}, stride={self.stride}, padding={self.padding})\"\n",
    "\n",
    "# 測試\n",
    "conv = Conv2D(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "print(conv)\n",
    "\n",
    "X_test = np.random.randn(2, 3, 8, 8)\n",
    "Y_test = conv.forward(X_test)\n",
    "print(f\"\\n輸入形狀: {X_test.shape}\")\n",
    "print(f\"輸出形狀: {Y_test.shape}\")\n",
    "print(\"（padding=1 使得輸出大小與輸入相同）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：im2col 加速技巧（選做）\n",
    "\n",
    "im2col (image to column) 是一種將卷積運算轉換為矩陣乘法的技巧，可以利用高度優化的 BLAS 庫來加速。\n",
    "\n",
    "### 基本思想\n",
    "\n",
    "1. 把每個感受野「展開」成一列\n",
    "2. 把卷積核「展開」成一行\n",
    "3. 卷積變成簡單的矩陣乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(X, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    將輸入展開成矩陣，用於加速卷積\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, C, H, W)\n",
    "    kernel_size : int or tuple (kH, kW)\n",
    "    stride : int\n",
    "    padding : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : np.ndarray, shape (N * H_out * W_out, C * kH * kW)\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    if isinstance(kernel_size, int):\n",
    "        kH, kW = kernel_size, kernel_size\n",
    "    else:\n",
    "        kH, kW = kernel_size\n",
    "    \n",
    "    # 零填充\n",
    "    if padding > 0:\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        X_pad = X\n",
    "    \n",
    "    _, _, H_pad, W_pad = X_pad.shape\n",
    "    \n",
    "    H_out = (H_pad - kH) // stride + 1\n",
    "    W_out = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 展開\n",
    "    col = np.zeros((N, C, kH, kW, H_out, W_out))\n",
    "    \n",
    "    for p in range(kH):\n",
    "        p_max = p + stride * H_out\n",
    "        for q in range(kW):\n",
    "            q_max = q + stride * W_out\n",
    "            col[:, :, p, q, :, :] = X_pad[:, :, p:p_max:stride, q:q_max:stride]\n",
    "    \n",
    "    # 重排形狀: (N, C, kH, kW, H_out, W_out) -> (N * H_out * W_out, C * kH * kW)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * H_out * W_out, -1)\n",
    "    \n",
    "    return col\n",
    "\n",
    "def col2im(col, X_shape, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    im2col 的逆操作\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : np.ndarray, shape (N * H_out * W_out, C * kH * kW)\n",
    "    X_shape : tuple (N, C, H, W)\n",
    "    kernel_size : int or tuple\n",
    "    stride : int\n",
    "    padding : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray, shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    if isinstance(kernel_size, int):\n",
    "        kH, kW = kernel_size, kernel_size\n",
    "    else:\n",
    "        kH, kW = kernel_size\n",
    "    \n",
    "    H_pad = H + 2 * padding\n",
    "    W_pad = W + 2 * padding\n",
    "    H_out = (H_pad - kH) // stride + 1\n",
    "    W_out = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 重排形狀\n",
    "    col = col.reshape(N, H_out, W_out, C, kH, kW).transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    X_pad = np.zeros((N, C, H_pad, W_pad))\n",
    "    \n",
    "    for p in range(kH):\n",
    "        p_max = p + stride * H_out\n",
    "        for q in range(kW):\n",
    "            q_max = q + stride * W_out\n",
    "            X_pad[:, :, p:p_max:stride, q:q_max:stride] += col[:, :, p, q, :, :]\n",
    "    \n",
    "    # 移除 padding\n",
    "    if padding > 0:\n",
    "        X = X_pad[:, :, padding:-padding, padding:-padding]\n",
    "    else:\n",
    "        X = X_pad\n",
    "    \n",
    "    return X\n",
    "\n",
    "# 測試 im2col\n",
    "X_test = np.random.randn(2, 3, 4, 4)\n",
    "col = im2col(X_test, kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "print(f\"輸入形狀: {X_test.shape}\")\n",
    "print(f\"im2col 後形狀: {col.shape}\")\n",
    "print(f\"預期形狀: (N * H_out * W_out, C * kH * kW) = (2 * 3 * 3, 3 * 2 * 2) = (18, 12)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DFast:\n",
    "    \"\"\"\n",
    "    使用 im2col 加速的 Conv2D\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        kH, kW = self.kernel_size\n",
    "        std = np.sqrt(2.0 / (in_channels * kH * kW))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kH, kW) * std\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        N, C_in, H, W_in = X.shape\n",
    "        C_out = self.out_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        S = self.stride\n",
    "        P = self.padding\n",
    "        \n",
    "        H_out = (H + 2 * P - kH) // S + 1\n",
    "        W_out = (W_in + 2 * P - kW) // S + 1\n",
    "        \n",
    "        # im2col\n",
    "        col = im2col(X, self.kernel_size, S, P)\n",
    "        \n",
    "        # 展開卷積核: (C_out, C_in, kH, kW) -> (C_out, C_in * kH * kW)\n",
    "        W_row = self.W.reshape(C_out, -1)\n",
    "        \n",
    "        # 矩陣乘法: (N*H_out*W_out, C_in*kH*kW) @ (C_in*kH*kW, C_out) -> (N*H_out*W_out, C_out)\n",
    "        out = col @ W_row.T + self.b\n",
    "        \n",
    "        # 重排形狀\n",
    "        Y = out.reshape(N, H_out, W_out, C_out).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.cache = (X, col)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        X, col = self.cache\n",
    "        N, C_out, H_out, W_out = dY.shape\n",
    "        C_in = self.in_channels\n",
    "        kH, kW = self.kernel_size\n",
    "        \n",
    "        # 展開 dY: (N, C_out, H_out, W_out) -> (N*H_out*W_out, C_out)\n",
    "        dY_col = dY.transpose(0, 2, 3, 1).reshape(-1, C_out)\n",
    "        \n",
    "        # db\n",
    "        self.db = np.sum(dY_col, axis=0)\n",
    "        \n",
    "        # dW: col.T @ dY_col\n",
    "        # col: (N*H_out*W_out, C_in*kH*kW)\n",
    "        # dY_col: (N*H_out*W_out, C_out)\n",
    "        # dW: (C_in*kH*kW, C_out) -> transpose -> (C_out, C_in*kH*kW) -> reshape\n",
    "        dW_flat = col.T @ dY_col  # (C_in*kH*kW, C_out)\n",
    "        self.dW = dW_flat.T.reshape(C_out, C_in, kH, kW)\n",
    "        \n",
    "        # dX: dY_col @ W\n",
    "        W_row = self.W.reshape(C_out, -1)\n",
    "        dcol = dY_col @ W_row  # (N*H_out*W_out, C_in*kH*kW)\n",
    "        \n",
    "        # col2im\n",
    "        dX = col2im(dcol, X.shape, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "# 比較樸素版本和快速版本\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "X_bench = np.random.randn(4, 3, 16, 16)\n",
    "\n",
    "conv_naive = Conv2D(3, 8, 3, stride=1, padding=1)\n",
    "conv_fast = Conv2DFast(3, 8, 3, stride=1, padding=1)\n",
    "conv_fast.W = conv_naive.W.copy()\n",
    "conv_fast.b = conv_naive.b.copy()\n",
    "\n",
    "# 前向傳播比較\n",
    "start = time.time()\n",
    "Y_naive = conv_naive.forward(X_bench)\n",
    "time_naive = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "Y_fast = conv_fast.forward(X_bench)\n",
    "time_fast = time.time() - start\n",
    "\n",
    "print(f\"樸素版本時間: {time_naive*1000:.2f} ms\")\n",
    "print(f\"im2col 版本時間: {time_fast*1000:.2f} ms\")\n",
    "print(f\"加速比: {time_naive / time_fast:.2f}x\")\n",
    "print(f\"輸出差異: {np.max(np.abs(Y_naive - Y_fast)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作帶 dilation 的卷積\n",
    "\n",
    "Dilation（膨脹）讓卷積核「擴張」，可以增加感受野而不增加參數。\n",
    "\n",
    "對於 dilation=d，卷積核相當於在原本的元素之間插入 d-1 個零。\n",
    "\n",
    "**提示**：修改 h_start 和 w_start 的計算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward_dilated(X, W, b, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"\n",
    "    帶 dilation 的卷積前向傳播\n",
    "    \n",
    "    dilation=1 等於普通卷積\n",
    "    dilation=2 表示卷積核元素之間間隔 1 個像素\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = X.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    # 有效卷積核大小\n",
    "    kH_eff = kH + (kH - 1) * (dilation - 1)\n",
    "    kW_eff = kW + (kW - 1) * (dilation - 1)\n",
    "    \n",
    "    # 零填充\n",
    "    if padding > 0:\n",
    "        X_pad = np.pad(X, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    else:\n",
    "        X_pad = X\n",
    "    \n",
    "    _, _, H_pad, W_pad = X_pad.shape\n",
    "    \n",
    "    # 輸出大小\n",
    "    H_out = (H_pad - kH_eff) // stride + 1\n",
    "    W_out = (W_pad - kW_eff) // stride + 1\n",
    "    \n",
    "    Y = np.zeros((N, C_out, H_out, W_out))\n",
    "    \n",
    "    # 解答：修改卷積邏輯\n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    # 計算起始位置\n",
    "                    h_start = i * stride\n",
    "                    w_start = j * stride\n",
    "                    \n",
    "                    # 對卷積核的每個位置\n",
    "                    total = 0\n",
    "                    for p in range(kH):\n",
    "                        for q in range(kW):\n",
    "                            # 考慮 dilation 的偏移\n",
    "                            h_idx = h_start + p * dilation\n",
    "                            w_idx = w_start + q * dilation\n",
    "                            \n",
    "                            for c_in in range(C_in):\n",
    "                                total += X_pad[n, c_in, h_idx, w_idx] * W[c_out, c_in, p, q]\n",
    "                    \n",
    "                    Y[n, c_out, i, j] = total + b[c_out]\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# 測試\n",
    "X_test = np.random.randn(1, 1, 7, 7)\n",
    "W_test = np.random.randn(1, 1, 3, 3)\n",
    "b_test = np.zeros(1)\n",
    "\n",
    "# 普通卷積\n",
    "Y_d1 = conv2d_forward_dilated(X_test, W_test, b_test, dilation=1)\n",
    "print(f\"dilation=1: 輸入 {X_test.shape} -> 輸出 {Y_d1.shape}\")\n",
    "\n",
    "# 膨脹卷積\n",
    "Y_d2 = conv2d_forward_dilated(X_test, W_test, b_test, dilation=2)\n",
    "print(f\"dilation=2: 輸入 {X_test.shape} -> 輸出 {Y_d2.shape}\")\n",
    "\n",
    "Y_d3 = conv2d_forward_dilated(X_test, W_test, b_test, dilation=3)\n",
    "print(f\"dilation=3: 輸入 {X_test.shape} -> 輸出 {Y_d3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 dilation 效果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# 產生測試卷積核\n",
    "kernel = np.array([[1, 0, 1],\n",
    "                   [0, 1, 0],\n",
    "                   [1, 0, 1]])\n",
    "\n",
    "dilations = [1, 2, 3]\n",
    "\n",
    "for idx, d in enumerate(dilations):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # 產生膨脹後的卷積核（用於視覺化）\n",
    "    kH, kW = kernel.shape\n",
    "    kH_eff = kH + (kH - 1) * (d - 1)\n",
    "    kW_eff = kW + (kW - 1) * (d - 1)\n",
    "    \n",
    "    kernel_dilated = np.zeros((kH_eff, kW_eff))\n",
    "    for i in range(kH):\n",
    "        for j in range(kW):\n",
    "            kernel_dilated[i * d, j * d] = kernel[i, j]\n",
    "    \n",
    "    ax.imshow(kernel_dilated, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'dilation={d} (effective size: {kH_eff}x{kW_eff})')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # 標記原始卷積核位置\n",
    "    for i in range(kH):\n",
    "        for j in range(kW):\n",
    "            if kernel[i, j] > 0:\n",
    "                ax.text(j * d, i * d, '1', ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.suptitle('Dilated Convolution Kernels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：驗證反向傳播的「full convolution」解釋\n",
    "\n",
    "對輸入的梯度 $dX$ 可以理解為 $dY$ 與翻轉的 $W$ 做 full convolution。\n",
    "\n",
    "**任務**：直接用 full convolution 計算 dX，與我們的 backward 結果比較。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_conv2d(X, W):\n",
    "    \"\"\"\n",
    "    Full convolution（輸出比輸入大）\n",
    "    \n",
    "    等效於 padding = kernel_size - 1 的卷積\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = X.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    # Full convolution 的 padding\n",
    "    pad_h = kH - 1\n",
    "    pad_w = kW - 1\n",
    "    \n",
    "    X_pad = np.pad(X, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n",
    "    \n",
    "    H_out = H + kH - 1\n",
    "    W_out = W_in + kW - 1\n",
    "    \n",
    "    Y = np.zeros((N, C_out, H_out, W_out))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            for i in range(H_out):\n",
    "                for j in range(W_out):\n",
    "                    receptive = X_pad[n, :, i:i+kH, j:j+kW]\n",
    "                    Y[n, c_out, i, j] = np.sum(receptive * W[c_out])\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# 驗證 dX 的 full convolution 解釋\n",
    "np.random.seed(42)\n",
    "N, C_in, H, W_in = 1, 1, 4, 4\n",
    "C_out, kH, kW = 1, 2, 2\n",
    "\n",
    "X = np.random.randn(N, C_in, H, W_in)\n",
    "W_conv = np.random.randn(C_out, C_in, kH, kW)\n",
    "b_conv = np.zeros(C_out)\n",
    "\n",
    "# 前向傳播\n",
    "Y = conv2d_forward_naive(X, W_conv, b_conv)\n",
    "dY = np.random.randn(*Y.shape)\n",
    "\n",
    "# 使用 backward 計算 dX\n",
    "dX_backward, _, _ = conv2d_backward_naive(dY, X, W_conv)\n",
    "\n",
    "# 使用 full convolution 計算 dX\n",
    "# dX = dY * flip(W)，但要處理通道\n",
    "# 翻轉 W\n",
    "W_flipped = np.flip(np.flip(W_conv, axis=2), axis=3)\n",
    "\n",
    "# 注意：這裡簡化為單通道情況\n",
    "# 對於多通道，需要額外處理\n",
    "dX_full = full_conv2d(dY, W_flipped)\n",
    "\n",
    "print(f\"dX from backward: shape = {dX_backward.shape}\")\n",
    "print(dX_backward[0, 0])\n",
    "\n",
    "print(f\"\\ndX from full conv: shape = {dX_full.shape}\")\n",
    "print(dX_full[0, 0])\n",
    "\n",
    "print(f\"\\n差異: {np.max(np.abs(dX_backward - dX_full)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們深入學習了 Conv2D 層的實作：\n",
    "\n",
    "### 前向傳播\n",
    "$$Y[n, c_{out}, i, j] = \\sum_{c_{in}} \\sum_{p, q} X[n, c_{in}, i \\cdot S + p, j \\cdot S + q] \\cdot W[c_{out}, c_{in}, p, q] + b[c_{out}]$$\n",
    "\n",
    "### 反向傳播\n",
    "\n",
    "| 梯度 | 公式 | 解釋 |\n",
    "|------|------|------|\n",
    "| $db$ | $\\sum_{n,i,j} dY[n,:,i,j]$ | 對空間位置和樣本求和 |\n",
    "| $dW$ | $dY \\star X$ | dY 和 X 的互相關 |\n",
    "| $dX$ | $dY \\ast \\text{flip}(W)$ | dY 和翻轉 W 的 full convolution |\n",
    "\n",
    "### im2col 加速\n",
    "- 將卷積轉換為矩陣乘法\n",
    "- 可利用高度優化的 BLAS 庫\n",
    "- 空間換時間（需要額外記憶體存 col）\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們將實作 **Pooling 層**，這是 CNN 中另一個重要的組件。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
