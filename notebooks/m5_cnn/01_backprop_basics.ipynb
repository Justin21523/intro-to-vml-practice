{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 反向傳播基礎 Backpropagation Basics\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解計算圖 (Computational Graph) 的概念\n",
    "2. 掌握反向傳播中的連鎖律 (Chain Rule)\n",
    "3. 從一維標量例子到向量化版本的梯度推導\n",
    "4. 使用數值微分驗證解析梯度\n",
    "\n",
    "## 為什麼反向傳播重要？\n",
    "\n",
    "神經網路的訓練核心就是**最小化損失函數**。要做到這點，我們需要知道損失函數對每個參數的梯度，然後沿著梯度的反方向更新參數。\n",
    "\n",
    "反向傳播 (Backpropagation) 是一個高效計算這些梯度的演算法，它利用 **chain rule** 從輸出一路往回傳遞梯度到每個參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Backpropagation Basics loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：計算圖 Computational Graph\n",
    "\n",
    "### 什麼是計算圖？\n",
    "\n",
    "計算圖是一種將數學運算表示為有向圖的方式：\n",
    "- **節點 (Node)**: 代表變數或操作\n",
    "- **邊 (Edge)**: 代表資料流動方向\n",
    "\n",
    "### 簡單例子\n",
    "\n",
    "考慮一個簡單的函數：$f(x, y, z) = (x + y) \\cdot z$\n",
    "\n",
    "我們可以拆解成兩步：\n",
    "1. $q = x + y$ (加法)\n",
    "2. $f = q \\cdot z$ (乘法)\n",
    "\n",
    "```\n",
    "     x ──┐\n",
    "         ├──(+)── q ──┐\n",
    "     y ──┘            ├──(×)── f\n",
    "     z ───────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向傳播 Forward Pass\n",
    "def forward_example(x, y, z):\n",
    "    \"\"\"\n",
    "    計算 f(x, y, z) = (x + y) * z\n",
    "    同時回傳中間變數供反向傳播使用\n",
    "    \"\"\"\n",
    "    q = x + y       # 加法\n",
    "    f = q * z       # 乘法\n",
    "    return f, q     # 回傳輸出和中間值\n",
    "\n",
    "# 測試\n",
    "x, y, z = 2.0, 3.0, 4.0\n",
    "f, q = forward_example(x, y, z)\n",
    "print(f\"x = {x}, y = {y}, z = {z}\")\n",
    "print(f\"q = x + y = {q}\")\n",
    "print(f\"f = q * z = {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：Chain Rule 連鎖律\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "如果 $y = g(x)$ 且 $z = f(y)$，則：\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "**直觀理解**：梯度是「局部梯度」乘以「上游梯度」\n",
    "\n",
    "### 在計算圖上的應用\n",
    "\n",
    "對於上面的例子 $f = (x + y) \\cdot z$：\n",
    "\n",
    "**前向傳播**：\n",
    "- $q = x + y$\n",
    "- $f = q \\cdot z$\n",
    "\n",
    "**反向傳播**（假設我們要計算 $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$）：\n",
    "\n",
    "1. **從輸出開始**：$\\frac{\\partial f}{\\partial f} = 1$\n",
    "\n",
    "2. **乘法節點**（$f = q \\cdot z$）的局部梯度：\n",
    "   - $\\frac{\\partial f}{\\partial q} = z$\n",
    "   - $\\frac{\\partial f}{\\partial z} = q$\n",
    "\n",
    "3. **加法節點**（$q = x + y$）的局部梯度：\n",
    "   - $\\frac{\\partial q}{\\partial x} = 1$\n",
    "   - $\\frac{\\partial q}{\\partial y} = 1$\n",
    "\n",
    "4. **鏈式法則**：\n",
    "   - $\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x} = z \\cdot 1 = z$\n",
    "   - $\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial y} = z \\cdot 1 = z$\n",
    "   - $\\frac{\\partial f}{\\partial z} = q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向傳播 Backward Pass\n",
    "def backward_example(x, y, z, q):\n",
    "    \"\"\"\n",
    "    計算 f = (x + y) * z 對 x, y, z 的梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y, z : 輸入值\n",
    "    q : 中間值 (x + y)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_dx, df_dy, df_dz : 梯度\n",
    "    \"\"\"\n",
    "    # 從輸出開始，初始梯度為 1\n",
    "    df_df = 1.0\n",
    "    \n",
    "    # 乘法節點的反向傳播\n",
    "    # f = q * z\n",
    "    # ∂f/∂q = z, ∂f/∂z = q\n",
    "    df_dq = z * df_df\n",
    "    df_dz = q * df_df\n",
    "    \n",
    "    # 加法節點的反向傳播\n",
    "    # q = x + y\n",
    "    # ∂q/∂x = 1, ∂q/∂y = 1\n",
    "    df_dx = 1.0 * df_dq\n",
    "    df_dy = 1.0 * df_dq\n",
    "    \n",
    "    return df_dx, df_dy, df_dz\n",
    "\n",
    "# 測試\n",
    "df_dx, df_dy, df_dz = backward_example(x, y, z, q)\n",
    "print(f\"∂f/∂x = {df_dx} (should be z = {z})\")\n",
    "print(f\"∂f/∂y = {df_dy} (should be z = {z})\")\n",
    "print(f\"∂f/∂z = {df_dz} (should be q = {q})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數值驗證\n",
    "\n",
    "使用數值微分來驗證我們的解析梯度是否正確：\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} \\approx \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    計算標量函數 f 對 x 的數值梯度（中央差分）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        標量函數\n",
    "    x : float\n",
    "        求梯度的點\n",
    "    eps : float\n",
    "        微小擾動\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : float\n",
    "        數值梯度\n",
    "    \"\"\"\n",
    "    return (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "# 驗證 ∂f/∂x\n",
    "f_x = lambda x_: (x_ + y) * z\n",
    "num_grad_x = numerical_gradient(f_x, x)\n",
    "print(f\"解析梯度 ∂f/∂x = {df_dx}\")\n",
    "print(f\"數值梯度 ∂f/∂x ≈ {num_grad_x}\")\n",
    "print(f\"相對誤差: {abs(df_dx - num_grad_x) / (abs(df_dx) + 1e-8):.2e}\")\n",
    "\n",
    "# 驗證 ∂f/∂y\n",
    "f_y = lambda y_: (x + y_) * z\n",
    "num_grad_y = numerical_gradient(f_y, y)\n",
    "print(f\"\\n解析梯度 ∂f/∂y = {df_dy}\")\n",
    "print(f\"數值梯度 ∂f/∂y ≈ {num_grad_y}\")\n",
    "\n",
    "# 驗證 ∂f/∂z\n",
    "f_z = lambda z_: (x + y) * z_\n",
    "num_grad_z = numerical_gradient(f_z, z)\n",
    "print(f\"\\n解析梯度 ∂f/∂z = {df_dz}\")\n",
    "print(f\"數值梯度 ∂f/∂z ≈ {num_grad_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：線性回歸的反向傳播\n",
    "\n",
    "現在讓我們用一個更實際的例子：線性回歸的梯度計算。\n",
    "\n",
    "### 模型\n",
    "\n",
    "$$\\hat{y} = wx + b$$\n",
    "\n",
    "### 損失函數（單一樣本）\n",
    "\n",
    "$$L = (\\hat{y} - y)^2 = (wx + b - y)^2$$\n",
    "\n",
    "### 計算圖\n",
    "\n",
    "```\n",
    "w ──┐\n",
    "    ├──(×)── p ──┐\n",
    "x ──┘            ├──(+)── q ──┐\n",
    "b ───────────────┘            ├──(-)── r ──(²)── L\n",
    "y ────────────────────────────┘\n",
    "```\n",
    "\n",
    "其中：\n",
    "- $p = wx$\n",
    "- $q = p + b = wx + b = \\hat{y}$\n",
    "- $r = q - y = \\hat{y} - y$\n",
    "- $L = r^2$\n",
    "\n",
    "### 手推梯度\n",
    "\n",
    "使用 chain rule：\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial r} = 2r = 2(\\hat{y} - y)$\n",
    "\n",
    "2. $\\frac{\\partial L}{\\partial q} = \\frac{\\partial L}{\\partial r} \\cdot \\frac{\\partial r}{\\partial q} = 2r \\cdot 1 = 2(\\hat{y} - y)$\n",
    "\n",
    "3. $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial q} \\cdot \\frac{\\partial q}{\\partial b} = 2(\\hat{y} - y) \\cdot 1 = 2(\\hat{y} - y)$\n",
    "\n",
    "4. $\\frac{\\partial L}{\\partial p} = \\frac{\\partial L}{\\partial q} \\cdot \\frac{\\partial q}{\\partial p} = 2(\\hat{y} - y) \\cdot 1 = 2(\\hat{y} - y)$\n",
    "\n",
    "5. $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial p} \\cdot \\frac{\\partial p}{\\partial w} = 2(\\hat{y} - y) \\cdot x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionBackprop:\n",
    "    \"\"\"\n",
    "    用計算圖方式實作線性回歸的前向/反向傳播\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 初始化參數\n",
    "        self.w = np.random.randn()\n",
    "        self.b = np.random.randn()\n",
    "        \n",
    "        # 梯度\n",
    "        self.dw = 0.0\n",
    "        self.db = 0.0\n",
    "        \n",
    "        # 快取（給 backward 用）\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "            輸入\n",
    "        y : float\n",
    "            真實標籤\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            MSE 損失\n",
    "        \"\"\"\n",
    "        # 計算預測值\n",
    "        p = self.w * x      # p = wx\n",
    "        q = p + self.b      # q = wx + b = y_hat\n",
    "        r = q - y           # r = y_hat - y\n",
    "        L = r ** 2          # L = (y_hat - y)^2\n",
    "        \n",
    "        # 儲存中間值供 backward 使用\n",
    "        self.cache = {'x': x, 'y': y, 'p': p, 'q': q, 'r': r}\n",
    "        \n",
    "        return L\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向傳播，計算 dw 和 db\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        r = self.cache['r']\n",
    "        \n",
    "        # 從輸出往回傳播\n",
    "        dL_dL = 1.0\n",
    "        \n",
    "        # L = r^2\n",
    "        # ∂L/∂r = 2r\n",
    "        dL_dr = 2 * r * dL_dL\n",
    "        \n",
    "        # r = q - y\n",
    "        # ∂r/∂q = 1\n",
    "        dL_dq = 1.0 * dL_dr\n",
    "        \n",
    "        # q = p + b\n",
    "        # ∂q/∂p = 1, ∂q/∂b = 1\n",
    "        dL_dp = 1.0 * dL_dq\n",
    "        dL_db = 1.0 * dL_dq\n",
    "        \n",
    "        # p = w * x\n",
    "        # ∂p/∂w = x\n",
    "        dL_dw = x * dL_dp\n",
    "        \n",
    "        self.dw = dL_dw\n",
    "        self.db = dL_db\n",
    "        \n",
    "        return self.dw, self.db\n",
    "\n",
    "# 測試\n",
    "model = LinearRegressionBackprop()\n",
    "model.w = 2.0\n",
    "model.b = 1.0\n",
    "\n",
    "x, y = 3.0, 10.0  # 真實值：2*3 + 1 = 7，但我們設 y=10\n",
    "loss = model.forward(x, y)\n",
    "dw, db = model.backward()\n",
    "\n",
    "print(f\"w = {model.w}, b = {model.b}\")\n",
    "print(f\"x = {x}, y = {y}\")\n",
    "print(f\"y_hat = w*x + b = {model.w * x + model.b}\")\n",
    "print(f\"Loss = (y_hat - y)^2 = {loss}\")\n",
    "print(f\"\\n解析梯度：\")\n",
    "print(f\"∂L/∂w = {dw}\")\n",
    "print(f\"∂L/∂b = {db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數值驗證\n",
    "eps = 1e-5\n",
    "\n",
    "# 驗證 dw\n",
    "model_plus = LinearRegressionBackprop()\n",
    "model_plus.w = model.w + eps\n",
    "model_plus.b = model.b\n",
    "loss_plus = model_plus.forward(x, y)\n",
    "\n",
    "model_minus = LinearRegressionBackprop()\n",
    "model_minus.w = model.w - eps\n",
    "model_minus.b = model.b\n",
    "loss_minus = model_minus.forward(x, y)\n",
    "\n",
    "num_dw = (loss_plus - loss_minus) / (2 * eps)\n",
    "print(f\"解析 ∂L/∂w = {dw}\")\n",
    "print(f\"數值 ∂L/∂w ≈ {num_dw}\")\n",
    "print(f\"相對誤差: {abs(dw - num_dw) / (abs(dw) + 1e-8):.2e}\")\n",
    "\n",
    "# 驗證 db\n",
    "model_plus.w = model.w\n",
    "model_plus.b = model.b + eps\n",
    "loss_plus = model_plus.forward(x, y)\n",
    "\n",
    "model_minus.w = model.w\n",
    "model_minus.b = model.b - eps\n",
    "loss_minus = model_minus.forward(x, y)\n",
    "\n",
    "num_db = (loss_plus - loss_minus) / (2 * eps)\n",
    "print(f\"\\n解析 ∂L/∂b = {db}\")\n",
    "print(f\"數值 ∂L/∂b ≈ {num_db}\")\n",
    "print(f\"相對誤差: {abs(db - num_db) / (abs(db) + 1e-8):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：向量化版本\n",
    "\n",
    "在實際的神經網路中，我們處理的是**向量和矩陣**，不是標量。讓我們推導向量化版本的梯度。\n",
    "\n",
    "### 設定\n",
    "\n",
    "- **輸入**: $X$ 的形狀是 $(N, D)$，其中 $N$ 是樣本數，$D$ 是特徵維度\n",
    "- **權重**: $W$ 的形狀是 $(D, M)$，其中 $M$ 是輸出維度\n",
    "- **偏置**: $b$ 的形狀是 $(M,)$\n",
    "- **輸出**: $Y = XW + b$，形狀是 $(N, M)$\n",
    "\n",
    "### 損失函數（以 MSE 為例）\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} (Y_{ij} - T_{ij})^2$$\n",
    "\n",
    "其中 $T$ 是目標值。\n",
    "\n",
    "### 梯度推導\n",
    "\n",
    "假設我們已經知道 $\\frac{\\partial L}{\\partial Y}$（形狀 $(N, M)$），我們要計算：\n",
    "- $\\frac{\\partial L}{\\partial X}$（形狀 $(N, D)$）\n",
    "- $\\frac{\\partial L}{\\partial W}$（形狀 $(D, M)$）\n",
    "- $\\frac{\\partial L}{\\partial b}$（形狀 $(M,)$）\n",
    "\n",
    "#### 推導方式：從單一元素開始\n",
    "\n",
    "考慮 $Y_{ij} = \\sum_k X_{ik} W_{kj} + b_j$\n",
    "\n",
    "**對 W 的梯度**：\n",
    "$$\\frac{\\partial L}{\\partial W_{kj}} = \\sum_i \\frac{\\partial L}{\\partial Y_{ij}} \\cdot \\frac{\\partial Y_{ij}}{\\partial W_{kj}} = \\sum_i \\frac{\\partial L}{\\partial Y_{ij}} \\cdot X_{ik}$$\n",
    "\n",
    "寫成矩陣形式：$\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "**對 X 的梯度**：\n",
    "$$\\frac{\\partial L}{\\partial X_{ik}} = \\sum_j \\frac{\\partial L}{\\partial Y_{ij}} \\cdot \\frac{\\partial Y_{ij}}{\\partial X_{ik}} = \\sum_j \\frac{\\partial L}{\\partial Y_{ij}} \\cdot W_{kj}$$\n",
    "\n",
    "寫成矩陣形式：$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$\n",
    "\n",
    "**對 b 的梯度**：\n",
    "$$\\frac{\\partial L}{\\partial b_j} = \\sum_i \\frac{\\partial L}{\\partial Y_{ij}}$$\n",
    "\n",
    "寫成向量形式：$\\frac{\\partial L}{\\partial b} = \\sum_i \\frac{\\partial L}{\\partial Y}$（對 axis=0 求和）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    \"\"\"\n",
    "    全連接層的前向/反向傳播實作\n",
    "    \n",
    "    Y = XW + b\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            輸入維度 D\n",
    "        out_features : int\n",
    "            輸出維度 M\n",
    "        \"\"\"\n",
    "        # Xavier 初始化\n",
    "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
    "        self.b = np.zeros(out_features)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # 快取\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "            輸入資料\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : np.ndarray, shape (N, M)\n",
    "            輸出\n",
    "        \"\"\"\n",
    "        self.cache = X\n",
    "        Y = X @ self.W + self.b\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dY : np.ndarray, shape (N, M)\n",
    "            損失對輸出的梯度 ∂L/∂Y\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : np.ndarray, shape (N, D)\n",
    "            損失對輸入的梯度 ∂L/∂X\n",
    "        \"\"\"\n",
    "        X = self.cache\n",
    "        \n",
    "        # ∂L/∂W = X^T @ dY\n",
    "        self.dW = X.T @ dY\n",
    "        \n",
    "        # ∂L/∂b = sum(dY, axis=0)\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        \n",
    "        # ∂L/∂X = dY @ W^T\n",
    "        dX = dY @ self.W.T\n",
    "        \n",
    "        return dX\n",
    "\n",
    "# 測試\n",
    "N, D, M = 4, 3, 2\n",
    "X = np.random.randn(N, D)\n",
    "fc = FullyConnectedLayer(D, M)\n",
    "\n",
    "# 前向傳播\n",
    "Y = fc.forward(X)\n",
    "print(f\"輸入 X 形狀: {X.shape}\")\n",
    "print(f\"權重 W 形狀: {fc.W.shape}\")\n",
    "print(f\"偏置 b 形狀: {fc.b.shape}\")\n",
    "print(f\"輸出 Y 形狀: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度檢驗\n",
    "def gradient_check_fc(layer, X, eps=1e-5):\n",
    "    \"\"\"\n",
    "    對全連接層進行梯度檢驗\n",
    "    \"\"\"\n",
    "    # 前向傳播，使用簡單的 MSE loss\n",
    "    Y = layer.forward(X)\n",
    "    \n",
    "    # 假設 loss = sum(Y^2)，則 dY = 2Y\n",
    "    loss = np.sum(Y ** 2)\n",
    "    dY = 2 * Y\n",
    "    \n",
    "    # 反向傳播\n",
    "    dX = layer.backward(dY)\n",
    "    \n",
    "    # 數值驗證 dW\n",
    "    print(\"=== 驗證 dW ===\")\n",
    "    num_dW = np.zeros_like(layer.W)\n",
    "    for i in range(layer.W.shape[0]):\n",
    "        for j in range(layer.W.shape[1]):\n",
    "            # W + eps\n",
    "            layer.W[i, j] += eps\n",
    "            Y_plus = layer.forward(X)\n",
    "            loss_plus = np.sum(Y_plus ** 2)\n",
    "            \n",
    "            # W - eps\n",
    "            layer.W[i, j] -= 2 * eps\n",
    "            Y_minus = layer.forward(X)\n",
    "            loss_minus = np.sum(Y_minus ** 2)\n",
    "            \n",
    "            # 恢復\n",
    "            layer.W[i, j] += eps\n",
    "            \n",
    "            num_dW[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff = np.abs(layer.dW - num_dW)\n",
    "    rel_error = np.max(diff / (np.abs(layer.dW) + np.abs(num_dW) + 1e-8))\n",
    "    print(f\"dW 相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"驗證通過: {rel_error < 1e-5}\")\n",
    "    \n",
    "    # 數值驗證 db\n",
    "    print(\"\\n=== 驗證 db ===\")\n",
    "    num_db = np.zeros_like(layer.b)\n",
    "    for j in range(layer.b.shape[0]):\n",
    "        layer.b[j] += eps\n",
    "        Y_plus = layer.forward(X)\n",
    "        loss_plus = np.sum(Y_plus ** 2)\n",
    "        \n",
    "        layer.b[j] -= 2 * eps\n",
    "        Y_minus = layer.forward(X)\n",
    "        loss_minus = np.sum(Y_minus ** 2)\n",
    "        \n",
    "        layer.b[j] += eps\n",
    "        num_db[j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff = np.abs(layer.db - num_db)\n",
    "    rel_error = np.max(diff / (np.abs(layer.db) + np.abs(num_db) + 1e-8))\n",
    "    print(f\"db 相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"驗證通過: {rel_error < 1e-5}\")\n",
    "    \n",
    "    # 數值驗證 dX\n",
    "    print(\"\\n=== 驗證 dX ===\")\n",
    "    num_dX = np.zeros_like(X)\n",
    "    X_test = X.copy()\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X_test[i, j] += eps\n",
    "            Y_plus = layer.forward(X_test)\n",
    "            loss_plus = np.sum(Y_plus ** 2)\n",
    "            \n",
    "            X_test[i, j] -= 2 * eps\n",
    "            Y_minus = layer.forward(X_test)\n",
    "            loss_minus = np.sum(Y_minus ** 2)\n",
    "            \n",
    "            X_test[i, j] += eps\n",
    "            num_dX[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff = np.abs(dX - num_dX)\n",
    "    rel_error = np.max(diff / (np.abs(dX) + np.abs(num_dX) + 1e-8))\n",
    "    print(f\"dX 相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"驗證通過: {rel_error < 1e-5}\")\n",
    "\n",
    "gradient_check_fc(fc, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：常見操作的梯度\n",
    "\n",
    "在神經網路中，有幾種常見操作的梯度模式值得記住：\n",
    "\n",
    "### 加法節點\n",
    "\n",
    "$f = x + y$\n",
    "- $\\frac{\\partial f}{\\partial x} = 1$\n",
    "- $\\frac{\\partial f}{\\partial y} = 1$\n",
    "\n",
    "**直觀理解**：梯度「平均分配」給兩個輸入\n",
    "\n",
    "### 乘法節點\n",
    "\n",
    "$f = x \\cdot y$\n",
    "- $\\frac{\\partial f}{\\partial x} = y$\n",
    "- $\\frac{\\partial f}{\\partial y} = x$\n",
    "\n",
    "**直觀理解**：梯度「交換」\n",
    "\n",
    "### Max 節點\n",
    "\n",
    "$f = \\max(x, y)$\n",
    "- 梯度只流向較大的那個輸入\n",
    "- 這就是 ReLU 和 MaxPooling 梯度的原理\n",
    "\n",
    "### 複製節點（多輸出）\n",
    "\n",
    "如果一個變數被用在多個地方，梯度要**相加**。\n",
    "\n",
    "例如 $f = x + x^2$，可以看成 $a = x$, $b = x$, $f = a + b^2$\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial a} + \\frac{\\partial f}{\\partial b} \\cdot \\frac{\\partial b^2}{\\partial b} = 1 + 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化不同操作的梯度流\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 加法節點\n",
    "ax = axes[0]\n",
    "ax.text(0.2, 0.7, 'x', fontsize=16, ha='center')\n",
    "ax.text(0.2, 0.3, 'y', fontsize=16, ha='center')\n",
    "ax.text(0.5, 0.5, '+', fontsize=20, ha='center', \n",
    "        bbox=dict(boxstyle='circle', facecolor='lightblue'))\n",
    "ax.text(0.8, 0.5, 'f', fontsize=16, ha='center')\n",
    "\n",
    "ax.annotate('', xy=(0.43, 0.57), xytext=(0.25, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.43, 0.43), xytext=(0.25, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.75, 0.5), xytext=(0.57, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "\n",
    "# 反向箭頭\n",
    "ax.annotate('dout', xy=(0.57, 0.45), xytext=(0.75, 0.35),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red')\n",
    "ax.annotate('dout', xy=(0.25, 0.65), xytext=(0.43, 0.57),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n",
    "ax.annotate('dout', xy=(0.25, 0.35), xytext=(0.43, 0.43),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('加法節點: 梯度平均分配', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 乘法節點\n",
    "ax = axes[1]\n",
    "ax.text(0.2, 0.7, 'x', fontsize=16, ha='center')\n",
    "ax.text(0.2, 0.3, 'y', fontsize=16, ha='center')\n",
    "ax.text(0.5, 0.5, '×', fontsize=20, ha='center',\n",
    "        bbox=dict(boxstyle='circle', facecolor='lightgreen'))\n",
    "ax.text(0.8, 0.5, 'f', fontsize=16, ha='center')\n",
    "\n",
    "ax.annotate('', xy=(0.43, 0.57), xytext=(0.25, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.43, 0.43), xytext=(0.25, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.75, 0.5), xytext=(0.57, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "\n",
    "ax.annotate('dout', xy=(0.57, 0.45), xytext=(0.75, 0.35),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red')\n",
    "ax.annotate('y·dout', xy=(0.25, 0.65), xytext=(0.43, 0.57),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n",
    "ax.annotate('x·dout', xy=(0.25, 0.35), xytext=(0.43, 0.43),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=10)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('乘法節點: 梯度交換', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# Max 節點\n",
    "ax = axes[2]\n",
    "ax.text(0.2, 0.7, 'x', fontsize=16, ha='center')\n",
    "ax.text(0.2, 0.3, 'y', fontsize=16, ha='center')\n",
    "ax.text(0.5, 0.5, 'max', fontsize=14, ha='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax.text(0.8, 0.5, 'f', fontsize=16, ha='center')\n",
    "\n",
    "ax.annotate('', xy=(0.43, 0.57), xytext=(0.25, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.43, 0.43), xytext=(0.25, 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "ax.annotate('', xy=(0.75, 0.5), xytext=(0.57, 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='blue'))\n",
    "\n",
    "ax.annotate('dout', xy=(0.57, 0.45), xytext=(0.75, 0.35),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red')\n",
    "ax.annotate('dout (if x>y)', xy=(0.25, 0.65), xytext=(0.35, 0.75),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), color='red', fontsize=9)\n",
    "ax.annotate('0 (if x>y)', xy=(0.25, 0.35), xytext=(0.35, 0.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'), color='gray', fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Max 節點: 梯度只流向最大值', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作 sigmoid 的前向和反向傳播\n",
    "\n",
    "Sigmoid 函數：$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "**提示**：sigmoid 的導數有一個漂亮的形式 $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid 層的前向/反向傳播\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            任意形狀的輸入\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray\n",
    "            與輸入相同形狀\n",
    "        \"\"\"\n",
    "        # 解答：\n",
    "        # 數值穩定的 sigmoid 實作\n",
    "        # 對於 x >= 0: sigmoid = 1 / (1 + exp(-x))\n",
    "        # 對於 x < 0: sigmoid = exp(x) / (1 + exp(x))\n",
    "        out = np.where(x >= 0,\n",
    "                       1 / (1 + np.exp(-x)),\n",
    "                       np.exp(x) / (1 + np.exp(x)))\n",
    "        self.cache = out  # 儲存輸出值，不是輸入\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray\n",
    "            上游梯度，形狀與 forward 輸出相同\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray\n",
    "            對輸入的梯度\n",
    "        \"\"\"\n",
    "        # 解答：\n",
    "        # σ'(x) = σ(x) * (1 - σ(x))\n",
    "        # 這裡 self.cache 已經是 σ(x) 了\n",
    "        sigmoid_out = self.cache\n",
    "        dx = dout * sigmoid_out * (1 - sigmoid_out)\n",
    "        return dx\n",
    "\n",
    "# 測試\n",
    "sigmoid = Sigmoid()\n",
    "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "y = sigmoid.forward(x)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"sigmoid(x) = {y}\")\n",
    "\n",
    "# 驗證梯度\n",
    "dout = np.ones_like(x)\n",
    "dx = sigmoid.backward(dout)\n",
    "print(f\"\\n解析梯度 dx = {dx}\")\n",
    "\n",
    "# 數值驗證\n",
    "eps = 1e-5\n",
    "num_dx = np.zeros_like(x)\n",
    "for i in range(len(x)):\n",
    "    x_plus = x.copy()\n",
    "    x_plus[i] += eps\n",
    "    sigmoid_new = Sigmoid()\n",
    "    y_plus = sigmoid_new.forward(x_plus)\n",
    "    \n",
    "    x_minus = x.copy()\n",
    "    x_minus[i] -= eps\n",
    "    sigmoid_new = Sigmoid()\n",
    "    y_minus = sigmoid_new.forward(x_minus)\n",
    "    \n",
    "    # 假設 loss = sum(y)\n",
    "    num_dx[i] = (np.sum(y_plus) - np.sum(y_minus)) / (2 * eps)\n",
    "\n",
    "print(f\"數值梯度 dx ≈ {num_dx}\")\n",
    "print(f\"相對誤差: {np.max(np.abs(dx - num_dx) / (np.abs(dx) + np.abs(num_dx) + 1e-8)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：實作多層網路的反向傳播\n",
    "\n",
    "建構一個簡單的兩層網路：\n",
    "- 輸入 → FC1 → Sigmoid → FC2 → 輸出\n",
    "\n",
    "**提示**：反向傳播時要以相反順序呼叫各層的 backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \"\"\"\n",
    "    兩層網路：FC1 → Sigmoid → FC2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            輸入維度\n",
    "        hidden_features : int\n",
    "            隱藏層維度\n",
    "        out_features : int\n",
    "            輸出維度\n",
    "        \"\"\"\n",
    "        # 解答：初始化各層\n",
    "        self.fc1 = FullyConnectedLayer(in_features, hidden_features)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.fc2 = FullyConnectedLayer(hidden_features, out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, in_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, out_features)\n",
    "        \"\"\"\n",
    "        # 解答：\n",
    "        h1 = self.fc1.forward(X)\n",
    "        h2 = self.sigmoid.forward(h1)\n",
    "        out = self.fc2.forward(h2)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, out_features)\n",
    "            損失對輸出的梯度\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : np.ndarray, shape (N, in_features)\n",
    "            損失對輸入的梯度\n",
    "        \"\"\"\n",
    "        # 解答：以相反順序呼叫 backward\n",
    "        dh2 = self.fc2.backward(dout)\n",
    "        dh1 = self.sigmoid.backward(dh2)\n",
    "        dX = self.fc1.backward(dh1)\n",
    "        return dX\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"\n",
    "        回傳所有參數和對應的梯度\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (self.fc1.W, self.fc1.dW),\n",
    "            (self.fc1.b, self.fc1.db),\n",
    "            (self.fc2.W, self.fc2.dW),\n",
    "            (self.fc2.b, self.fc2.db),\n",
    "        ]\n",
    "\n",
    "# 測試\n",
    "net = TwoLayerNet(in_features=4, hidden_features=8, out_features=2)\n",
    "X = np.random.randn(3, 4)  # 3 個樣本，4 維特徵\n",
    "\n",
    "# 前向傳播\n",
    "Y = net.forward(X)\n",
    "print(f\"輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {Y.shape}\")\n",
    "\n",
    "# 假設簡單的 MSE loss\n",
    "target = np.random.randn(3, 2)\n",
    "loss = np.mean((Y - target) ** 2)\n",
    "dY = 2 * (Y - target) / Y.size\n",
    "\n",
    "# 反向傳播\n",
    "dX = net.backward(dY)\n",
    "print(f\"\\ndX 形狀: {dX.shape}\")\n",
    "\n",
    "# 顯示各層梯度\n",
    "print(\"\\n各層參數的梯度形狀:\")\n",
    "for i, (param, grad) in enumerate(net.get_params_and_grads()):\n",
    "    print(f\"  參數 {i}: {param.shape}, 梯度: {grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度檢驗整個網路\n",
    "def gradient_check_network(net, X, target, eps=1e-5):\n",
    "    \"\"\"\n",
    "    對整個網路進行梯度檢驗\n",
    "    \"\"\"\n",
    "    # 前向傳播 + 反向傳播\n",
    "    Y = net.forward(X)\n",
    "    loss = np.mean((Y - target) ** 2)\n",
    "    dY = 2 * (Y - target) / Y.size\n",
    "    net.backward(dY)\n",
    "    \n",
    "    params_and_grads = net.get_params_and_grads()\n",
    "    \n",
    "    print(\"=== 網路梯度檢驗 ===\")\n",
    "    \n",
    "    for idx, (param, grad) in enumerate(params_and_grads):\n",
    "        # 取幾個隨機位置來檢驗\n",
    "        num_checks = min(5, param.size)\n",
    "        flat_idx = np.random.choice(param.size, num_checks, replace=False)\n",
    "        \n",
    "        for i in flat_idx:\n",
    "            # 轉成多維索引\n",
    "            multi_idx = np.unravel_index(i, param.shape)\n",
    "            \n",
    "            # param + eps\n",
    "            old_val = param[multi_idx]\n",
    "            param[multi_idx] = old_val + eps\n",
    "            Y_plus = net.forward(X)\n",
    "            loss_plus = np.mean((Y_plus - target) ** 2)\n",
    "            \n",
    "            # param - eps\n",
    "            param[multi_idx] = old_val - eps\n",
    "            Y_minus = net.forward(X)\n",
    "            loss_minus = np.mean((Y_minus - target) ** 2)\n",
    "            \n",
    "            # 恢復\n",
    "            param[multi_idx] = old_val\n",
    "            \n",
    "            num_grad = (loss_plus - loss_minus) / (2 * eps)\n",
    "            analytic_grad = grad[multi_idx]\n",
    "            \n",
    "            rel_error = abs(num_grad - analytic_grad) / (abs(num_grad) + abs(analytic_grad) + 1e-8)\n",
    "            \n",
    "            if rel_error > 1e-4:\n",
    "                print(f\"  參數 {idx}, 位置 {multi_idx}: 解析={analytic_grad:.6f}, 數值={num_grad:.6f}, 誤差={rel_error:.2e} ❌\")\n",
    "        \n",
    "    print(\"梯度檢驗完成!\")\n",
    "\n",
    "gradient_check_network(net, X, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：使用梯度下降訓練網路\n",
    "\n",
    "用上面的兩層網路學習一個簡單的函數（例如 XOR 問題）\n",
    "\n",
    "**提示**：XOR 問題需要非線性（這就是為什麼我們需要 sigmoid）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 資料集\n",
    "X_xor = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "], dtype=np.float64)\n",
    "\n",
    "y_xor = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(\"XOR 資料集:\")\n",
    "for i in range(4):\n",
    "    print(f\"  {X_xor[i]} -> {y_xor[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解答：訓練網路解決 XOR\n",
    "\n",
    "# 建立網路（需要足夠的隱藏單元來學習 XOR）\n",
    "np.random.seed(42)\n",
    "net = TwoLayerNet(in_features=2, hidden_features=8, out_features=1)\n",
    "\n",
    "# 訓練參數\n",
    "learning_rate = 1.0\n",
    "epochs = 5000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 前向傳播\n",
    "    Y = net.forward(X_xor)\n",
    "    \n",
    "    # 計算 MSE 損失\n",
    "    loss = np.mean((Y - y_xor) ** 2)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # 計算梯度\n",
    "    dY = 2 * (Y - y_xor) / Y.size\n",
    "    \n",
    "    # 反向傳播\n",
    "    net.backward(dY)\n",
    "    \n",
    "    # 更新參數（SGD）\n",
    "    for param, grad in net.get_params_and_grads():\n",
    "        param -= learning_rate * grad\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:4d}, Loss: {loss:.6f}\")\n",
    "\n",
    "print(f\"\\n最終 Loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss 曲線\n",
    "ax = axes[0]\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 決策邊界\n",
    "ax = axes[1]\n",
    "\n",
    "# 產生網格\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100),\n",
    "                     np.linspace(-0.5, 1.5, 100))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# 預測\n",
    "Z = net.forward(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# 畫決策邊界\n",
    "ax.contourf(xx, yy, Z, levels=50, cmap='RdYlBu_r', alpha=0.7)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# 畫資料點\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_xor.flatten()]\n",
    "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2, zorder=5)\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('XOR Decision Boundary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 顯示最終預測\n",
    "print(\"\\n最終預測:\")\n",
    "Y_final = net.forward(X_xor)\n",
    "for i in range(4):\n",
    "    pred = 1 if Y_final[i, 0] > 0.5 else 0\n",
    "    actual = int(y_xor[i, 0])\n",
    "    correct = \"✓\" if pred == actual else \"✗\"\n",
    "    print(f\"  {X_xor[i]} -> pred: {Y_final[i, 0]:.4f} ({pred}) | actual: {actual} {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們學習了：\n",
    "\n",
    "1. **計算圖**：將複雜運算拆解成簡單操作的有向圖\n",
    "\n",
    "2. **連鎖律 (Chain Rule)**：反向傳播的核心\n",
    "   $$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "3. **常見操作的梯度模式**：\n",
    "   - 加法：梯度平均分配\n",
    "   - 乘法：梯度交換\n",
    "   - Max：梯度只流向最大值\n",
    "   - 複製：梯度相加\n",
    "\n",
    "4. **向量化梯度**：\n",
    "   - $\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$\n",
    "   - $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$\n",
    "   - $\\frac{\\partial L}{\\partial b} = \\sum_{\\text{axis}=0} \\frac{\\partial L}{\\partial Y}$\n",
    "\n",
    "5. **梯度檢驗**：使用數值微分驗證解析梯度\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們會實作更多層的類型：\n",
    "- ReLU（比 Sigmoid 更常用的激活函數）\n",
    "- Softmax + Cross-Entropy（分類問題的標準損失）\n",
    "- Conv2D（卷積層，CNN 的核心）\n",
    "- MaxPool（池化層）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
