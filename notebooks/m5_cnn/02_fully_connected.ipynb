{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 全連接層 Fully Connected Layer\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 深入理解全連接層 (FC Layer) 的前向傳播\n",
    "2. 詳細推導 FC Layer 的反向傳播公式（用 index 展開方式）\n",
    "3. 實作完整的 FC Layer class，包含 Xavier/He 初始化\n",
    "4. 嚴格的梯度檢驗\n",
    "\n",
    "## 全連接層的定義\n",
    "\n",
    "全連接層（也叫 Dense Layer 或 Linear Layer）是神經網路最基本的組件。\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}$$\n",
    "\n",
    "其中：\n",
    "- $\\mathbf{X}$: 輸入，形狀 $(N, D)$，$N$ 是 batch size，$D$ 是輸入維度\n",
    "- $\\mathbf{W}$: 權重，形狀 $(D, M)$，$M$ 是輸出維度\n",
    "- $\\mathbf{b}$: 偏置，形狀 $(M,)$\n",
    "- $\\mathbf{Y}$: 輸出，形狀 $(N, M)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Fully Connected Layer module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：前向傳播\n",
    "\n",
    "### 公式推導（index 展開）\n",
    "\n",
    "輸出的每個元素：\n",
    "\n",
    "$$Y_{n,m} = \\sum_{d=1}^{D} X_{n,d} W_{d,m} + b_m$$\n",
    "\n",
    "這就是一個加權和加上偏置。\n",
    "\n",
    "### 向量化實作\n",
    "\n",
    "用矩陣乘法可以一次算出所有元素：\n",
    "\n",
    "```python\n",
    "Y = X @ W + b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_forward_naive(X, W, b):\n",
    "    \"\"\"\n",
    "    全連接層前向傳播（樸素版本，用來理解公式）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray, shape (N, D)\n",
    "    W : np.ndarray, shape (D, M)\n",
    "    b : np.ndarray, shape (M,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y : np.ndarray, shape (N, M)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M = W.shape[1]\n",
    "    \n",
    "    Y = np.zeros((N, M))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for m in range(M):\n",
    "            # Y[n, m] = sum_d X[n, d] * W[d, m] + b[m]\n",
    "            total = 0\n",
    "            for d in range(D):\n",
    "                total += X[n, d] * W[d, m]\n",
    "            Y[n, m] = total + b[m]\n",
    "    \n",
    "    return Y\n",
    "\n",
    "def fc_forward_vectorized(X, W, b):\n",
    "    \"\"\"\n",
    "    全連接層前向傳播（向量化版本）\n",
    "    \"\"\"\n",
    "    return X @ W + b\n",
    "\n",
    "# 測試兩個版本是否一致\n",
    "N, D, M = 4, 5, 3\n",
    "X = np.random.randn(N, D)\n",
    "W = np.random.randn(D, M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "Y_naive = fc_forward_naive(X, W, b)\n",
    "Y_vec = fc_forward_vectorized(X, W, b)\n",
    "\n",
    "print(f\"樸素版本形狀: {Y_naive.shape}\")\n",
    "print(f\"向量化版本形狀: {Y_vec.shape}\")\n",
    "print(f\"兩者差異: {np.max(np.abs(Y_naive - Y_vec)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：反向傳播（詳細推導）\n",
    "\n",
    "假設我們有損失函數 $L$，且已知 $\\frac{\\partial L}{\\partial Y}$（記作 $dY$，形狀 $(N, M)$）。\n",
    "\n",
    "我們需要計算：\n",
    "1. $\\frac{\\partial L}{\\partial X}$（形狀 $(N, D)$）\n",
    "2. $\\frac{\\partial L}{\\partial W}$（形狀 $(D, M)$）\n",
    "3. $\\frac{\\partial L}{\\partial b}$（形狀 $(M,)$）\n",
    "\n",
    "### 推導 $\\frac{\\partial L}{\\partial W}$\n",
    "\n",
    "使用 chain rule，考慮 $L$ 對 $W_{d,m}$ 的梯度：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{d,m}} = \\sum_{n=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Y_{n,j}} \\frac{\\partial Y_{n,j}}{\\partial W_{d,m}}$$\n",
    "\n",
    "因為 $Y_{n,j} = \\sum_{k} X_{n,k} W_{k,j} + b_j$，所以：\n",
    "\n",
    "$$\\frac{\\partial Y_{n,j}}{\\partial W_{d,m}} = \\begin{cases} X_{n,d} & \\text{if } j = m \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "代入：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{d,m}} = \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial Y_{n,m}} \\cdot X_{n,d} = \\sum_{n=1}^{N} X_{n,d} \\cdot dY_{n,m}$$\n",
    "\n",
    "寫成矩陣形式：$\\frac{\\partial L}{\\partial W} = X^T \\cdot dY$\n",
    "\n",
    "### 推導 $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "同樣使用 chain rule：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X_{n,d}} = \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Y_{n,j}} \\frac{\\partial Y_{n,j}}{\\partial X_{n,d}}$$\n",
    "\n",
    "因為 $Y_{n,j} = \\sum_{k} X_{n,k} W_{k,j} + b_j$，所以：\n",
    "\n",
    "$$\\frac{\\partial Y_{n,j}}{\\partial X_{n,d}} = W_{d,j}$$\n",
    "\n",
    "代入：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X_{n,d}} = \\sum_{j=1}^{M} dY_{n,j} \\cdot W_{d,j}$$\n",
    "\n",
    "寫成矩陣形式：$\\frac{\\partial L}{\\partial X} = dY \\cdot W^T$\n",
    "\n",
    "### 推導 $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b_m} = \\sum_{n=1}^{N} \\frac{\\partial L}{\\partial Y_{n,m}} \\frac{\\partial Y_{n,m}}{\\partial b_m} = \\sum_{n=1}^{N} dY_{n,m}$$\n",
    "\n",
    "寫成向量形式：$\\frac{\\partial L}{\\partial b} = \\sum_{n} dY$（對 axis=0 求和）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_backward_naive(dY, X, W):\n",
    "    \"\"\"\n",
    "    全連接層反向傳播（樸素版本）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dY : np.ndarray, shape (N, M)\n",
    "        損失對輸出的梯度\n",
    "    X : np.ndarray, shape (N, D)\n",
    "        前向傳播時的輸入（cache）\n",
    "    W : np.ndarray, shape (D, M)\n",
    "        權重矩陣\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dX : np.ndarray, shape (N, D)\n",
    "    dW : np.ndarray, shape (D, M)\n",
    "    db : np.ndarray, shape (M,)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M = W.shape[1]\n",
    "    \n",
    "    dX = np.zeros((N, D))\n",
    "    dW = np.zeros((D, M))\n",
    "    db = np.zeros(M)\n",
    "    \n",
    "    # 計算 dW\n",
    "    for d in range(D):\n",
    "        for m in range(M):\n",
    "            for n in range(N):\n",
    "                dW[d, m] += X[n, d] * dY[n, m]\n",
    "    \n",
    "    # 計算 dX\n",
    "    for n in range(N):\n",
    "        for d in range(D):\n",
    "            for m in range(M):\n",
    "                dX[n, d] += dY[n, m] * W[d, m]\n",
    "    \n",
    "    # 計算 db\n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            db[m] += dY[n, m]\n",
    "    \n",
    "    return dX, dW, db\n",
    "\n",
    "def fc_backward_vectorized(dY, X, W):\n",
    "    \"\"\"\n",
    "    全連接層反向傳播（向量化版本）\n",
    "    \"\"\"\n",
    "    dW = X.T @ dY           # (D, N) @ (N, M) = (D, M)\n",
    "    dX = dY @ W.T           # (N, M) @ (M, D) = (N, D)\n",
    "    db = np.sum(dY, axis=0) # (M,)\n",
    "    \n",
    "    return dX, dW, db\n",
    "\n",
    "# 測試\n",
    "dY = np.random.randn(N, M)\n",
    "\n",
    "dX_naive, dW_naive, db_naive = fc_backward_naive(dY, X, W)\n",
    "dX_vec, dW_vec, db_vec = fc_backward_vectorized(dY, X, W)\n",
    "\n",
    "print(\"=== 樸素版本 vs 向量化版本 ===\")\n",
    "print(f\"dX 差異: {np.max(np.abs(dX_naive - dX_vec)):.2e}\")\n",
    "print(f\"dW 差異: {np.max(np.abs(dW_naive - dW_vec)):.2e}\")\n",
    "print(f\"db 差異: {np.max(np.abs(db_naive - db_vec)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：權重初始化\n",
    "\n",
    "權重初始化對神經網路訓練非常重要。如果初始化不當，會導致：\n",
    "- **梯度消失**：激活值太小，梯度接近零\n",
    "- **梯度爆炸**：激活值太大，梯度暴增\n",
    "\n",
    "### Xavier 初始化 (Glorot Initialization)\n",
    "\n",
    "適用於 sigmoid/tanh 激活函數：\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{1}{n_{in}}\\right) \\text{ 或 } W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in} + n_{out}}\\right)$$\n",
    "\n",
    "### He 初始化\n",
    "\n",
    "適用於 ReLU 激活函數：\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{in}}\\right)$$\n",
    "\n",
    "### 直觀理解\n",
    "\n",
    "假設輸入 $x$ 的每個元素有方差 $\\sigma^2$，我們希望輸出也有相同的方差。\n",
    "\n",
    "對於 $y = \\sum_{i=1}^{n_{in}} w_i x_i$：\n",
    "\n",
    "$$\\text{Var}(y) = n_{in} \\cdot \\text{Var}(w) \\cdot \\text{Var}(x)$$\n",
    "\n",
    "要讓 $\\text{Var}(y) = \\text{Var}(x)$，需要 $\\text{Var}(w) = \\frac{1}{n_{in}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(n_in, n_out):\n",
    "    \"\"\"\n",
    "    Xavier 初始化\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_in : int\n",
    "        輸入維度\n",
    "    n_out : int\n",
    "        輸出維度\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : np.ndarray, shape (n_in, n_out)\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2.0 / (n_in + n_out))\n",
    "    return np.random.randn(n_in, n_out) * std\n",
    "\n",
    "def he_init(n_in, n_out):\n",
    "    \"\"\"\n",
    "    He 初始化（適用於 ReLU）\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2.0 / n_in)\n",
    "    return np.random.randn(n_in, n_out) * std\n",
    "\n",
    "# 比較不同初始化方式對前向傳播的影響\n",
    "def analyze_init(init_func, name, layers=10, hidden_dim=256):\n",
    "    \"\"\"\n",
    "    分析初始化方式對深度網路激活值的影響\n",
    "    \"\"\"\n",
    "    x = np.random.randn(32, hidden_dim)  # batch of 32, 256 features\n",
    "    \n",
    "    activations = [x]\n",
    "    \n",
    "    for i in range(layers):\n",
    "        W = init_func(hidden_dim, hidden_dim)\n",
    "        x = x @ W  # 無激活函數，純線性\n",
    "        activations.append(x)\n",
    "    \n",
    "    # 計算每層的統計量\n",
    "    means = [np.mean(a) for a in activations]\n",
    "    stds = [np.std(a) for a in activations]\n",
    "    \n",
    "    return means, stds\n",
    "\n",
    "# 標準初始化\n",
    "def standard_init(n_in, n_out):\n",
    "    return np.random.randn(n_in, n_out) * 0.01\n",
    "\n",
    "# 測試不同初始化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for init_func, name, color in [\n",
    "    (standard_init, 'Standard (0.01)', 'red'),\n",
    "    (xavier_init, 'Xavier', 'blue'),\n",
    "    (he_init, 'He', 'green')\n",
    "]:\n",
    "    means, stds = analyze_init(init_func, name)\n",
    "    \n",
    "    axes[0].plot(range(len(means)), means, 'o-', label=name, color=color)\n",
    "    axes[1].plot(range(len(stds)), stds, 'o-', label=name, color=color)\n",
    "\n",
    "axes[0].set_xlabel('Layer')\n",
    "axes[0].set_ylabel('Mean')\n",
    "axes[0].set_title('Activation Mean per Layer')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Layer')\n",
    "axes[1].set_ylabel('Std')\n",
    "axes[1].set_title('Activation Std per Layer')\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"- Standard init: 激活值迅速趨近於零（梯度消失）\")\n",
    "print(\"- Xavier/He init: 激活值的標準差保持穩定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：完整的 FC Layer 類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    \"\"\"\n",
    "    全連接層（線性層）\n",
    "    \n",
    "    前向：Y = X @ W + b\n",
    "    反向：\n",
    "        dW = X.T @ dY\n",
    "        db = sum(dY, axis=0)\n",
    "        dX = dY @ W.T\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int\n",
    "        輸入維度\n",
    "    out_features : int\n",
    "        輸出維度\n",
    "    init : str\n",
    "        初始化方式：'xavier', 'he', 'normal'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, init='xavier'):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # 權重初始化\n",
    "        if init == 'xavier':\n",
    "            std = np.sqrt(2.0 / (in_features + out_features))\n",
    "        elif init == 'he':\n",
    "            std = np.sqrt(2.0 / in_features)\n",
    "        else:  # normal\n",
    "            std = 0.01\n",
    "        \n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # 快取\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (N, D)\n",
    "            輸入，D == in_features\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Y : np.ndarray, shape (N, M)\n",
    "            輸出，M == out_features\n",
    "        \"\"\"\n",
    "        self.cache = X\n",
    "        Y = X @ self.W + self.b\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dY : np.ndarray, shape (N, M)\n",
    "            損失對輸出的梯度\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dX : np.ndarray, shape (N, D)\n",
    "            損失對輸入的梯度\n",
    "        \"\"\"\n",
    "        X = self.cache\n",
    "        \n",
    "        # 計算梯度\n",
    "        self.dW = X.T @ dY\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        dX = dY @ self.W.T\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"FullyConnected({self.in_features}, {self.out_features})\"\n",
    "\n",
    "# 測試\n",
    "fc = FullyConnected(in_features=10, out_features=5, init='he')\n",
    "print(f\"層結構: {fc}\")\n",
    "print(f\"權重形狀: {fc.W.shape}\")\n",
    "print(f\"偏置形狀: {fc.b.shape}\")\n",
    "\n",
    "# 前向傳播測試\n",
    "X = np.random.randn(3, 10)\n",
    "Y = fc.forward(X)\n",
    "print(f\"\\n輸入形狀: {X.shape}\")\n",
    "print(f\"輸出形狀: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：嚴格的梯度檢驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_fc(fc_layer, X, eps=1e-5, verbose=True):\n",
    "    \"\"\"\n",
    "    對全連接層進行嚴格的梯度檢驗\n",
    "    \n",
    "    使用 L = sum(Y^2) 作為測試損失函數\n",
    "    則 dL/dY = 2Y\n",
    "    \"\"\"\n",
    "    # 前向傳播\n",
    "    Y = fc_layer.forward(X)\n",
    "    \n",
    "    # 假設 loss = sum(Y^2)\n",
    "    loss = np.sum(Y ** 2)\n",
    "    dY = 2 * Y\n",
    "    \n",
    "    # 反向傳播\n",
    "    dX = fc_layer.backward(dY)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # === 檢驗 dW ===\n",
    "    if verbose:\n",
    "        print(\"=== 檢驗 dW ===\")\n",
    "    \n",
    "    dW_numerical = np.zeros_like(fc_layer.W)\n",
    "    \n",
    "    for i in range(fc_layer.W.shape[0]):\n",
    "        for j in range(fc_layer.W.shape[1]):\n",
    "            old_val = fc_layer.W[i, j]\n",
    "            \n",
    "            # W + eps\n",
    "            fc_layer.W[i, j] = old_val + eps\n",
    "            Y_plus = fc_layer.forward(X)\n",
    "            loss_plus = np.sum(Y_plus ** 2)\n",
    "            \n",
    "            # W - eps\n",
    "            fc_layer.W[i, j] = old_val - eps\n",
    "            Y_minus = fc_layer.forward(X)\n",
    "            loss_minus = np.sum(Y_minus ** 2)\n",
    "            \n",
    "            # 恢復\n",
    "            fc_layer.W[i, j] = old_val\n",
    "            \n",
    "            dW_numerical[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff_W = np.abs(fc_layer.dW - dW_numerical)\n",
    "    rel_error_W = np.max(diff_W / (np.abs(fc_layer.dW) + np.abs(dW_numerical) + 1e-8))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  最大絕對誤差: {np.max(diff_W):.2e}\")\n",
    "        print(f\"  最大相對誤差: {rel_error_W:.2e}\")\n",
    "    \n",
    "    if rel_error_W > 1e-4:\n",
    "        if verbose:\n",
    "            print(\"  ❌ 梯度檢驗失敗!\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"  ✓ 梯度檢驗通過\")\n",
    "    \n",
    "    # === 檢驗 db ===\n",
    "    if verbose:\n",
    "        print(\"\\n=== 檢驗 db ===\")\n",
    "    \n",
    "    db_numerical = np.zeros_like(fc_layer.b)\n",
    "    \n",
    "    for j in range(fc_layer.b.shape[0]):\n",
    "        old_val = fc_layer.b[j]\n",
    "        \n",
    "        fc_layer.b[j] = old_val + eps\n",
    "        Y_plus = fc_layer.forward(X)\n",
    "        loss_plus = np.sum(Y_plus ** 2)\n",
    "        \n",
    "        fc_layer.b[j] = old_val - eps\n",
    "        Y_minus = fc_layer.forward(X)\n",
    "        loss_minus = np.sum(Y_minus ** 2)\n",
    "        \n",
    "        fc_layer.b[j] = old_val\n",
    "        \n",
    "        db_numerical[j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff_b = np.abs(fc_layer.db - db_numerical)\n",
    "    rel_error_b = np.max(diff_b / (np.abs(fc_layer.db) + np.abs(db_numerical) + 1e-8))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  最大絕對誤差: {np.max(diff_b):.2e}\")\n",
    "        print(f\"  最大相對誤差: {rel_error_b:.2e}\")\n",
    "    \n",
    "    if rel_error_b > 1e-4:\n",
    "        if verbose:\n",
    "            print(\"  ❌ 梯度檢驗失敗!\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"  ✓ 梯度檢驗通過\")\n",
    "    \n",
    "    # === 檢驗 dX ===\n",
    "    if verbose:\n",
    "        print(\"\\n=== 檢驗 dX ===\")\n",
    "    \n",
    "    dX_numerical = np.zeros_like(X)\n",
    "    X_test = X.copy()\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            old_val = X_test[i, j]\n",
    "            \n",
    "            X_test[i, j] = old_val + eps\n",
    "            Y_plus = fc_layer.forward(X_test)\n",
    "            loss_plus = np.sum(Y_plus ** 2)\n",
    "            \n",
    "            X_test[i, j] = old_val - eps\n",
    "            Y_minus = fc_layer.forward(X_test)\n",
    "            loss_minus = np.sum(Y_minus ** 2)\n",
    "            \n",
    "            X_test[i, j] = old_val\n",
    "            \n",
    "            dX_numerical[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff_X = np.abs(dX - dX_numerical)\n",
    "    rel_error_X = np.max(diff_X / (np.abs(dX) + np.abs(dX_numerical) + 1e-8))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  最大絕對誤差: {np.max(diff_X):.2e}\")\n",
    "        print(f\"  最大相對誤差: {rel_error_X:.2e}\")\n",
    "    \n",
    "    if rel_error_X > 1e-4:\n",
    "        if verbose:\n",
    "            print(\"  ❌ 梯度檢驗失敗!\")\n",
    "        all_passed = False\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"  ✓ 梯度檢驗通過\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# 執行梯度檢驗\n",
    "fc = FullyConnected(10, 5)\n",
    "X = np.random.randn(3, 10)\n",
    "passed = gradient_check_fc(fc, X)\n",
    "print(f\"\\n總體結果: {'全部通過 ✓' if passed else '有錯誤 ✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作帶正則化的 FC Layer\n",
    "\n",
    "加入 L2 正則化（weight decay）到梯度計算中：\n",
    "\n",
    "$$L_{total} = L_{data} + \\frac{\\lambda}{2} \\|W\\|^2$$\n",
    "\n",
    "**提示**：正則化項對 $W$ 的梯度是 $\\lambda W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedWithL2:\n",
    "    \"\"\"\n",
    "    帶 L2 正則化的全連接層\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, weight_decay=0.0, init='xavier'):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # 初始化\n",
    "        if init == 'xavier':\n",
    "            std = np.sqrt(2.0 / (in_features + out_features))\n",
    "        elif init == 'he':\n",
    "            std = np.sqrt(2.0 / in_features)\n",
    "        else:\n",
    "            std = 0.01\n",
    "        \n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \"\"\"\n",
    "        self.cache = X\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        \"\"\"\n",
    "        反向傳播（含 L2 正則化）\n",
    "        \n",
    "        正則化項 (lambda/2) * ||W||^2 對 W 的梯度是 lambda * W\n",
    "        \"\"\"\n",
    "        X = self.cache\n",
    "        \n",
    "        # 資料梯度\n",
    "        dW_data = X.T @ dY\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        dX = dY @ self.W.T\n",
    "        \n",
    "        # 加上正則化梯度\n",
    "        # 解答：dW_total = dW_data + weight_decay * W\n",
    "        self.dW = dW_data + self.weight_decay * self.W\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def get_regularization_loss(self):\n",
    "        \"\"\"\n",
    "        回傳正則化損失 (lambda/2) * ||W||^2\n",
    "        \"\"\"\n",
    "        return 0.5 * self.weight_decay * np.sum(self.W ** 2)\n",
    "\n",
    "# 測試\n",
    "fc_l2 = FullyConnectedWithL2(10, 5, weight_decay=0.1)\n",
    "X = np.random.randn(3, 10)\n",
    "\n",
    "# 前向傳播\n",
    "Y = fc_l2.forward(X)\n",
    "\n",
    "# 假設損失函數\n",
    "data_loss = np.sum(Y ** 2)\n",
    "reg_loss = fc_l2.get_regularization_loss()\n",
    "total_loss = data_loss + reg_loss\n",
    "\n",
    "print(f\"資料損失: {data_loss:.4f}\")\n",
    "print(f\"正則化損失: {reg_loss:.4f}\")\n",
    "print(f\"總損失: {total_loss:.4f}\")\n",
    "\n",
    "# 反向傳播\n",
    "dY = 2 * Y\n",
    "dX = fc_l2.backward(dY)\n",
    "\n",
    "print(f\"\\n梯度形狀: dW={fc_l2.dW.shape}, db={fc_l2.db.shape}, dX={dX.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證帶正則化的梯度\n",
    "def gradient_check_fc_l2(fc_layer, X, eps=1e-5):\n",
    "    \"\"\"\n",
    "    檢驗帶 L2 正則化的 FC 層梯度\n",
    "    \"\"\"\n",
    "    # 前向 + 反向\n",
    "    Y = fc_layer.forward(X)\n",
    "    data_loss = np.sum(Y ** 2)\n",
    "    reg_loss = fc_layer.get_regularization_loss()\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    dY = 2 * Y\n",
    "    fc_layer.backward(dY)\n",
    "    \n",
    "    # 數值梯度（對 total_loss）\n",
    "    print(\"=== 檢驗帶 L2 正則化的 dW ===\")\n",
    "    \n",
    "    dW_numerical = np.zeros_like(fc_layer.W)\n",
    "    \n",
    "    for i in range(fc_layer.W.shape[0]):\n",
    "        for j in range(fc_layer.W.shape[1]):\n",
    "            old_val = fc_layer.W[i, j]\n",
    "            \n",
    "            fc_layer.W[i, j] = old_val + eps\n",
    "            Y_plus = fc_layer.forward(X)\n",
    "            loss_plus = np.sum(Y_plus ** 2) + fc_layer.get_regularization_loss()\n",
    "            \n",
    "            fc_layer.W[i, j] = old_val - eps\n",
    "            Y_minus = fc_layer.forward(X)\n",
    "            loss_minus = np.sum(Y_minus ** 2) + fc_layer.get_regularization_loss()\n",
    "            \n",
    "            fc_layer.W[i, j] = old_val\n",
    "            \n",
    "            dW_numerical[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(fc_layer.dW - dW_numerical) / \n",
    "                       (np.abs(fc_layer.dW) + np.abs(dW_numerical) + 1e-8))\n",
    "    print(f\"  最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"  通過: {rel_error < 1e-4}\")\n",
    "\n",
    "gradient_check_fc_l2(fc_l2, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：實作 Dropout\n",
    "\n",
    "Dropout 是一種正則化技術，在訓練時隨機「丟棄」一些神經元。\n",
    "\n",
    "**訓練時**：以機率 $p$ 將神經元設為 0，其餘的要除以 $(1-p)$（rescale）\n",
    "\n",
    "**測試時**：什麼都不做\n",
    "\n",
    "**提示**：反向傳播時，梯度也要經過相同的 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropout 層\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float\n",
    "        丟棄機率（0 到 1）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            任意形狀的輸入\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 解答：\n",
    "            # 1. 產生隨機 mask（機率 1-p 為 True）\n",
    "            # 2. 將 mask 應用到輸入\n",
    "            # 3. Rescale（除以 1-p）\n",
    "            self.mask = (np.random.rand(*X.shape) > self.p)\n",
    "            out = X * self.mask / (1 - self.p)\n",
    "        else:\n",
    "            out = X\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 解答：梯度也經過相同的 mask 和 rescale\n",
    "            dX = dout * self.mask / (1 - self.p)\n",
    "        else:\n",
    "            dX = dout\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "# 測試 Dropout\n",
    "np.random.seed(42)\n",
    "dropout = Dropout(p=0.5)\n",
    "\n",
    "X = np.ones((2, 5))\n",
    "print(\"輸入 (全 1):\")\n",
    "print(X)\n",
    "\n",
    "# 訓練模式\n",
    "dropout.train()\n",
    "out_train = dropout.forward(X)\n",
    "print(\"\\n訓練模式輸出 (p=0.5):\")\n",
    "print(out_train)\n",
    "print(f\"輸出平均: {np.mean(out_train):.2f} (應接近 1.0)\")\n",
    "\n",
    "# 測試模式\n",
    "dropout.eval()\n",
    "out_test = dropout.forward(X)\n",
    "print(\"\\n測試模式輸出:\")\n",
    "print(out_test)\n",
    "\n",
    "# 反向傳播測試\n",
    "dropout.train()\n",
    "_ = dropout.forward(X)  # 重新產生 mask\n",
    "dout = np.ones_like(X)\n",
    "dX = dropout.backward(dout)\n",
    "print(\"\\n反向傳播的梯度:\")\n",
    "print(dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：組合 FC + Dropout 訓練網路\n",
    "\n",
    "用 FC + Dropout 訓練一個簡單的分類器，觀察 Dropout 對過擬合的影響。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生簡單的分類資料\n",
    "np.random.seed(42)\n",
    "N = 100  # 樣本數\n",
    "\n",
    "# 兩類的資料\n",
    "X_class0 = np.random.randn(N//2, 2) + np.array([2, 2])\n",
    "X_class1 = np.random.randn(N//2, 2) + np.array([-2, -2])\n",
    "\n",
    "X_train = np.vstack([X_class0, X_class1])\n",
    "y_train = np.hstack([np.zeros(N//2), np.ones(N//2)]).astype(int)\n",
    "\n",
    "# 打亂\n",
    "perm = np.random.permutation(N)\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', label='Class 0')\n",
    "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Training Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 層（用於二分類）\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = np.where(X >= 0,\n",
    "                       1 / (1 + np.exp(-X)),\n",
    "                       np.exp(X) / (1 + np.exp(X)))\n",
    "        self.cache = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.cache * (1 - self.cache)\n",
    "\n",
    "# 訓練網路（含 Dropout）\n",
    "class ClassifierWithDropout:\n",
    "    def __init__(self, use_dropout=True, p=0.5):\n",
    "        self.fc1 = FullyConnected(2, 16, init='he')\n",
    "        self.sigmoid1 = Sigmoid()\n",
    "        self.dropout1 = Dropout(p=p) if use_dropout else None\n",
    "        self.fc2 = FullyConnected(16, 1, init='he')\n",
    "        self.sigmoid2 = Sigmoid()\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "    \n",
    "    def train_mode(self):\n",
    "        if self.dropout1:\n",
    "            self.dropout1.train()\n",
    "    \n",
    "    def eval_mode(self):\n",
    "        if self.dropout1:\n",
    "            self.dropout1.eval()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        h = self.fc1.forward(X)\n",
    "        h = self.sigmoid1.forward(h)\n",
    "        if self.use_dropout:\n",
    "            h = self.dropout1.forward(h)\n",
    "        h = self.fc2.forward(h)\n",
    "        out = self.sigmoid2.forward(h)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.sigmoid2.backward(dout)\n",
    "        dout = self.fc2.backward(dout)\n",
    "        if self.use_dropout:\n",
    "            dout = self.dropout1.backward(dout)\n",
    "        dout = self.sigmoid1.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        return [\n",
    "            (self.fc1.W, self.fc1.dW),\n",
    "            (self.fc1.b, self.fc1.db),\n",
    "            (self.fc2.W, self.fc2.dW),\n",
    "            (self.fc2.b, self.fc2.db),\n",
    "        ]\n",
    "\n",
    "def train_classifier(model, X, y, epochs=500, lr=0.5):\n",
    "    \"\"\"\n",
    "    訓練二分類器\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train_mode()\n",
    "        \n",
    "        # 前向傳播\n",
    "        y_pred = model.forward(X)\n",
    "        \n",
    "        # Binary Cross-Entropy loss\n",
    "        eps = 1e-8\n",
    "        loss = -np.mean(y.reshape(-1, 1) * np.log(y_pred + eps) + \n",
    "                        (1 - y.reshape(-1, 1)) * np.log(1 - y_pred + eps))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 梯度\n",
    "        dout = (y_pred - y.reshape(-1, 1)) / (y_pred * (1 - y_pred) + eps) / len(y)\n",
    "        \n",
    "        # 反向傳播\n",
    "        model.backward(dout)\n",
    "        \n",
    "        # 更新參數\n",
    "        for param, grad in model.get_params_and_grads():\n",
    "            param -= lr * grad\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 訓練兩個模型：有/無 Dropout\n",
    "np.random.seed(42)\n",
    "model_no_dropout = ClassifierWithDropout(use_dropout=False)\n",
    "losses_no_dropout = train_classifier(model_no_dropout, X_train, y_train, epochs=1000)\n",
    "\n",
    "np.random.seed(42)\n",
    "model_with_dropout = ClassifierWithDropout(use_dropout=True, p=0.5)\n",
    "losses_with_dropout = train_classifier(model_with_dropout, X_train, y_train, epochs=1000)\n",
    "\n",
    "# 比較\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_no_dropout, label='No Dropout', alpha=0.7)\n",
    "plt.plot(losses_with_dropout, label='With Dropout (p=0.5)', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCE Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"無 Dropout 最終 loss: {losses_no_dropout[-1]:.4f}\")\n",
    "print(f\"有 Dropout 最終 loss: {losses_with_dropout[-1]:.4f}\")\n",
    "print(\"\\n注意：有 Dropout 的訓練 loss 較高是正常的，因為訓練時有隨機丟棄。\")\n",
    "print(\"Dropout 的效果要在驗證集上才能看出來（減少過擬合）。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們深入學習了：\n",
    "\n",
    "1. **全連接層的數學**：\n",
    "   - 前向：$Y = XW + b$\n",
    "   - 反向：$dW = X^T \\cdot dY$, $db = \\sum dY$, $dX = dY \\cdot W^T$\n",
    "\n",
    "2. **權重初始化的重要性**：\n",
    "   - Xavier 初始化：適合 sigmoid/tanh\n",
    "   - He 初始化：適合 ReLU\n",
    "\n",
    "3. **正則化技術**：\n",
    "   - L2 正則化：加上 $\\lambda W$ 到梯度\n",
    "   - Dropout：訓練時隨機丟棄神經元\n",
    "\n",
    "4. **梯度檢驗**：用數值微分驗證解析梯度\n",
    "\n",
    "### 關鍵公式總結\n",
    "\n",
    "| 操作 | 前向 | 反向（對參數） | 反向（對輸入） |\n",
    "|------|------|----------------|----------------|\n",
    "| FC | $Y = XW + b$ | $dW = X^T dY$ | $dX = dY W^T$ |\n",
    "| L2 Reg | - | $dW += \\lambda W$ | - |\n",
    "| Dropout | $Y = X \\odot m / (1-p)$ | - | $dX = dY \\odot m / (1-p)$ |\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們會實作激活函數（ReLU, Sigmoid, Softmax）和損失函數（Cross-Entropy）的完整前向/反向傳播。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
