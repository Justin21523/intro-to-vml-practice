{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 激活函數 Activation Functions\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解激活函數的作用（引入非線性）\n",
    "2. 實作 ReLU, Sigmoid, Tanh 及其反向傳播\n",
    "3. 實作 Softmax + Cross-Entropy Loss（合併計算以保持數值穩定）\n",
    "4. 理解各種激活函數的優缺點\n",
    "\n",
    "## 為什麼需要激活函數？\n",
    "\n",
    "如果神經網路只有線性層，多層線性層的組合仍然是線性的：\n",
    "\n",
    "$$Y = W_2(W_1 X + b_1) + b_2 = (W_2 W_1)X + (W_2 b_1 + b_2) = W'X + b'$$\n",
    "\n",
    "這樣再多層也等於一層！激活函數引入**非線性**，讓網路能學習複雜的函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Activation Functions module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：ReLU (Rectified Linear Unit)\n",
    "\n",
    "ReLU 是目前最常用的激活函數。\n",
    "\n",
    "### 定義\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "### 梯度\n",
    "\n",
    "$$\\frac{\\partial \\text{ReLU}(x)}{\\partial x} = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "### 優點\n",
    "- 計算簡單快速\n",
    "- 解決梯度消失問題（對正值部分）\n",
    "- 稀疏激活（有些神經元輸出為 0）\n",
    "\n",
    "### 缺點\n",
    "- **Dead ReLU 問題**：如果神經元進入負區，梯度永遠為 0，無法恢復"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU 激活函數層\n",
    "    \n",
    "    forward: out = max(0, x)\n",
    "    backward: dx = dout * (x > 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            任意形狀的輸入\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray\n",
    "            與輸入相同形狀\n",
    "        \"\"\"\n",
    "        self.cache = x\n",
    "        out = np.maximum(0, x)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray\n",
    "            上游梯度\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray\n",
    "            對輸入的梯度\n",
    "        \"\"\"\n",
    "        x = self.cache\n",
    "        dx = dout * (x > 0).astype(float)\n",
    "        return dx\n",
    "\n",
    "# 視覺化 ReLU\n",
    "x = np.linspace(-5, 5, 100)\n",
    "relu = ReLU()\n",
    "y = relu.forward(x)\n",
    "\n",
    "# 梯度（假設 loss = sum(y)）\n",
    "dy = np.ones_like(x)\n",
    "dx = relu.backward(dy)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, y, 'b-', linewidth=2, label='ReLU(x)')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('ReLU(x)')\n",
    "ax.set_title('ReLU Forward')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, dx, 'r-', linewidth=2, label=\"ReLU'(x)\")\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"ReLU'(x)\")\n",
    "ax.set_title('ReLU Gradient')\n",
    "ax.set_ylim(-0.1, 1.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "為了解決 Dead ReLU 問題，Leaky ReLU 讓負值區域也有一個小的斜率：\n",
    "\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "通常 $\\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU:\n",
    "    \"\"\"\n",
    "    Leaky ReLU 激活函數\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        out = np.where(x > 0, x, self.alpha * x)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache\n",
    "        dx = dout * np.where(x > 0, 1, self.alpha)\n",
    "        return dx\n",
    "\n",
    "# 視覺化 Leaky ReLU\n",
    "leaky_relu = LeakyReLU(alpha=0.1)  # 用較大的 alpha 以便觀察\n",
    "y_leaky = leaky_relu.forward(x)\n",
    "dx_leaky = leaky_relu.backward(np.ones_like(x))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, y, 'b-', linewidth=2, label='ReLU')\n",
    "ax.plot(x, y_leaky, 'g-', linewidth=2, label='LeakyReLU (α=0.1)')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('ReLU vs LeakyReLU')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, dx, 'b-', linewidth=2, label=\"ReLU'\")\n",
    "ax.plot(x, dx_leaky, 'g-', linewidth=2, label=\"LeakyReLU'\")\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x)\")\n",
    "ax.set_title('Gradients')\n",
    "ax.set_ylim(-0.1, 1.5)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：Sigmoid\n",
    "\n",
    "### 定義\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "### 梯度\n",
    "\n",
    "$$\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "**推導**：\n",
    "$$\\frac{d\\sigma}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} \\cdot \\frac{e^{-x}}{1 + e^{-x}} = \\sigma(1 - \\sigma)$$\n",
    "\n",
    "### 特點\n",
    "- 輸出範圍 $(0, 1)$，適合輸出機率\n",
    "- **梯度消失問題**：當 $|x|$ 大時，$\\sigma'(x) \\approx 0$\n",
    "- 輸出不是 zero-centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid 激活函數層\n",
    "    \n",
    "    forward: out = 1 / (1 + exp(-x))\n",
    "    backward: dx = dout * out * (1 - out)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        數值穩定的 sigmoid 實作\n",
    "        \"\"\"\n",
    "        # 分開處理正負值以避免 overflow\n",
    "        out = np.where(x >= 0,\n",
    "                       1 / (1 + np.exp(-x)),\n",
    "                       np.exp(x) / (1 + np.exp(x)))\n",
    "        self.cache = out  # 儲存 output（不是 input）給 backward 用\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播：dx = dout * σ * (1 - σ)\n",
    "        \"\"\"\n",
    "        out = self.cache\n",
    "        dx = dout * out * (1 - out)\n",
    "        return dx\n",
    "\n",
    "# 視覺化 Sigmoid\n",
    "sigmoid = Sigmoid()\n",
    "y_sigmoid = sigmoid.forward(x)\n",
    "dx_sigmoid = sigmoid.backward(np.ones_like(x))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, y_sigmoid, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.5, color='k', linewidth=0.5, linestyle='--')\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('σ(x)')\n",
    "ax.set_title('Sigmoid')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, dx_sigmoid, 'r-', linewidth=2)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"σ'(x)\")\n",
    "ax.set_title('Sigmoid Gradient (max at x=0)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"σ(0) = {sigmoid.forward(np.array([0]))[0]:.4f}\")\n",
    "print(f\"σ'(0) = {sigmoid.cache[0] * (1 - sigmoid.cache[0]):.4f}\")\n",
    "print(f\"最大梯度值: {np.max(dx_sigmoid):.4f} (理論上 max = 0.25 at x=0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：Tanh\n",
    "\n",
    "### 定義\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\\sigma(2x) - 1$$\n",
    "\n",
    "### 梯度\n",
    "\n",
    "$$\\frac{\\partial \\tanh(x)}{\\partial x} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "### 特點\n",
    "- 輸出範圍 $(-1, 1)$，**zero-centered**\n",
    "- 仍有梯度消失問題（但比 sigmoid 好）\n",
    "- 在 RNN 中仍常用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh 激活函數層\n",
    "    \n",
    "    forward: out = tanh(x)\n",
    "    backward: dx = dout * (1 - out^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.cache = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        out = self.cache\n",
    "        dx = dout * (1 - out ** 2)\n",
    "        return dx\n",
    "\n",
    "# 視覺化 Tanh\n",
    "tanh = Tanh()\n",
    "y_tanh = tanh.forward(x)\n",
    "dx_tanh = tanh.backward(np.ones_like(x))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, y_sigmoid, 'b-', linewidth=2, label='Sigmoid')\n",
    "ax.plot(x, y_tanh, 'g-', linewidth=2, label='Tanh')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Sigmoid vs Tanh')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(x, dx_sigmoid, 'b-', linewidth=2, label=\"Sigmoid'\")\n",
    "ax.plot(x, dx_tanh, 'g-', linewidth=2, label=\"Tanh'\")\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x)\")\n",
    "ax.set_title('Gradient Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"tanh(0) = {y_tanh[len(x)//2]:.4f}\")\n",
    "print(f\"tanh'(0) = {dx_tanh[len(x)//2]:.4f}\")\n",
    "print(\"注意：Tanh 是 zero-centered (output 包含正負值)\")\n",
    "print(\"      Tanh 的最大梯度 = 1 (at x=0)，比 Sigmoid 的 0.25 大\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：Softmax + Cross-Entropy Loss\n",
    "\n",
    "### Softmax\n",
    "\n",
    "將 $K$ 個 raw scores（logits）轉換成機率分佈：\n",
    "\n",
    "$$p_i = \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$L = -\\sum_{i=1}^{K} y_i \\log(p_i)$$\n",
    "\n",
    "其中 $y$ 是 one-hot 編碼的真實標籤。如果真實類別是 $c$：\n",
    "\n",
    "$$L = -\\log(p_c)$$\n",
    "\n",
    "### 為什麼要合併計算？\n",
    "\n",
    "單獨計算 softmax 的梯度很複雜，但 **Softmax + Cross-Entropy 合併後的梯度非常簡潔**：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_i} = p_i - y_i$$\n",
    "\n",
    "這也很直觀：梯度就是「預測」減「真實」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    數值穩定的 Softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray, shape (N, K)\n",
    "        Raw scores (logits)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    p : np.ndarray, shape (N, K)\n",
    "        機率分佈，每行總和為 1\n",
    "    \"\"\"\n",
    "    # 減去最大值以防止 exp overflow\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "    exp_z = np.exp(z_shifted)\n",
    "    p = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    return p\n",
    "\n",
    "# 測試 softmax\n",
    "z = np.array([[1.0, 2.0, 3.0],\n",
    "              [1000.0, 1001.0, 1002.0]])  # 第二行測試數值穩定性\n",
    "\n",
    "p = softmax(z)\n",
    "print(\"Logits:\")\n",
    "print(z)\n",
    "print(\"\\nSoftmax 輸出 (機率):\")\n",
    "print(p)\n",
    "print(f\"\\n每行總和: {np.sum(p, axis=1)}\")\n",
    "print(\"（即使輸入很大，輸出仍然是有效的機率分佈）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"\n",
    "    Softmax + Cross-Entropy Loss（合併計算）\n",
    "    \n",
    "    這個層接收 raw scores (logits)，輸出 loss。\n",
    "    反向傳播時直接計算 dL/d(logits) = softmax(logits) - y\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, z, y):\n",
    "        \"\"\"\n",
    "        前向傳播：計算 Softmax Cross-Entropy Loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        z : np.ndarray, shape (N, K)\n",
    "            Raw scores (logits)\n",
    "        y : np.ndarray, shape (N,)\n",
    "            真實類別標籤（整數）\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            平均 cross-entropy loss\n",
    "        \"\"\"\n",
    "        N = z.shape[0]\n",
    "        \n",
    "        # 計算 softmax\n",
    "        p = softmax(z)\n",
    "        \n",
    "        # 計算 cross-entropy loss\n",
    "        # L = -log(p[正確類別])\n",
    "        eps = 1e-10  # 避免 log(0)\n",
    "        log_likelihood = -np.log(p[np.arange(N), y] + eps)\n",
    "        loss = np.mean(log_likelihood)\n",
    "        \n",
    "        # 儲存給 backward 用\n",
    "        self.cache = (p, y)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向傳播：dL/dz = p - y_one_hot\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dz : np.ndarray, shape (N, K)\n",
    "            對 logits 的梯度\n",
    "        \"\"\"\n",
    "        p, y = self.cache\n",
    "        N = p.shape[0]\n",
    "        \n",
    "        # 梯度 = softmax 輸出 - one-hot 標籤\n",
    "        dz = p.copy()\n",
    "        dz[np.arange(N), y] -= 1\n",
    "        dz /= N  # 平均\n",
    "        \n",
    "        return dz\n",
    "\n",
    "# 測試\n",
    "z = np.array([[1.0, 2.0, 0.5],\n",
    "              [0.5, 0.3, 0.8],\n",
    "              [2.0, 1.0, 0.0]])\n",
    "y = np.array([1, 2, 0])  # 正確類別\n",
    "\n",
    "loss_fn = SoftmaxCrossEntropy()\n",
    "loss = loss_fn.forward(z, y)\n",
    "dz = loss_fn.backward()\n",
    "\n",
    "print(\"Logits:\")\n",
    "print(z)\n",
    "print(f\"\\n真實標籤: {y}\")\n",
    "print(f\"\\nSoftmax 輸出:\")\n",
    "print(softmax(z))\n",
    "print(f\"\\nCross-Entropy Loss: {loss:.4f}\")\n",
    "print(f\"\\n梯度 dL/dz:\")\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度公式的數學推導\n",
    "\n",
    "為什麼 $\\frac{\\partial L}{\\partial z_i} = p_i - y_i$？\n",
    "\n",
    "設真實類別是 $c$（即 $y_c = 1$，其他 $y_j = 0$）\n",
    "\n",
    "$$L = -\\log(p_c) = -\\log\\left(\\frac{e^{z_c}}{\\sum_j e^{z_j}}\\right) = -z_c + \\log\\left(\\sum_j e^{z_j}\\right)$$\n",
    "\n",
    "對 $z_i$ 求偏導：\n",
    "\n",
    "**情況 1**：$i = c$（對正確類別）\n",
    "$$\\frac{\\partial L}{\\partial z_c} = -1 + \\frac{e^{z_c}}{\\sum_j e^{z_j}} = -1 + p_c = p_c - 1 = p_c - y_c$$\n",
    "\n",
    "**情況 2**：$i \\neq c$（對其他類別）\n",
    "$$\\frac{\\partial L}{\\partial z_i} = 0 + \\frac{e^{z_i}}{\\sum_j e^{z_j}} = p_i = p_i - 0 = p_i - y_i$$\n",
    "\n",
    "合併：$\\frac{\\partial L}{\\partial z_i} = p_i - y_i$ ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度檢驗\n",
    "def gradient_check_softmax_ce(z, y, eps=1e-5):\n",
    "    \"\"\"\n",
    "    檢驗 Softmax + Cross-Entropy 的梯度\n",
    "    \"\"\"\n",
    "    loss_fn = SoftmaxCrossEntropy()\n",
    "    loss = loss_fn.forward(z, y)\n",
    "    dz_analytic = loss_fn.backward()\n",
    "    \n",
    "    # 數值梯度\n",
    "    dz_numerical = np.zeros_like(z)\n",
    "    \n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            z_plus = z.copy()\n",
    "            z_plus[i, j] += eps\n",
    "            loss_plus = SoftmaxCrossEntropy().forward(z_plus, y)\n",
    "            \n",
    "            z_minus = z.copy()\n",
    "            z_minus[i, j] -= eps\n",
    "            loss_minus = SoftmaxCrossEntropy().forward(z_minus, y)\n",
    "            \n",
    "            dz_numerical[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    diff = np.abs(dz_analytic - dz_numerical)\n",
    "    rel_error = np.max(diff / (np.abs(dz_analytic) + np.abs(dz_numerical) + 1e-8))\n",
    "    \n",
    "    print(\"=== Softmax + Cross-Entropy 梯度檢驗 ===\")\n",
    "    print(f\"最大絕對誤差: {np.max(diff):.2e}\")\n",
    "    print(f\"最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"通過: {rel_error < 1e-5}\")\n",
    "\n",
    "gradient_check_softmax_ce(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：激活函數比較\n",
    "\n",
    "讓我們比較不同激活函數在梯度傳遞方面的表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較各種激活函數\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "x = np.linspace(-6, 6, 200)\n",
    "\n",
    "activations = [\n",
    "    ('ReLU', ReLU()),\n",
    "    ('Leaky ReLU (α=0.1)', LeakyReLU(0.1)),\n",
    "    ('Sigmoid', Sigmoid()),\n",
    "    ('Tanh', Tanh()),\n",
    "]\n",
    "\n",
    "for idx, (name, act) in enumerate(activations):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    \n",
    "    y = act.forward(x)\n",
    "    dy = act.backward(np.ones_like(x))\n",
    "    \n",
    "    # 函數值\n",
    "    ax = axes[row, col]\n",
    "    ax.plot(x, y, 'b-', linewidth=2, label='f(x)')\n",
    "    ax.plot(x, dy, 'r--', linewidth=2, label=\"f'(x)\")\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-2, 4)\n",
    "\n",
    "# 梯度消失問題演示\n",
    "ax = axes[1, 2]\n",
    "\n",
    "# 模擬深度網路中梯度的衰減\n",
    "depths = np.arange(1, 21)\n",
    "x_val = 0.5  # 激活值\n",
    "\n",
    "sigmoid_grads = []\n",
    "tanh_grads = []\n",
    "relu_grads = []\n",
    "\n",
    "for d in depths:\n",
    "    # Sigmoid: 最大梯度 = 0.25\n",
    "    sigmoid_grads.append(0.25 ** d)\n",
    "    # Tanh: 最大梯度 = 1，但實際通常 < 1\n",
    "    tanh_grads.append(0.6 ** d)  # 假設平均梯度 0.6\n",
    "    # ReLU: 梯度 = 1\n",
    "    relu_grads.append(1.0 ** d)\n",
    "\n",
    "ax.semilogy(depths, sigmoid_grads, 'b-o', label='Sigmoid (0.25^d)')\n",
    "ax.semilogy(depths, tanh_grads, 'g-o', label='Tanh (0.6^d)')\n",
    "ax.semilogy(depths, relu_grads, 'r-o', label='ReLU (1^d)')\n",
    "ax.set_xlabel('Network Depth')\n",
    "ax.set_ylabel('Gradient Magnitude')\n",
    "ax.set_title('Gradient Vanishing Problem')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== 激活函數比較 ===\")\n",
    "print(\"\\n| 激活函數 | 輸出範圍 | 優點 | 缺點 |\")\n",
    "print(\"|----------|----------|------|------|\")\n",
    "print(\"| ReLU | [0, ∞) | 計算快，不飽和 | Dead ReLU 問題 |\")\n",
    "print(\"| Leaky ReLU | (-∞, ∞) | 解決 Dead ReLU | 需調 α |\")\n",
    "print(\"| Sigmoid | (0, 1) | 輸出可解釋為機率 | 梯度消失，非 zero-centered |\")\n",
    "print(\"| Tanh | (-1, 1) | Zero-centered | 梯度消失 |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作 ELU (Exponential Linear Unit)\n",
    "\n",
    "$$\\text{ELU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "\n",
    "**提示**：反向傳播時，對於 $x \\leq 0$，$\\frac{d}{dx}[\\alpha(e^x - 1)] = \\alpha e^x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU:\n",
    "    \"\"\"\n",
    "    ELU 激活函數\n",
    "    \n",
    "    ELU(x) = x           if x > 0\n",
    "           = α(e^x - 1)  if x <= 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \"\"\"\n",
    "        # 解答：\n",
    "        self.cache = x\n",
    "        out = np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        對於 x > 0: 梯度 = 1\n",
    "        對於 x <= 0: 梯度 = α * e^x = ELU(x) + α\n",
    "        \"\"\"\n",
    "        x = self.cache\n",
    "        # 解答：\n",
    "        # 對於 x <= 0，d/dx [α(e^x - 1)] = α * e^x\n",
    "        dx = dout * np.where(x > 0, 1, self.alpha * np.exp(x))\n",
    "        return dx\n",
    "\n",
    "# 測試 ELU\n",
    "elu = ELU(alpha=1.0)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y_elu = elu.forward(x)\n",
    "dx_elu = elu.backward(np.ones_like(x))\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "relu_test = ReLU()\n",
    "y_relu = relu_test.forward(x)\n",
    "ax.plot(x, y_relu, 'b-', linewidth=2, label='ReLU')\n",
    "ax.plot(x, y_elu, 'g-', linewidth=2, label='ELU (α=1)')\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('ReLU vs ELU')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "dx_relu = relu_test.backward(np.ones_like(x))\n",
    "ax.plot(x, dx_relu, 'b-', linewidth=2, label=\"ReLU'\")\n",
    "ax.plot(x, dx_elu, 'g-', linewidth=2, label=\"ELU'\")\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel(\"f'(x)\")\n",
    "ax.set_title('Gradient Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ELU 優點：\")\n",
    "print(\"1. 負值區域有非零輸出，比 ReLU 更接近 zero-mean\")\n",
    "print(\"2. 負值區域梯度不為零，解決 Dead ReLU 問題\")\n",
    "print(\"3. 負值區域有平滑的飽和，提供 noise robustness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度檢驗 ELU\n",
    "def gradient_check_elu(elu, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    檢驗 ELU 的梯度\n",
    "    \"\"\"\n",
    "    y = elu.forward(x)\n",
    "    loss = np.sum(y ** 2)\n",
    "    dout = 2 * y\n",
    "    dx_analytic = elu.backward(dout)\n",
    "    \n",
    "    # 數值梯度\n",
    "    dx_numerical = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        elu_new = ELU(elu.alpha)\n",
    "        y_plus = elu_new.forward(x_plus)\n",
    "        loss_plus = np.sum(y_plus ** 2)\n",
    "        \n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= eps\n",
    "        elu_new = ELU(elu.alpha)\n",
    "        y_minus = elu_new.forward(x_minus)\n",
    "        loss_minus = np.sum(y_minus ** 2)\n",
    "        \n",
    "        dx_numerical[i] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(dx_analytic - dx_numerical) / \n",
    "                       (np.abs(dx_analytic) + np.abs(dx_numerical) + 1e-8))\n",
    "    print(f\"ELU 梯度檢驗 - 最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"通過: {rel_error < 1e-5}\")\n",
    "\n",
    "x_test = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
    "elu_test = ELU(alpha=1.0)\n",
    "gradient_check_elu(elu_test, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：實作完整的分類網路\n",
    "\n",
    "組合 FC + ReLU + Softmax/CE 建立一個多層分類網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先定義 FullyConnected 類別（從上一個 notebook）\n",
    "class FullyConnected:\n",
    "    def __init__(self, in_features, out_features, init='he'):\n",
    "        if init == 'xavier':\n",
    "            std = np.sqrt(2.0 / (in_features + out_features))\n",
    "        elif init == 'he':\n",
    "            std = np.sqrt(2.0 / in_features)\n",
    "        else:\n",
    "            std = 0.01\n",
    "        \n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        X = self.cache\n",
    "        self.dW = X.T @ dY\n",
    "        self.db = np.sum(dY, axis=0)\n",
    "        return dY @ self.W.T\n",
    "\n",
    "\n",
    "class MultiLayerClassifier:\n",
    "    \"\"\"\n",
    "    多層分類器\n",
    "    \n",
    "    架構：FC -> ReLU -> FC -> ReLU -> FC -> Softmax/CE\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, num_classes):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            輸入維度\n",
    "        hidden_dims : list of int\n",
    "            隱藏層維度列表\n",
    "        num_classes : int\n",
    "            類別數\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        \n",
    "        # 建立網路\n",
    "        dims = [input_dim] + hidden_dims + [num_classes]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            self.layers.append(FullyConnected(dims[i], dims[i+1], init='he'))\n",
    "            if i < len(dims) - 2:  # 最後一層不加 ReLU\n",
    "                self.layers.append(ReLU())\n",
    "        \n",
    "        self.loss_fn = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向傳播（不包括 loss）\n",
    "        \"\"\"\n",
    "        out = X\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out  # logits\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"\n",
    "        計算損失\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)\n",
    "        return self.loss_fn.forward(logits, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \"\"\"\n",
    "        dout = self.loss_fn.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"\n",
    "        回傳所有參數和梯度\n",
    "        \"\"\"\n",
    "        params_and_grads = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'W'):\n",
    "                params_and_grads.append((layer.W, layer.dW))\n",
    "                params_and_grads.append((layer.b, layer.db))\n",
    "        return params_and_grads\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        預測類別\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)\n",
    "        return np.argmax(logits, axis=1)\n",
    "\n",
    "# 測試\n",
    "net = MultiLayerClassifier(input_dim=10, hidden_dims=[32, 16], num_classes=5)\n",
    "X = np.random.randn(8, 10)\n",
    "y = np.random.randint(0, 5, 8)\n",
    "\n",
    "loss = net.loss(X, y)\n",
    "print(f\"初始 loss: {loss:.4f}\")\n",
    "\n",
    "net.backward()\n",
    "print(f\"\\n網路層數: {len(net.layers)}\")\n",
    "print(\"層結構:\")\n",
    "for i, layer in enumerate(net.layers):\n",
    "    print(f\"  {i}: {type(layer).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在簡單資料集上訓練\n",
    "\n",
    "# 產生多類別資料\n",
    "np.random.seed(42)\n",
    "N_per_class = 100\n",
    "num_classes = 3\n",
    "\n",
    "# 產生螺旋資料\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for k in range(num_classes):\n",
    "    r = np.linspace(0.0, 1, N_per_class)\n",
    "    t = np.linspace(k * 4, (k + 1) * 4, N_per_class) + np.random.randn(N_per_class) * 0.2\n",
    "    X_list.append(np.column_stack([r * np.sin(t), r * np.cos(t)]))\n",
    "    y_list.append(np.full(N_per_class, k))\n",
    "\n",
    "X_train = np.vstack(X_list)\n",
    "y_train = np.hstack(y_list).astype(int)\n",
    "\n",
    "# 打亂\n",
    "perm = np.random.permutation(len(y_train))\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# 視覺化資料\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for k in range(num_classes):\n",
    "    mask = y_train == k\n",
    "    plt.scatter(X_train[mask, 0], X_train[mask, 1], c=colors[k], label=f'Class {k}', alpha=0.5)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Spiral Dataset (3 classes)')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練網路\n",
    "np.random.seed(42)\n",
    "net = MultiLayerClassifier(input_dim=2, hidden_dims=[100, 50], num_classes=3)\n",
    "\n",
    "lr = 1.0\n",
    "epochs = 1000\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward + Loss\n",
    "    loss = net.loss(X_train, y_train)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Accuracy\n",
    "    pred = net.predict(X_train)\n",
    "    acc = np.mean(pred == y_train)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    # Backward\n",
    "    net.backward()\n",
    "    \n",
    "    # Update (SGD)\n",
    "    for param, grad in net.get_params_and_grads():\n",
    "        param -= lr * grad\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\n最終 Loss: {losses[-1]:.4f}\")\n",
    "print(f\"最終 Accuracy: {accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化結果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss 曲線\n",
    "ax = axes[0]\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy 曲線\n",
    "ax = axes[1]\n",
    "ax.plot(accuracies)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Training Accuracy')\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 決策邊界\n",
    "ax = axes[2]\n",
    "\n",
    "# 產生網格\n",
    "x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
    "y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "# 預測\n",
    "Z = net.predict(grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# 畫決策邊界\n",
    "ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "\n",
    "# 畫資料點\n",
    "for k in range(num_classes):\n",
    "    mask = y_train == k\n",
    "    ax.scatter(X_train[mask, 0], X_train[mask, 1], c=colors[k], label=f'Class {k}', \n",
    "               edgecolors='black', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_title('Decision Boundary')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們學習了：\n",
    "\n",
    "1. **激活函數的作用**：引入非線性，讓多層網路能學習複雜函數\n",
    "\n",
    "2. **常見激活函數**：\n",
    "\n",
    "| 函數 | 公式 | 梯度 | 特點 |\n",
    "|------|------|------|------|\n",
    "| ReLU | $\\max(0, x)$ | $1_{x>0}$ | 簡單高效，可能 dead |\n",
    "| Leaky ReLU | $\\max(\\alpha x, x)$ | $1$ or $\\alpha$ | 解決 dead ReLU |\n",
    "| Sigmoid | $\\frac{1}{1+e^{-x}}$ | $\\sigma(1-\\sigma)$ | 輸出機率，梯度消失 |\n",
    "| Tanh | $\\tanh(x)$ | $1-\\tanh^2(x)$ | Zero-centered |\n",
    "| ELU | $x$ or $\\alpha(e^x-1)$ | $1$ or $\\alpha e^x$ | 平滑，近 zero-mean |\n",
    "\n",
    "3. **Softmax + Cross-Entropy**：\n",
    "   - 合併計算以保持數值穩定\n",
    "   - 梯度超級簡潔：$p - y$\n",
    "\n",
    "4. **梯度消失問題**：Sigmoid/Tanh 在深層網路中梯度會指數衰減，ReLU 解決此問題\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們將實作 **Conv2D** 層，這是 CNN 的核心組件！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
