{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Architecture - 語義分割網路\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解語義分割（semantic segmentation）任務\n",
    "2. 理解 encoder-decoder 架構\n",
    "3. 理解 skip connections 在 U-Net 中的作用\n",
    "4. 實作上採樣方法：Nearest neighbor, Bilinear, Transposed Convolution\n",
    "5. 實作簡化版 U-Net（含 forward 和 backward）\n",
    "\n",
    "## 參考資料\n",
    "\n",
    "- Ronneberger et al., \"U-Net: Convolutional Networks for Biomedical Image Segmentation\", MICCAI 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：語義分割問題\n",
    "\n",
    "### 1.1 什麼是語義分割？\n",
    "\n",
    "- **分類（Classification）**：整張圖一個標籤，例如「這是貓」\n",
    "- **物件偵測（Detection）**：找出物體的 bounding box\n",
    "- **語義分割（Semantic Segmentation）**：**每個像素**都有一個類別標籤\n",
    "\n",
    "```\n",
    "輸入：(H, W, 3) 彩色圖片\n",
    "輸出：(H, W) 每個像素的類別\n",
    "     或 (H, W, C) 每個像素在 C 個類別上的機率\n",
    "```\n",
    "\n",
    "### 1.2 挑戰\n",
    "\n",
    "- 輸出和輸入**大小相同**\n",
    "- 需要同時有：\n",
    "  - **High-level features**: 理解語義（這是什麼物體）\n",
    "  - **Low-level features**: 保留空間細節（邊界在哪裡）\n",
    "\n",
    "普通 CNN 經過多層 pooling 後，特徵圖變得很小，無法恢復細節。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範：語義分割任務\n",
    "\n",
    "def create_segmentation_example():\n",
    "    \"\"\"創建一個簡單的分割任務示例\"\"\"\n",
    "    img_size = 64\n",
    "    \n",
    "    # 創建圖片（帶有圓形和方形）\n",
    "    image = np.zeros((img_size, img_size))\n",
    "    mask = np.zeros((img_size, img_size), dtype=int)  # 0: background, 1: circle, 2: square\n",
    "    \n",
    "    # 畫圓形\n",
    "    y, x = np.ogrid[:img_size, :img_size]\n",
    "    cx, cy, r = 20, 20, 12\n",
    "    circle_mask = (x - cx)**2 + (y - cy)**2 <= r**2\n",
    "    image[circle_mask] = 0.8\n",
    "    mask[circle_mask] = 1\n",
    "    \n",
    "    # 畫方形\n",
    "    sx, sy, size = 40, 35, 15\n",
    "    image[sy:sy+size, sx:sx+size] = 0.6\n",
    "    mask[sy:sy+size, sx:sx+size] = 2\n",
    "    \n",
    "    # 加噪音\n",
    "    image += np.random.randn(img_size, img_size) * 0.1\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "image, mask = create_segmentation_example()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask, cmap='tab10', vmin=0, vmax=3)\n",
    "axes[1].set_title('Ground Truth Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# 顯示 overlay\n",
    "axes[2].imshow(image, cmap='gray')\n",
    "axes[2].imshow(mask, cmap='tab10', alpha=0.5, vmin=0, vmax=3)\n",
    "axes[2].set_title('Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"類別：0=背景(藍), 1=圓形(橙), 2=方形(綠)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：U-Net 架構概述\n",
    "\n",
    "### 2.1 Encoder-Decoder 結構\n",
    "\n",
    "```\n",
    "                        U-Net Architecture\n",
    "    \n",
    "    Encoder (Contracting)          Decoder (Expanding)\n",
    "    \n",
    "    [64x64, 64ch] ─────────────────────────────────────> [64x64, n_classes]\n",
    "          │                                                    ↑\n",
    "          │ MaxPool                                   Upsample │\n",
    "          ↓                                                    │\n",
    "    [32x32, 128ch] ─────────────────────────────────> [32x32, 128ch]\n",
    "          │                     Skip Connection              ↑\n",
    "          │ MaxPool                                   Upsample │\n",
    "          ↓                                                    │\n",
    "    [16x16, 256ch] ─────────────────────────────────> [16x16, 256ch]\n",
    "          │                     Skip Connection              ↑\n",
    "          │ MaxPool                                   Upsample │\n",
    "          ↓                                                    │\n",
    "    [8x8, 512ch]  ──────────────────────────────────> [8x8, 512ch]\n",
    "          │                                                    ↑\n",
    "          └──────────> [Bottleneck] ───────────────────────────┘\n",
    "```\n",
    "\n",
    "### 2.2 Skip Connections 的作用\n",
    "\n",
    "- **問題**：Decoder 需要恢復空間細節，但經過多次 downsampling 後已經丟失\n",
    "- **解法**：把 Encoder 對應層的特徵直接「跳接」到 Decoder\n",
    "- **好處**：\n",
    "  - Decoder 同時有 high-level（來自 bottleneck）和 low-level（來自 skip）特徵\n",
    "  - 梯度可以更直接地流回 Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：基礎組件\n",
    "\n",
    "從之前的 notebooks 引入並擴展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im2col 和 col2im（從 02_resnet_block.ipynb）\n",
    "\n",
    "def im2col(x, kH, kW, stride=1, pad=0):\n",
    "    \"\"\"將 4D tensor 展開成 2D matrix\"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    if pad > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    shape = (N, C, kH, kW, out_H, out_W)\n",
    "    strides = (x.strides[0], x.strides[1], x.strides[2], x.strides[3],\n",
    "               x.strides[2] * stride, x.strides[3] * stride)\n",
    "    \n",
    "    col = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_H * out_W, -1)\n",
    "    \n",
    "    return col\n",
    "\n",
    "def col2im(col, x_shape, kH, kW, stride=1, pad=0):\n",
    "    \"\"\"im2col 的逆操作\"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_pad = H + 2 * pad\n",
    "    W_pad = W + 2 * pad\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    col = col.reshape(N, out_H, out_W, C, kH, kW).transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    x_pad = np.zeros((N, C, H_pad, W_pad))\n",
    "    \n",
    "    for i in range(kH):\n",
    "        i_max = i + stride * out_H\n",
    "        for j in range(kW):\n",
    "            j_max = j + stride * out_W\n",
    "            x_pad[:, :, i:i_max:stride, j:j_max:stride] += col[:, :, i, j, :, :]\n",
    "    \n",
    "    if pad > 0:\n",
    "        return x_pad[:, :, pad:-pad, pad:-pad]\n",
    "    return x_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"2D Convolution Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        H_out = (H + 2 * self.padding - kH) // self.stride + 1\n",
    "        W_out = (W + 2 * self.padding - kW) // self.stride + 1\n",
    "        \n",
    "        col = im2col(x, kH, kW, self.stride, self.padding)\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        \n",
    "        out = col @ W_col.T + self.b\n",
    "        out = out.reshape(N, H_out, W_out, self.out_channels).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.cache = (x, col)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x, col = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, self.out_channels)\n",
    "        \n",
    "        self.dW = (dout_reshaped.T @ col).reshape(self.W.shape)\n",
    "        self.db = dout_reshaped.sum(axis=0)\n",
    "        \n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        dcol = dout_reshaped @ W_col\n",
    "        dx = col2im(dcol, x.shape, kH, kW, self.stride, self.padding)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2D:\n",
    "    \"\"\"Batch Normalization for Conv layers\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        self.cache = None\n",
    "        self.training = True\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        if self.training:\n",
    "            mean = x.mean(axis=(0, 2, 3))\n",
    "            var = x.var(axis=(0, 2, 3))\n",
    "            \n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = (x - mean.reshape(1, C, 1, 1)) / std.reshape(1, C, 1, 1)\n",
    "        out = self.gamma.reshape(1, C, 1, 1) * x_norm + self.beta.reshape(1, C, 1, 1)\n",
    "        \n",
    "        if self.training:\n",
    "            self.cache = (x, x_norm, mean, var, std)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x, x_norm, mean, var, std = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        m = N * H * W\n",
    "        \n",
    "        self.dgamma = (dout * x_norm).sum(axis=(0, 2, 3))\n",
    "        self.dbeta = dout.sum(axis=(0, 2, 3))\n",
    "        \n",
    "        dx_norm = dout * self.gamma.reshape(1, C, 1, 1)\n",
    "        \n",
    "        dvar = (-0.5 * dx_norm * (x - mean.reshape(1, C, 1, 1)) / \n",
    "                (var.reshape(1, C, 1, 1) + self.eps) ** 1.5).sum(axis=(0, 2, 3))\n",
    "        \n",
    "        dmean = (-dx_norm / std.reshape(1, C, 1, 1)).sum(axis=(0, 2, 3))\n",
    "        dmean += dvar * (-2 / m) * (x - mean.reshape(1, C, 1, 1)).sum(axis=(0, 2, 3))\n",
    "        \n",
    "        dx = dx_norm / std.reshape(1, C, 1, 1)\n",
    "        dx += (2 / m) * dvar.reshape(1, C, 1, 1) * (x - mean.reshape(1, C, 1, 1))\n",
    "        dx += dmean.reshape(1, C, 1, 1) / m\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache\n",
    "        return dout * (x > 0)\n",
    "\n",
    "\n",
    "class MaxPool2D:\n",
    "    \"\"\"Max Pooling 2D\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        s = self.stride\n",
    "        \n",
    "        out_H = (H - kH) // s + 1\n",
    "        out_W = (W - kW) // s + 1\n",
    "        \n",
    "        # Reshape for pooling\n",
    "        x_reshaped = x.reshape(N, C, out_H, s, out_W, s)\n",
    "        \n",
    "        # 只有在 stride == kernel_size 時才能這樣做\n",
    "        if s == kH:\n",
    "            out = x_reshaped.max(axis=(3, 5))\n",
    "            \n",
    "            # 記錄 max 的位置\n",
    "            x_col = x_reshaped.transpose(0, 1, 2, 4, 3, 5).reshape(N, C, out_H, out_W, -1)\n",
    "            max_idx = x_col.argmax(axis=-1)\n",
    "            self.cache = (x.shape, max_idx)\n",
    "        else:\n",
    "            # 通用情況：用 im2col\n",
    "            col = im2col(x, kH, kW, s, 0)\n",
    "            col = col.reshape(-1, C, kH * kW)\n",
    "            out = col.max(axis=2)\n",
    "            max_idx = col.argmax(axis=2)\n",
    "            out = out.reshape(N, out_H, out_W, C).transpose(0, 3, 1, 2)\n",
    "            self.cache = (x.shape, max_idx, col.shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x_shape, max_idx = self.cache[:2]\n",
    "        N, C, H, W = x_shape\n",
    "        kH = kW = self.kernel_size\n",
    "        s = self.stride\n",
    "        out_H, out_W = dout.shape[2], dout.shape[3]\n",
    "        \n",
    "        if s == kH:\n",
    "            dx = np.zeros(x_shape)\n",
    "            \n",
    "            # 將梯度放回 max 位置\n",
    "            for n in range(N):\n",
    "                for c in range(C):\n",
    "                    for i in range(out_H):\n",
    "                        for j in range(out_W):\n",
    "                            idx = max_idx[n, c, i, j]\n",
    "                            ii, jj = idx // kW, idx % kW\n",
    "                            dx[n, c, i*s + ii, j*s + jj] = dout[n, c, i, j]\n",
    "        else:\n",
    "            # 通用情況\n",
    "            col_shape = self.cache[2]\n",
    "            dcol = np.zeros(col_shape)\n",
    "            dout_flat = dout.transpose(0, 2, 3, 1).flatten()\n",
    "            \n",
    "            # 這裡簡化處理\n",
    "            dx = np.zeros(x_shape)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：上採樣方法\n",
    "\n",
    "Decoder 需要把特徵圖從小變大，有幾種方法：\n",
    "\n",
    "1. **Nearest Neighbor Upsampling**: 最簡單，直接複製\n",
    "2. **Bilinear Interpolation**: 線性插值\n",
    "3. **Transposed Convolution**: 可學習的上採樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestUpsample:\n",
    "    \"\"\"\n",
    "    Nearest Neighbor Upsampling\n",
    "    \n",
    "    最簡單的上採樣：直接把每個值複製到周圍的位置\n",
    "    \n",
    "    例如 scale=2:\n",
    "    [[1, 2],     [[1, 1, 2, 2],\n",
    "     [3, 4]]  ->  [1, 1, 2, 2],\n",
    "                  [3, 3, 4, 4],\n",
    "                  [3, 3, 4, 4]]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_factor=2):\n",
    "        self.scale_factor = scale_factor\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C, H*scale, W*scale)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        s = self.scale_factor\n",
    "        \n",
    "        # 使用 repeat 來複製\n",
    "        out = x.repeat(s, axis=2).repeat(s, axis=3)\n",
    "        \n",
    "        self.cache = x.shape\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward: 把對應位置的梯度加起來\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, C, H*scale, W*scale)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray, shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = self.cache\n",
    "        s = self.scale_factor\n",
    "        \n",
    "        # Reshape 然後 sum\n",
    "        # (N, C, H*s, W*s) -> (N, C, H, s, W, s) -> sum over s dimensions\n",
    "        dx = dout.reshape(N, C, H, s, W, s).sum(axis=(3, 5))\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 NearestUpsample\n",
    "\n",
    "x = np.array([[[[1, 2],\n",
    "                [3, 4]]]], dtype=float)\n",
    "\n",
    "print(\"Input (1, 1, 2, 2):\")\n",
    "print(x[0, 0])\n",
    "\n",
    "upsample = NearestUpsample(scale_factor=2)\n",
    "out = upsample.forward(x)\n",
    "\n",
    "print(\"\\nOutput (1, 1, 4, 4):\")\n",
    "print(out[0, 0])\n",
    "\n",
    "# 測試 backward\n",
    "dout = np.ones_like(out)\n",
    "dx = upsample.backward(dout)\n",
    "\n",
    "print(\"\\ndout (all ones):\")\n",
    "print(dout[0, 0])\n",
    "\n",
    "print(\"\\ndx (should be 4 for each element, because 2x2 gradients sum up):\")\n",
    "print(dx[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearUpsample:\n",
    "    \"\"\"\n",
    "    Bilinear Interpolation Upsampling\n",
    "    \n",
    "    使用雙線性插值來上採樣，比 nearest neighbor 更平滑\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_factor=2):\n",
    "        self.scale_factor = scale_factor\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C, H*scale, W*scale)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        s = self.scale_factor\n",
    "        new_H, new_W = H * s, W * s\n",
    "        \n",
    "        out = np.zeros((N, C, new_H, new_W))\n",
    "        \n",
    "        # 計算每個輸出位置對應的輸入位置\n",
    "        # 使用 align_corners=False 的方式\n",
    "        for i in range(new_H):\n",
    "            for j in range(new_W):\n",
    "                # 輸出 (i, j) 對應輸入的位置\n",
    "                src_i = (i + 0.5) / s - 0.5\n",
    "                src_j = (j + 0.5) / s - 0.5\n",
    "                \n",
    "                # 找到四個鄰居\n",
    "                i0 = int(np.floor(src_i))\n",
    "                j0 = int(np.floor(src_j))\n",
    "                i1 = i0 + 1\n",
    "                j1 = j0 + 1\n",
    "                \n",
    "                # 權重\n",
    "                wi = src_i - i0\n",
    "                wj = src_j - j0\n",
    "                \n",
    "                # Clamp indices\n",
    "                i0 = max(0, min(i0, H - 1))\n",
    "                i1 = max(0, min(i1, H - 1))\n",
    "                j0 = max(0, min(j0, W - 1))\n",
    "                j1 = max(0, min(j1, W - 1))\n",
    "                \n",
    "                # Bilinear interpolation\n",
    "                out[:, :, i, j] = ((1 - wi) * (1 - wj) * x[:, :, i0, j0] +\n",
    "                                   (1 - wi) * wj * x[:, :, i0, j1] +\n",
    "                                   wi * (1 - wj) * x[:, :, i1, j0] +\n",
    "                                   wi * wj * x[:, :, i1, j1])\n",
    "        \n",
    "        self.cache = (x.shape, s)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass for bilinear upsampling\n",
    "        \"\"\"\n",
    "        x_shape, s = self.cache\n",
    "        N, C, H, W = x_shape\n",
    "        new_H, new_W = H * s, W * s\n",
    "        \n",
    "        dx = np.zeros(x_shape)\n",
    "        \n",
    "        for i in range(new_H):\n",
    "            for j in range(new_W):\n",
    "                src_i = (i + 0.5) / s - 0.5\n",
    "                src_j = (j + 0.5) / s - 0.5\n",
    "                \n",
    "                i0 = int(np.floor(src_i))\n",
    "                j0 = int(np.floor(src_j))\n",
    "                i1 = i0 + 1\n",
    "                j1 = j0 + 1\n",
    "                \n",
    "                wi = src_i - i0\n",
    "                wj = src_j - j0\n",
    "                \n",
    "                # Clamp\n",
    "                i0_c = max(0, min(i0, H - 1))\n",
    "                i1_c = max(0, min(i1, H - 1))\n",
    "                j0_c = max(0, min(j0, W - 1))\n",
    "                j1_c = max(0, min(j1, W - 1))\n",
    "                \n",
    "                # 反向傳播：把 dout 分配回去\n",
    "                d = dout[:, :, i, j]\n",
    "                dx[:, :, i0_c, j0_c] += (1 - wi) * (1 - wj) * d\n",
    "                dx[:, :, i0_c, j1_c] += (1 - wi) * wj * d\n",
    "                dx[:, :, i1_c, j0_c] += wi * (1 - wj) * d\n",
    "                dx[:, :, i1_c, j1_c] += wi * wj * d\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 Nearest 和 Bilinear\n",
    "\n",
    "x = np.random.randn(1, 1, 4, 4)\n",
    "\n",
    "nearest = NearestUpsample(scale_factor=4)\n",
    "bilinear = BilinearUpsample(scale_factor=4)\n",
    "\n",
    "out_nearest = nearest.forward(x)\n",
    "out_bilinear = bilinear.forward(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(x[0, 0], cmap='viridis')\n",
    "axes[0].set_title('Original (4x4)')\n",
    "\n",
    "axes[1].imshow(out_nearest[0, 0], cmap='viridis')\n",
    "axes[1].set_title('Nearest Neighbor (16x16)')\n",
    "\n",
    "axes[2].imshow(out_bilinear[0, 0], cmap='viridis')\n",
    "axes[2].set_title('Bilinear (16x16)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：Bilinear 產生更平滑的結果\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposedConv2D:\n",
    "    \"\"\"\n",
    "    Transposed Convolution (Deconvolution)\n",
    "    \n",
    "    可學習的上採樣。實際上是正常卷積的「反向」操作。\n",
    "    \n",
    "    Forward: 把小的特徵圖變大\n",
    "    Backward: 做正常的卷積\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=2, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # 初始化權重\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.W = np.random.randn(in_channels, out_channels, kernel_size, kernel_size) * scale\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C_in, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \n",
    "        H_out = stride * (H - 1) + kernel_size - 2 * padding\n",
    "        \"\"\"\n",
    "        N, C_in, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        p = self.padding\n",
    "        \n",
    "        # 計算輸出大小\n",
    "        H_out = s * (H - 1) + k - 2 * p\n",
    "        W_out = s * (W - 1) + k - 2 * p\n",
    "        \n",
    "        # 初始化輸出（帶 padding）\n",
    "        out_padded = np.zeros((N, self.out_channels, H_out + 2 * p, W_out + 2 * p))\n",
    "        \n",
    "        # 對每個輸入位置，把對應的 kernel 加到輸出\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                # 輸出區域的起始位置\n",
    "                h_start = i * s\n",
    "                w_start = j * s\n",
    "                \n",
    "                # x[:, :, i, j] shape: (N, C_in)\n",
    "                # W shape: (C_in, C_out, k, k)\n",
    "                # 結果: (N, C_out, k, k)\n",
    "                contribution = np.einsum('nc,cokl->nokl', x[:, :, i, j], self.W)\n",
    "                out_padded[:, :, h_start:h_start+k, w_start:w_start+k] += contribution\n",
    "        \n",
    "        # 加 bias\n",
    "        out_padded += self.b.reshape(1, -1, 1, 1)\n",
    "        \n",
    "        # 移除 padding\n",
    "        if p > 0:\n",
    "            out = out_padded[:, :, p:-p, p:-p]\n",
    "        else:\n",
    "            out = out_padded\n",
    "        \n",
    "        self.cache = (x, H_out, W_out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \n",
    "        關鍵觀察：TransposedConv 的 backward 就是正常 Conv 的 forward\n",
    "        \"\"\"\n",
    "        x, H_out, W_out = self.cache\n",
    "        N, C_in, H, W = x.shape\n",
    "        k = self.kernel_size\n",
    "        s = self.stride\n",
    "        p = self.padding\n",
    "        \n",
    "        # Pad dout\n",
    "        if p > 0:\n",
    "            dout_padded = np.pad(dout, ((0, 0), (0, 0), (p, p), (p, p)))\n",
    "        else:\n",
    "            dout_padded = dout\n",
    "        \n",
    "        # Gradient for input x\n",
    "        dx = np.zeros_like(x)\n",
    "        \n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                h_start = i * s\n",
    "                w_start = j * s\n",
    "                \n",
    "                # dout_padded[:, :, h_start:h_start+k, w_start:w_start+k] shape: (N, C_out, k, k)\n",
    "                # W shape: (C_in, C_out, k, k)\n",
    "                # 結果: (N, C_in)\n",
    "                dout_patch = dout_padded[:, :, h_start:h_start+k, w_start:w_start+k]\n",
    "                dx[:, :, i, j] = np.einsum('nokl,cokl->nc', dout_patch, self.W)\n",
    "        \n",
    "        # Gradient for W\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        \n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                h_start = i * s\n",
    "                w_start = j * s\n",
    "                \n",
    "                # x[:, :, i, j] shape: (N, C_in)\n",
    "                # dout_padded[:, :, h_start:h_start+k, w_start:w_start+k] shape: (N, C_out, k, k)\n",
    "                dout_patch = dout_padded[:, :, h_start:h_start+k, w_start:w_start+k]\n",
    "                self.dW += np.einsum('nc,nokl->cokl', x[:, :, i, j], dout_patch)\n",
    "        \n",
    "        # Gradient for b\n",
    "        self.db = dout.sum(axis=(0, 2, 3))\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 TransposedConv2D\n",
    "\n",
    "x = np.random.randn(2, 8, 4, 4)  # 輸入 4x4\n",
    "trans_conv = TransposedConv2D(in_channels=8, out_channels=4, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "out = trans_conv.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected: (2, 4, 8, 8) - doubled spatial dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：U-Net 組件\n",
    "\n",
    "### 5.1 Encoder Block\n",
    "\n",
    "```\n",
    "Conv3x3 -> BN -> ReLU -> Conv3x3 -> BN -> ReLU -> MaxPool\n",
    "                                              ↓\n",
    "                                         (feature for skip)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv:\n",
    "    \"\"\"\n",
    "    U-Net 的基本卷積單元：\n",
    "    Conv3x3 -> BN -> ReLU -> Conv3x3 -> BN -> ReLU\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = BatchNorm2D(out_channels)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = BatchNorm2D(out_channels)\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        \n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def train(self):\n",
    "        self.bn1.training = True\n",
    "        self.bn2.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.bn1.training = False\n",
    "        self.bn2.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock:\n",
    "    \"\"\"\n",
    "    U-Net Encoder Block:\n",
    "    DoubleConv -> MaxPool\n",
    "    \n",
    "    返回：pooled output 和 skip connection 的特徵\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = MaxPool2D(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.skip_features = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pooled : np.ndarray - 給下一層 encoder 用\n",
    "        skip : np.ndarray - 給對應的 decoder 用（skip connection）\n",
    "        \"\"\"\n",
    "        # Double conv\n",
    "        features = self.double_conv.forward(x)\n",
    "        \n",
    "        # 保存 skip connection 的特徵（pool 之前）\n",
    "        self.skip_features = features\n",
    "        \n",
    "        # Pool\n",
    "        pooled = self.pool.forward(features)\n",
    "        \n",
    "        return pooled, features\n",
    "    \n",
    "    def backward(self, dout, dskip=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray - 來自下一層 encoder 的梯度\n",
    "        dskip : np.ndarray - 來自 skip connection 的梯度（從 decoder）\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray\n",
    "        \"\"\"\n",
    "        # Backward through pool\n",
    "        d_features = self.pool.backward(dout)\n",
    "        \n",
    "        # 加上 skip connection 的梯度\n",
    "        if dskip is not None:\n",
    "            d_features = d_features + dskip\n",
    "        \n",
    "        # Backward through double conv\n",
    "        dx = self.double_conv.backward(d_features)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def train(self):\n",
    "        self.double_conv.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.double_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock:\n",
    "    \"\"\"\n",
    "    U-Net Decoder Block:\n",
    "    Upsample -> Concat(skip) -> DoubleConv\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, upsample_method='nearest'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            輸入 channels（來自 bottleneck 或前一個 decoder）\n",
    "        out_channels : int\n",
    "            輸出 channels\n",
    "        upsample_method : str\n",
    "            'nearest' or 'bilinear' or 'transposed'\n",
    "        \"\"\"\n",
    "        self.upsample_method = upsample_method\n",
    "        \n",
    "        # Upsampling\n",
    "        if upsample_method == 'nearest':\n",
    "            self.upsample = NearestUpsample(scale_factor=2)\n",
    "        elif upsample_method == 'bilinear':\n",
    "            self.upsample = BilinearUpsample(scale_factor=2)\n",
    "        else:\n",
    "            self.upsample = TransposedConv2D(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        # Concat 後 channels 翻倍（in_channels + skip_channels）\n",
    "        # 假設 skip_channels = in_channels\n",
    "        self.double_conv = DoubleConv(in_channels * 2, out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray - 來自前一層的特徵\n",
    "        skip : np.ndarray - 來自 encoder 的 skip connection\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray\n",
    "        \"\"\"\n",
    "        # Upsample\n",
    "        upsampled = self.upsample.forward(x)\n",
    "        \n",
    "        # Concat along channel dimension\n",
    "        # upsampled: (N, C, H, W), skip: (N, C_skip, H, W)\n",
    "        concat = np.concatenate([upsampled, skip], axis=1)\n",
    "        \n",
    "        # Double conv\n",
    "        out = self.double_conv.forward(concat)\n",
    "        \n",
    "        self.cache = (x, skip, upsampled)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray - 給前一層 decoder/bottleneck 的梯度\n",
    "        dskip : np.ndarray - 給 encoder 的梯度（通過 skip connection）\n",
    "        \"\"\"\n",
    "        x, skip, upsampled = self.cache\n",
    "        \n",
    "        # Backward through double conv\n",
    "        d_concat = self.double_conv.backward(dout)\n",
    "        \n",
    "        # Split gradient for concat\n",
    "        C_up = upsampled.shape[1]\n",
    "        d_upsampled = d_concat[:, :C_up, :, :]\n",
    "        dskip = d_concat[:, C_up:, :, :]\n",
    "        \n",
    "        # Backward through upsample\n",
    "        dx = self.upsample.backward(d_upsampled)\n",
    "        \n",
    "        return dx, dskip\n",
    "    \n",
    "    def train(self):\n",
    "        self.double_conv.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.double_conv.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第六部分：完整的 SimpleUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNet:\n",
    "    \"\"\"\n",
    "    簡化版 U-Net\n",
    "    \n",
    "    結構（輸入 64x64）：\n",
    "    \n",
    "    Encoder:\n",
    "        [64x64, 1] -> Enc1 -> [32x32, 32]\n",
    "        [32x32, 32] -> Enc2 -> [16x16, 64]\n",
    "        \n",
    "    Bottleneck:\n",
    "        [16x16, 64] -> DoubleConv -> [16x16, 128]\n",
    "        \n",
    "    Decoder:\n",
    "        [16x16, 128] + skip[16x16, 64] -> Dec2 -> [32x32, 64]\n",
    "        [32x32, 64] + skip[32x32, 32] -> Dec1 -> [64x64, 32]\n",
    "        \n",
    "    Output:\n",
    "        [64x64, 32] -> Conv1x1 -> [64x64, n_classes]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, n_classes=3, base_channels=32):\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = EncoderBlock(in_channels, base_channels)         # 1 -> 32\n",
    "        self.enc2 = EncoderBlock(base_channels, base_channels * 2)   # 32 -> 64\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(base_channels * 2, base_channels * 4)  # 64 -> 128\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec2 = DecoderBlock(base_channels * 4, base_channels * 2)  # 128 -> 64\n",
    "        self.dec1 = DecoderBlock(base_channels * 2, base_channels)       # 64 -> 32\n",
    "        \n",
    "        # Output\n",
    "        self.out_conv = Conv2D(base_channels, n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # 保存 skip features\n",
    "        self.skip1 = None\n",
    "        self.skip2 = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, in_channels, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : np.ndarray, shape (N, n_classes, H, W)\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        x, self.skip1 = self.enc1.forward(x)   # skip1: (N, 32, H, W)\n",
    "        x, self.skip2 = self.enc2.forward(x)   # skip2: (N, 64, H/2, W/2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck.forward(x)         # (N, 128, H/4, W/4)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.dec2.forward(x, self.skip2)   # (N, 64, H/2, W/2)\n",
    "        x = self.dec1.forward(x, self.skip1)   # (N, 32, H, W)\n",
    "        \n",
    "        # Output\n",
    "        logits = self.out_conv.forward(x)      # (N, n_classes, H, W)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \"\"\"\n",
    "        # Output conv\n",
    "        dout = self.out_conv.backward(dlogits)\n",
    "        \n",
    "        # Decoder\n",
    "        dout, dskip1 = self.dec1.backward(dout)\n",
    "        dout, dskip2 = self.dec2.backward(dout)\n",
    "        \n",
    "        # Bottleneck\n",
    "        dout = self.bottleneck.backward(dout)\n",
    "        \n",
    "        # Encoder (需要加上 skip 的梯度)\n",
    "        dout = self.enc2.backward(dout, dskip2)\n",
    "        dout = self.enc1.backward(dout, dskip1)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"獲取所有參數和梯度\"\"\"\n",
    "        params = []\n",
    "        grads = []\n",
    "        \n",
    "        # Helper function\n",
    "        def add_double_conv(dc):\n",
    "            params.extend([dc.conv1.W, dc.conv1.b, dc.bn1.gamma, dc.bn1.beta,\n",
    "                          dc.conv2.W, dc.conv2.b, dc.bn2.gamma, dc.bn2.beta])\n",
    "            grads.extend([dc.conv1.dW, dc.conv1.db, dc.bn1.dgamma, dc.bn1.dbeta,\n",
    "                         dc.conv2.dW, dc.conv2.db, dc.bn2.dgamma, dc.bn2.dbeta])\n",
    "        \n",
    "        # Encoders\n",
    "        add_double_conv(self.enc1.double_conv)\n",
    "        add_double_conv(self.enc2.double_conv)\n",
    "        \n",
    "        # Bottleneck\n",
    "        add_double_conv(self.bottleneck)\n",
    "        \n",
    "        # Decoders\n",
    "        if hasattr(self.dec2.upsample, 'W'):  # TransposedConv\n",
    "            params.extend([self.dec2.upsample.W, self.dec2.upsample.b])\n",
    "            grads.extend([self.dec2.upsample.dW, self.dec2.upsample.db])\n",
    "        add_double_conv(self.dec2.double_conv)\n",
    "        \n",
    "        if hasattr(self.dec1.upsample, 'W'):\n",
    "            params.extend([self.dec1.upsample.W, self.dec1.upsample.b])\n",
    "            grads.extend([self.dec1.upsample.dW, self.dec1.upsample.db])\n",
    "        add_double_conv(self.dec1.double_conv)\n",
    "        \n",
    "        # Output\n",
    "        params.extend([self.out_conv.W, self.out_conv.b])\n",
    "        grads.extend([self.out_conv.dW, self.out_conv.db])\n",
    "        \n",
    "        return params, grads\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Set to training mode\"\"\"\n",
    "        self.enc1.train()\n",
    "        self.enc2.train()\n",
    "        self.bottleneck.train()\n",
    "        self.dec1.train()\n",
    "        self.dec2.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        self.enc1.eval()\n",
    "        self.enc2.eval()\n",
    "        self.bottleneck.eval()\n",
    "        self.dec1.eval()\n",
    "        self.dec2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 SimpleUNet\n",
    "\n",
    "model = SimpleUNet(in_channels=1, n_classes=3, base_channels=16)\n",
    "x = np.random.randn(2, 1, 64, 64)\n",
    "\n",
    "logits = model.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Expected: (2, 3, 64, 64) - same spatial size as input\")\n",
    "\n",
    "# 統計參數\n",
    "params, _ = model.get_params_and_grads()\n",
    "total_params = sum(p.size for p in params if p is not None)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第七部分：訓練 U-Net 做分割任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成訓練數據\n",
    "\n",
    "def generate_segmentation_data(n_samples, img_size=64):\n",
    "    \"\"\"生成簡單的分割數據集\n",
    "    \n",
    "    3 類：背景(0), 圓形(1), 方形(2)\n",
    "    \"\"\"\n",
    "    X = np.zeros((n_samples, 1, img_size, img_size))\n",
    "    y = np.zeros((n_samples, img_size, img_size), dtype=int)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 隨機背景噪音\n",
    "        X[i, 0] = np.random.randn(img_size, img_size) * 0.1\n",
    "        \n",
    "        # 隨機畫 0-2 個圓形\n",
    "        n_circles = np.random.randint(0, 3)\n",
    "        for _ in range(n_circles):\n",
    "            cx = np.random.randint(12, img_size - 12)\n",
    "            cy = np.random.randint(12, img_size - 12)\n",
    "            r = np.random.randint(6, 12)\n",
    "            \n",
    "            yy, xx = np.ogrid[:img_size, :img_size]\n",
    "            mask = (xx - cx)**2 + (yy - cy)**2 <= r**2\n",
    "            X[i, 0, mask] = 0.7 + np.random.rand() * 0.3\n",
    "            y[i, mask] = 1\n",
    "        \n",
    "        # 隨機畫 0-2 個方形\n",
    "        n_squares = np.random.randint(0, 3)\n",
    "        for _ in range(n_squares):\n",
    "            sx = np.random.randint(5, img_size - 20)\n",
    "            sy = np.random.randint(5, img_size - 20)\n",
    "            size = np.random.randint(8, 15)\n",
    "            \n",
    "            X[i, 0, sy:sy+size, sx:sx+size] = 0.5 + np.random.rand() * 0.3\n",
    "            y[i, sy:sy+size, sx:sx+size] = 2\n",
    "    \n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "# 生成數據\n",
    "np.random.seed(42)\n",
    "X_train, y_train = generate_segmentation_data(200, img_size=64)\n",
    "X_test, y_test = generate_segmentation_data(50, img_size=64)\n",
    "\n",
    "print(f\"Training set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# 顯示樣本\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    axes[0, i].imshow(X_train[i, 0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Image {i}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(y_train[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    axes[1, i].set_title(f'Mask {i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_wise_cross_entropy(logits, y):\n",
    "    \"\"\"計算 pixel-wise cross entropy loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : np.ndarray, shape (N, C, H, W)\n",
    "    y : np.ndarray, shape (N, H, W) - integer labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    dlogits : np.ndarray, shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    N, C, H, W = logits.shape\n",
    "    \n",
    "    # Softmax (along channel dimension)\n",
    "    logits_max = logits.max(axis=1, keepdims=True)\n",
    "    exp_logits = np.exp(logits - logits_max)\n",
    "    probs = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Gather probabilities for correct class\n",
    "    # 需要對每個像素位置提取正確類別的機率\n",
    "    n_idx = np.arange(N).reshape(-1, 1, 1)\n",
    "    h_idx = np.arange(H).reshape(1, -1, 1)\n",
    "    w_idx = np.arange(W).reshape(1, 1, -1)\n",
    "    \n",
    "    correct_probs = probs[n_idx, y, h_idx, w_idx]\n",
    "    \n",
    "    # Cross entropy loss\n",
    "    loss = -np.mean(np.log(correct_probs + 1e-8))\n",
    "    \n",
    "    # Gradient\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[n_idx, y, h_idx, w_idx] -= 1\n",
    "    dlogits /= (N * H * W)\n",
    "    \n",
    "    return loss, dlogits\n",
    "\n",
    "\n",
    "def pixel_accuracy(logits, y):\n",
    "    \"\"\"計算像素準確率\"\"\"\n",
    "    preds = logits.argmax(axis=1)  # (N, H, W)\n",
    "    return (preds == y).mean()\n",
    "\n",
    "\n",
    "def iou_score(logits, y, n_classes=3):\n",
    "    \"\"\"計算 mean IoU (Intersection over Union)\"\"\"\n",
    "    preds = logits.argmax(axis=1)\n",
    "    ious = []\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        pred_c = (preds == c)\n",
    "        true_c = (y == c)\n",
    "        \n",
    "        intersection = (pred_c & true_c).sum()\n",
    "        union = (pred_c | true_c).sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            ious.append(intersection / union)\n",
    "    \n",
    "    return np.mean(ious) if ious else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, X_train, y_train, X_test, y_test,\n",
    "               epochs=20, batch_size=8, lr=0.01, momentum=0.9):\n",
    "    \"\"\"訓練 U-Net\"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    # Momentum velocities\n",
    "    params, _ = model.get_params_and_grads()\n",
    "    velocities = [np.zeros_like(p) if p is not None else None for p in params]\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    train_ious = []\n",
    "    test_accs = []\n",
    "    test_ious = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_iou = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward\n",
    "            logits = model.forward(X_batch)\n",
    "            loss, dlogits = pixel_wise_cross_entropy(logits, y_batch)\n",
    "            \n",
    "            # Backward\n",
    "            model.backward(dlogits)\n",
    "            \n",
    "            # Update\n",
    "            params, grads = model.get_params_and_grads()\n",
    "            for j, (p, g) in enumerate(zip(params, grads)):\n",
    "                if p is not None and g is not None:\n",
    "                    velocities[j] = momentum * velocities[j] - lr * g\n",
    "                    p += velocities[j]\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += pixel_accuracy(logits, y_batch)\n",
    "            epoch_iou += iou_score(logits, y_batch)\n",
    "        \n",
    "        # Epoch metrics\n",
    "        train_loss = epoch_loss / n_batches\n",
    "        train_acc = epoch_acc / n_batches\n",
    "        train_iou = epoch_iou / n_batches\n",
    "        \n",
    "        # Test metrics\n",
    "        model.eval()\n",
    "        test_logits = model.forward(X_test)\n",
    "        test_acc = pixel_accuracy(test_logits, y_test)\n",
    "        test_iou = iou_score(test_logits, y_test)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_ious.append(train_iou)\n",
    "        test_accs.append(test_acc)\n",
    "        test_ious.append(test_iou)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: loss={train_loss:.4f}, \"\n",
    "                  f\"train_acc={train_acc:.4f}, train_iou={train_iou:.4f}, \"\n",
    "                  f\"test_acc={test_acc:.4f}, test_iou={test_iou:.4f}\")\n",
    "    \n",
    "    return train_losses, train_accs, train_ious, test_accs, test_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 U-Net\n",
    "print(\"Training SimpleUNet...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "unet = SimpleUNet(in_channels=1, n_classes=3, base_channels=16)\n",
    "\n",
    "results = train_unet(\n",
    "    unet, X_train, y_train, X_test, y_test,\n",
    "    epochs=25, batch_size=8, lr=0.01\n",
    ")\n",
    "\n",
    "train_losses, train_accs, train_ious, test_accs, test_ious = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製訓練曲線\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pixel Accuracy\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Pixel Accuracy')\n",
    "axes[1].set_title('Pixel Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU\n",
    "axes[2].plot(train_ious, label='Train')\n",
    "axes[2].plot(test_ious, label='Test')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Mean IoU')\n",
    "axes[2].set_title('Mean IoU')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化預測結果\n",
    "\n",
    "unet.eval()\n",
    "test_logits = unet.forward(X_test[:8])\n",
    "test_preds = test_logits.argmax(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "\n",
    "for i in range(8):\n",
    "    # Input image\n",
    "    axes[0, i].imshow(X_test[i, 0], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Input', fontsize=12)\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[1, i].imshow(y_test[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Ground Truth', fontsize=12)\n",
    "    \n",
    "    # Prediction\n",
    "    axes[2, i].imshow(test_preds[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('Prediction', fontsize=12)\n",
    "\n",
    "plt.suptitle('U-Net Segmentation Results', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n類別：藍色=背景, 橙色=圓形, 綠色=方形\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第八部分：Skip Connection 的重要性\n",
    "\n",
    "為了展示 skip connection 的效果，我們比較有無 skip 的網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleUNetNoSkip:\n",
    "    \"\"\"\n",
    "    沒有 skip connection 的 encoder-decoder 網路\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, n_classes=3, base_channels=32):\n",
    "        # Encoder\n",
    "        self.enc1_conv = DoubleConv(in_channels, base_channels)\n",
    "        self.pool1 = MaxPool2D(2, 2)\n",
    "        \n",
    "        self.enc2_conv = DoubleConv(base_channels, base_channels * 2)\n",
    "        self.pool2 = MaxPool2D(2, 2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(base_channels * 2, base_channels * 4)\n",
    "        \n",
    "        # Decoder (no concat, so channels don't double)\n",
    "        self.up2 = NearestUpsample(2)\n",
    "        self.dec2_conv = DoubleConv(base_channels * 4, base_channels * 2)\n",
    "        \n",
    "        self.up1 = NearestUpsample(2)\n",
    "        self.dec1_conv = DoubleConv(base_channels * 2, base_channels)\n",
    "        \n",
    "        # Output\n",
    "        self.out_conv = Conv2D(base_channels, n_classes, 1, 1, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.enc1_conv.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.enc2_conv.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck.forward(x)\n",
    "        \n",
    "        # Decoder (no skip connections)\n",
    "        x = self.up2.forward(x)\n",
    "        x = self.dec2_conv.forward(x)\n",
    "        \n",
    "        x = self.up1.forward(x)\n",
    "        x = self.dec1_conv.forward(x)\n",
    "        \n",
    "        # Output\n",
    "        return self.out_conv.forward(x)\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        dout = self.out_conv.backward(dlogits)\n",
    "        \n",
    "        dout = self.dec1_conv.backward(dout)\n",
    "        dout = self.up1.backward(dout)\n",
    "        \n",
    "        dout = self.dec2_conv.backward(dout)\n",
    "        dout = self.up2.backward(dout)\n",
    "        \n",
    "        dout = self.bottleneck.backward(dout)\n",
    "        \n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.enc2_conv.backward(dout)\n",
    "        \n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.enc1_conv.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        params = []\n",
    "        grads = []\n",
    "        \n",
    "        def add_double_conv(dc):\n",
    "            params.extend([dc.conv1.W, dc.conv1.b, dc.bn1.gamma, dc.bn1.beta,\n",
    "                          dc.conv2.W, dc.conv2.b, dc.bn2.gamma, dc.bn2.beta])\n",
    "            grads.extend([dc.conv1.dW, dc.conv1.db, dc.bn1.dgamma, dc.bn1.dbeta,\n",
    "                         dc.conv2.dW, dc.conv2.db, dc.bn2.dgamma, dc.bn2.dbeta])\n",
    "        \n",
    "        add_double_conv(self.enc1_conv)\n",
    "        add_double_conv(self.enc2_conv)\n",
    "        add_double_conv(self.bottleneck)\n",
    "        add_double_conv(self.dec2_conv)\n",
    "        add_double_conv(self.dec1_conv)\n",
    "        params.extend([self.out_conv.W, self.out_conv.b])\n",
    "        grads.extend([self.out_conv.dW, self.out_conv.db])\n",
    "        \n",
    "        return params, grads\n",
    "    \n",
    "    def train(self):\n",
    "        for dc in [self.enc1_conv, self.enc2_conv, self.bottleneck,\n",
    "                   self.dec2_conv, self.dec1_conv]:\n",
    "            dc.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for dc in [self.enc1_conv, self.enc2_conv, self.bottleneck,\n",
    "                   self.dec2_conv, self.dec1_conv]:\n",
    "            dc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練沒有 skip connection 的網路\n",
    "print(\"Training UNet WITHOUT skip connections...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "unet_no_skip = SimpleUNetNoSkip(in_channels=1, n_classes=3, base_channels=16)\n",
    "\n",
    "results_no_skip = train_unet(\n",
    "    unet_no_skip, X_train, y_train, X_test, y_test,\n",
    "    epochs=25, batch_size=8, lr=0.01\n",
    ")\n",
    "\n",
    "_, _, train_ious_no_skip, _, test_ious_no_skip = results_no_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較有無 skip connection\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training IoU\n",
    "axes[0].plot(train_ious, 'b-', label='With Skip Connections')\n",
    "axes[0].plot(train_ious_no_skip, 'r--', label='Without Skip Connections')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Mean IoU')\n",
    "axes[0].set_title('Training Mean IoU')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test IoU\n",
    "axes[1].plot(test_ious, 'b-', label='With Skip Connections')\n",
    "axes[1].plot(test_ious_no_skip, 'r--', label='Without Skip Connections')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Mean IoU')\n",
    "axes[1].set_title('Test Mean IoU')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Test IoU:\")\n",
    "print(f\"  With skip connections: {test_ious[-1]:.4f}\")\n",
    "print(f\"  Without skip connections: {test_ious_no_skip[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化比較預測結果\n",
    "\n",
    "unet.eval()\n",
    "unet_no_skip.eval()\n",
    "\n",
    "logits_with_skip = unet.forward(X_test[:4])\n",
    "logits_no_skip = unet_no_skip.forward(X_test[:4])\n",
    "\n",
    "preds_with_skip = logits_with_skip.argmax(axis=1)\n",
    "preds_no_skip = logits_no_skip.argmax(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "titles = ['Input', 'Ground Truth', 'With Skip', 'Without Skip']\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i, 0].imshow(X_test[i, 0], cmap='gray')\n",
    "    axes[i, 1].imshow(y_test[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    axes[i, 2].imshow(preds_with_skip[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    axes[i, 3].imshow(preds_no_skip[i], cmap='tab10', vmin=0, vmax=3)\n",
    "    \n",
    "    for j in range(4):\n",
    "        axes[i, j].axis('off')\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(titles[j])\n",
    "\n",
    "plt.suptitle('Comparison: With vs Without Skip Connections', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：有 skip connection 的網路能更好地保留邊界細節\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### U-Net 的核心設計\n",
    "\n",
    "1. **Encoder-Decoder 結構**\n",
    "   - Encoder：逐步降低解析度，提取 high-level 特徵\n",
    "   - Decoder：逐步恢復解析度，生成 pixel-wise 輸出\n",
    "\n",
    "2. **Skip Connections**\n",
    "   - 把 encoder 的特徵直接接到對應的 decoder\n",
    "   - 保留細節資訊（邊界、紋理）\n",
    "   - 幫助梯度流動\n",
    "\n",
    "3. **上採樣方法**\n",
    "   - Nearest Neighbor：簡單快速\n",
    "   - Bilinear Interpolation：更平滑\n",
    "   - Transposed Convolution：可學習\n",
    "\n",
    "### 實作要點\n",
    "\n",
    "- 每個 encoder block：DoubleConv → MaxPool，保存 skip features\n",
    "- 每個 decoder block：Upsample → Concat(skip) → DoubleConv\n",
    "- Backward 時要正確處理 skip connection 的梯度分流\n",
    "\n",
    "### 應用場景\n",
    "\n",
    "- 醫學影像分割（原始應用）\n",
    "- 語義分割\n",
    "- 實例分割（結合其他技術）\n",
    "- 圖像修復、超解析度等"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
