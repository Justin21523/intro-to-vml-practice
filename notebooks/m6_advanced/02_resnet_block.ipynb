{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Block - 殘差塊\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解深度網路訓練的困難（梯度消失/爆炸、degradation problem）\n",
    "2. 理解 residual connection 的原理和為什麼有效\n",
    "3. 實作 Basic Block（含 forward 和 backward）\n",
    "4. 實作 Bottleneck Block（選做）\n",
    "5. 驗證 residual connection 對梯度流的影響\n",
    "\n",
    "## 參考資料\n",
    "\n",
    "- He et al., \"Deep Residual Learning for Image Recognition\", CVPR 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：深度網路的問題\n",
    "\n",
    "### 1.1 Degradation Problem\n",
    "\n",
    "在 ResNet 之前，人們發現一個奇怪的現象：\n",
    "\n",
    "- 更深的網路反而有**更高的 training error**（不是 test error！）\n",
    "- 這不是 overfitting，因為連訓練集都表現不好\n",
    "- 理論上，更深的網路至少應該和淺的一樣好（多出來的層可以學習 identity mapping）\n",
    "\n",
    "### 1.2 梯度消失/爆炸\n",
    "\n",
    "假設一個 L 層的網路，每層的 Jacobian 為 $J_l$：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial h_L} \\cdot J_L \\cdot J_{L-1} \\cdots J_2 \\cdot \\frac{\\partial h_1}{\\partial W_1}$$\n",
    "\n",
    "如果每個 $||J_l|| < 1$，連乘後會**指數級衰減**（消失）\n",
    "\n",
    "如果每個 $||J_l|| > 1$，連乘後會**指數級增長**（爆炸）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範：梯度消失\n",
    "\n",
    "def simulate_gradient_flow(n_layers, jacobian_norm):\n",
    "    \"\"\"模擬梯度在深層網路中的流動\"\"\"\n",
    "    gradient = 1.0  # 從輸出層的梯度開始\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        # 每經過一層，梯度乘以 Jacobian 的 norm\n",
    "        gradient *= jacobian_norm\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# 比較不同的 Jacobian norm\n",
    "n_layers = 50\n",
    "norms = [0.9, 0.99, 1.0, 1.01, 1.1]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for norm in [0.9, 0.95, 0.99]:\n",
    "    grads = simulate_gradient_flow(n_layers, norm)\n",
    "    plt.plot(grads, label=f'||J|| = {norm}')\n",
    "plt.xlabel('Layer (from output to input)')\n",
    "plt.ylabel('Gradient magnitude')\n",
    "plt.title('Gradient Vanishing (||J|| < 1)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for norm in [1.01, 1.05, 1.1]:\n",
    "    grads = simulate_gradient_flow(n_layers, norm)\n",
    "    plt.plot(grads, label=f'||J|| = {norm}')\n",
    "plt.xlabel('Layer (from output to input)')\n",
    "plt.ylabel('Gradient magnitude')\n",
    "plt.title('Gradient Exploding (||J|| > 1)')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"50 layers with ||J||=0.9: gradient = {0.9**50:.2e}\")\n",
    "print(f\"50 layers with ||J||=1.1: gradient = {1.1**50:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：Residual Connection\n",
    "\n",
    "### 2.1 核心想法\n",
    "\n",
    "原本網路學習：$H(x) = $ 目標映射\n",
    "\n",
    "ResNet 改成學習：$F(x) = H(x) - x$（殘差）\n",
    "\n",
    "輸出變成：$H(x) = F(x) + x$\n",
    "\n",
    "```\n",
    "        ┌──────────────────────┐\n",
    "        │                      │\n",
    "        │        identity      │\n",
    "        │                      │\n",
    "x ──────┼───→ [F(x)] ─────────(+)───→ H(x) = F(x) + x\n",
    "        │                      │\n",
    "        │   Conv → BN → ReLU   │\n",
    "        │   Conv → BN         │\n",
    "        │                      │\n",
    "        └──────────────────────┘\n",
    "```\n",
    "\n",
    "### 2.2 為什麼這樣更好？\n",
    "\n",
    "**從優化角度**：\n",
    "- 如果最優解接近 identity（即 $H(x) \\approx x$），那麼網路只需學習 $F(x) \\approx 0$\n",
    "- 學習 $F(x) = 0$ 比學習 $H(x) = x$ 容易（把權重推向 0 很簡單）\n",
    "\n",
    "**從梯度角度**：\n",
    "$$\\frac{\\partial H}{\\partial x} = \\frac{\\partial F}{\\partial x} + I$$\n",
    "\n",
    "這個 $+I$（identity）保證了：\n",
    "- 即使 $\\frac{\\partial F}{\\partial x}$ 很小，梯度也至少為 1\n",
    "- 提供了一條「高速公路」讓梯度直接流回去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較有無 residual connection 的梯度流\n",
    "\n",
    "def simulate_gradient_with_residual(n_layers, f_jacobian_norm):\n",
    "    \"\"\"模擬有 residual connection 的梯度流\n",
    "    \n",
    "    對於 y = F(x) + x:\n",
    "    dy/dx = dF/dx + I\n",
    "    \n",
    "    這裡簡化為純量：dy/dx = f_jacobian_norm + 1\n",
    "    \"\"\"\n",
    "    # 總 Jacobian = dF/dx + 1\n",
    "    total_jacobian = f_jacobian_norm + 1\n",
    "    \n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        gradient *= total_jacobian\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "n_layers = 50\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 左圖：沒有 residual\n",
    "plt.subplot(1, 2, 1)\n",
    "for f_norm in [0.3, 0.5, 0.7]:\n",
    "    grads_no_res = simulate_gradient_flow(n_layers, f_norm)\n",
    "    plt.plot(grads_no_res, label=f'Without residual (||∂F/∂x|| = {f_norm})')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Gradient magnitude')\n",
    "plt.title('Without Residual Connection')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylim([1e-20, 1e5])\n",
    "\n",
    "# 右圖：有 residual\n",
    "plt.subplot(1, 2, 2)\n",
    "for f_norm in [-0.3, 0.0, 0.3]:\n",
    "    grads_with_res = simulate_gradient_with_residual(n_layers, f_norm)\n",
    "    plt.plot(grads_with_res, label=f'With residual (||∂F/∂x|| = {f_norm})')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Gradient magnitude')\n",
    "plt.title('With Residual Connection')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylim([1e-20, 1e5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：\")\n",
    "print(\"- 沒有 residual：梯度消失很快\")\n",
    "print(\"- 有 residual：即使 F 的梯度小，總梯度仍然穩定\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：先準備基礎組件\n",
    "\n",
    "ResNet Block 需要用到：Conv2D, BatchNorm2D, ReLU\n",
    "\n",
    "這裡重新實作這些組件（從之前的 notebooks 簡化而來）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具函數：im2col 和 col2im\n",
    "\n",
    "def im2col(x, kH, kW, stride=1, pad=0):\n",
    "    \"\"\"將 4D tensor 展開成 2D matrix 以便做矩陣乘法\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape (N, C, H, W)\n",
    "    kH, kW : int - kernel size\n",
    "    stride : int\n",
    "    pad : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : np.ndarray, shape (N*out_H*out_W, C*kH*kW)\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    # Padding\n",
    "    if pad > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 使用 stride_tricks 來高效提取 patches\n",
    "    shape = (N, C, kH, kW, out_H, out_W)\n",
    "    strides = (x.strides[0], x.strides[1], x.strides[2], x.strides[3],\n",
    "               x.strides[2] * stride, x.strides[3] * stride)\n",
    "    \n",
    "    col = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_H * out_W, -1)\n",
    "    \n",
    "    return col\n",
    "\n",
    "def col2im(col, x_shape, kH, kW, stride=1, pad=0):\n",
    "    \"\"\"im2col 的逆操作\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : np.ndarray, shape (N*out_H*out_W, C*kH*kW)\n",
    "    x_shape : tuple - 原始 x 的 shape (N, C, H, W)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray, shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_pad = H + 2 * pad\n",
    "    W_pad = W + 2 * pad\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    col = col.reshape(N, out_H, out_W, C, kH, kW).transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    x_pad = np.zeros((N, C, H_pad, W_pad))\n",
    "    \n",
    "    for i in range(kH):\n",
    "        i_max = i + stride * out_H\n",
    "        for j in range(kW):\n",
    "            j_max = j + stride * out_W\n",
    "            x_pad[:, :, i:i_max:stride, j:j_max:stride] += col[:, :, i, j, :, :]\n",
    "    \n",
    "    if pad > 0:\n",
    "        return x_pad[:, :, pad:-pad, pad:-pad]\n",
    "    return x_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"2D Convolution Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # He initialization\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass using im2col\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C_in, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        # 計算輸出大小\n",
    "        H_out = (H + 2 * self.padding - kH) // self.stride + 1\n",
    "        W_out = (W + 2 * self.padding - kW) // self.stride + 1\n",
    "        \n",
    "        # im2col\n",
    "        col = im2col(x, kH, kW, self.stride, self.padding)\n",
    "        \n",
    "        # Reshape weights\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        \n",
    "        # Matrix multiplication\n",
    "        out = col @ W_col.T + self.b\n",
    "        \n",
    "        # Reshape output\n",
    "        out = out.reshape(N, H_out, W_out, self.out_channels).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.cache = (x, col)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray, shape (N, C_in, H, W)\n",
    "        \"\"\"\n",
    "        x, col = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        # Reshape dout: (N, C_out, H_out, W_out) -> (N*H_out*W_out, C_out)\n",
    "        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, self.out_channels)\n",
    "        \n",
    "        # Gradient for W and b\n",
    "        self.dW = (dout_reshaped.T @ col).reshape(self.W.shape)\n",
    "        self.db = dout_reshaped.sum(axis=0)\n",
    "        \n",
    "        # Gradient for x\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        dcol = dout_reshaped @ W_col\n",
    "        dx = col2im(dcol, x.shape, kH, kW, self.stride, self.padding)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2D:\n",
    "    \"\"\"Batch Normalization for Conv layers\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        self.cache = None\n",
    "        self.training = True\n",
    "        \n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        if self.training:\n",
    "            # 計算 mean 和 var（對 N, H, W 取平均，保留 C）\n",
    "            mean = x.mean(axis=(0, 2, 3))\n",
    "            var = x.var(axis=(0, 2, 3))\n",
    "            \n",
    "            # 更新 running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = (x - mean.reshape(1, C, 1, 1)) / std.reshape(1, C, 1, 1)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma.reshape(1, C, 1, 1) * x_norm + self.beta.reshape(1, C, 1, 1)\n",
    "        \n",
    "        if self.training:\n",
    "            self.cache = (x, x_norm, mean, var, std)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        x, x_norm, mean, var, std = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        m = N * H * W  # number of elements per channel\n",
    "        \n",
    "        # Gradients for gamma and beta\n",
    "        self.dgamma = (dout * x_norm).sum(axis=(0, 2, 3))\n",
    "        self.dbeta = dout.sum(axis=(0, 2, 3))\n",
    "        \n",
    "        # Gradient for x_norm\n",
    "        dx_norm = dout * self.gamma.reshape(1, C, 1, 1)\n",
    "        \n",
    "        # Gradient for x\n",
    "        dvar = (-0.5 * dx_norm * (x - mean.reshape(1, C, 1, 1)) / \n",
    "                (var.reshape(1, C, 1, 1) + self.eps) ** 1.5).sum(axis=(0, 2, 3))\n",
    "        \n",
    "        dmean = (-dx_norm / std.reshape(1, C, 1, 1)).sum(axis=(0, 2, 3))\n",
    "        dmean += dvar * (-2 / m) * (x - mean.reshape(1, C, 1, 1)).sum(axis=(0, 2, 3))\n",
    "        \n",
    "        dx = dx_norm / std.reshape(1, C, 1, 1)\n",
    "        dx += (2 / m) * dvar.reshape(1, C, 1, 1) * (x - mean.reshape(1, C, 1, 1))\n",
    "        dx += dmean.reshape(1, C, 1, 1) / m\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU activation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache\n",
    "        return dout * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：Basic Block 實作\n",
    "\n",
    "### 4.1 Basic Block 結構\n",
    "\n",
    "```\n",
    "              ┌────────────────┐\n",
    "              │   Identity     │  (or 1x1 conv if dimension mismatch)\n",
    "              │                │\n",
    "x ────────────┼────────────────┼────────────┐\n",
    "              │                │            │\n",
    "              ↓                │            │\n",
    "        ┌──────────┐          │            │\n",
    "        │ Conv 3x3 │          │            │\n",
    "        │   BN     │          │            │\n",
    "        │  ReLU    │          │            │\n",
    "        └────┬─────┘          │            │\n",
    "             │                │            │\n",
    "             ↓                │            │\n",
    "        ┌──────────┐          │            │\n",
    "        │ Conv 3x3 │          │            │\n",
    "        │   BN     │          │            │\n",
    "        └────┬─────┘          │            │\n",
    "             │                │            │\n",
    "             ↓                │            │\n",
    "           F(x)               +            x (or shortcut(x))\n",
    "             │                │            │\n",
    "             └────────────────┼────────────┘\n",
    "                              │\n",
    "                              ↓\n",
    "                            ReLU\n",
    "                              │\n",
    "                              ↓\n",
    "                          F(x) + x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock:\n",
    "    \"\"\"\n",
    "    ResNet Basic Block:\n",
    "    x -> Conv3x3 -> BN -> ReLU -> Conv3x3 -> BN -> (+x) -> ReLU\n",
    "    \n",
    "    如果 stride > 1 或 in_channels != out_channels，\n",
    "    shortcut 需要用 1x1 conv 來調整維度。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            輸入 channels 數\n",
    "        out_channels : int\n",
    "            輸出 channels 數\n",
    "        stride : int\n",
    "            第一個 conv 的 stride（用於 downsample）\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        # 主路徑\n",
    "        # 第一個 conv：可能有 stride（用於 downsample）\n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=3, \n",
    "                           stride=stride, padding=1)\n",
    "        self.bn1 = BatchNorm2D(out_channels)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        # 第二個 conv：stride 固定為 1\n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3,\n",
    "                           stride=1, padding=1)\n",
    "        self.bn2 = BatchNorm2D(out_channels)\n",
    "        \n",
    "        # 最後的 ReLU（在 add 之後）\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        # Shortcut：如果維度不匹配，需要 1x1 conv\n",
    "        self.use_shortcut_conv = (stride != 1) or (in_channels != out_channels)\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_conv = Conv2D(in_channels, out_channels, kernel_size=1,\n",
    "                                        stride=stride, padding=0)\n",
    "            self.shortcut_bn = BatchNorm2D(out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C_in, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \"\"\"\n",
    "        # 保存輸入\n",
    "        identity = x\n",
    "        \n",
    "        # 主路徑\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        \n",
    "        # Shortcut path\n",
    "        if self.use_shortcut_conv:\n",
    "            identity = self.shortcut_conv.forward(x)\n",
    "            identity = self.shortcut_bn.forward(identity)\n",
    "        \n",
    "        # Residual addition\n",
    "        out = out + identity\n",
    "        \n",
    "        # Final ReLU\n",
    "        out = self.relu2.forward(out)\n",
    "        \n",
    "        self.cache = (x,)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \n",
    "        關鍵：在 residual addition 處，梯度分流到兩條路徑\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray, shape (N, C_in, H, W)\n",
    "        \"\"\"\n",
    "        x, = self.cache\n",
    "        \n",
    "        # Backward through final ReLU\n",
    "        dout = self.relu2.backward(dout)\n",
    "        \n",
    "        # 在 addition 處分流\n",
    "        # d(out + identity) = dout for both branches\n",
    "        d_main = dout  # 主路徑的梯度\n",
    "        d_shortcut = dout.copy()  # shortcut 的梯度\n",
    "        \n",
    "        # 主路徑 backward\n",
    "        d_main = self.bn2.backward(d_main)\n",
    "        d_main = self.conv2.backward(d_main)\n",
    "        \n",
    "        d_main = self.relu1.backward(d_main)\n",
    "        d_main = self.bn1.backward(d_main)\n",
    "        d_main = self.conv1.backward(d_main)\n",
    "        \n",
    "        # Shortcut backward\n",
    "        if self.use_shortcut_conv:\n",
    "            d_shortcut = self.shortcut_bn.backward(d_shortcut)\n",
    "            d_shortcut = self.shortcut_conv.backward(d_shortcut)\n",
    "        \n",
    "        # 合併兩條路徑的梯度\n",
    "        dx = d_main + d_shortcut\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Set to training mode\"\"\"\n",
    "        self.bn1.training = True\n",
    "        self.bn2.training = True\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_bn.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        self.bn1.training = False\n",
    "        self.bn2.training = False\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_bn.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 BasicBlock forward\n",
    "\n",
    "# Case 1: 維度不變\n",
    "block1 = BasicBlock(in_channels=16, out_channels=16, stride=1)\n",
    "x1 = np.random.randn(2, 16, 8, 8)\n",
    "out1 = block1.forward(x1)\n",
    "print(f\"Case 1 (same dim): input {x1.shape} -> output {out1.shape}\")\n",
    "\n",
    "# Case 2: Downsample（stride=2）\n",
    "block2 = BasicBlock(in_channels=16, out_channels=32, stride=2)\n",
    "x2 = np.random.randn(2, 16, 8, 8)\n",
    "out2 = block2.forward(x2)\n",
    "print(f\"Case 2 (downsample): input {x2.shape} -> output {out2.shape}\")\n",
    "\n",
    "# Case 3: 只改 channels\n",
    "block3 = BasicBlock(in_channels=16, out_channels=32, stride=1)\n",
    "x3 = np.random.randn(2, 16, 8, 8)\n",
    "out3 = block3.forward(x3)\n",
    "print(f\"Case 3 (channel change): input {x3.shape} -> output {out3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Check for BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"計算數值梯度\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + eps\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = old_val - eps\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * eps)\n",
    "        x[idx] = old_val\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def gradient_check(analytic_grad, numeric_grad, name=\"\"):\n",
    "    \"\"\"比較解析梯度和數值梯度\"\"\"\n",
    "    diff = np.abs(analytic_grad - numeric_grad)\n",
    "    max_diff = np.max(diff)\n",
    "    rel_error = max_diff / (np.maximum(np.abs(analytic_grad).max(), np.abs(numeric_grad).max()) + 1e-8)\n",
    "    \n",
    "    status = \"✓ PASS\" if rel_error < 1e-4 else \"✗ FAIL\"\n",
    "    print(f\"{name}: max_diff = {max_diff:.2e}, rel_error = {rel_error:.2e} {status}\")\n",
    "    return rel_error < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check for BasicBlock\n",
    "print(\"Gradient Check for BasicBlock\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 使用小型輸入避免太慢\n",
    "np.random.seed(42)\n",
    "block = BasicBlock(in_channels=4, out_channels=8, stride=2)\n",
    "x = np.random.randn(2, 4, 6, 6)\n",
    "\n",
    "# Forward and backward\n",
    "out = block.forward(x)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx_analytic = block.backward(dout)\n",
    "\n",
    "# Numerical gradient for dx\n",
    "def f_dx(x_):\n",
    "    block_copy = BasicBlock(in_channels=4, out_channels=8, stride=2)\n",
    "    # Copy weights\n",
    "    block_copy.conv1.W = block.conv1.W.copy()\n",
    "    block_copy.conv1.b = block.conv1.b.copy()\n",
    "    block_copy.conv2.W = block.conv2.W.copy()\n",
    "    block_copy.conv2.b = block.conv2.b.copy()\n",
    "    block_copy.bn1.gamma = block.bn1.gamma.copy()\n",
    "    block_copy.bn1.beta = block.bn1.beta.copy()\n",
    "    block_copy.bn2.gamma = block.bn2.gamma.copy()\n",
    "    block_copy.bn2.beta = block.bn2.beta.copy()\n",
    "    if block.use_shortcut_conv:\n",
    "        block_copy.shortcut_conv.W = block.shortcut_conv.W.copy()\n",
    "        block_copy.shortcut_conv.b = block.shortcut_conv.b.copy()\n",
    "        block_copy.shortcut_bn.gamma = block.shortcut_bn.gamma.copy()\n",
    "        block_copy.shortcut_bn.beta = block.shortcut_bn.beta.copy()\n",
    "    out_ = block_copy.forward(x_)\n",
    "    return np.sum(out_ * dout)\n",
    "\n",
    "dx_numeric = numerical_gradient(f_dx, x.copy())\n",
    "gradient_check(dx_analytic, dx_numeric, \"dx\")\n",
    "\n",
    "print(\"\\nBasicBlock gradient check completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：Bottleneck Block（選做）\n",
    "\n",
    "### 5.1 Bottleneck 結構\n",
    "\n",
    "對於更深的 ResNet（ResNet-50 及以上），使用 Bottleneck Block 來減少計算量：\n",
    "\n",
    "```\n",
    "x ────────────┬────────────────────────────────┐\n",
    "              │                                │\n",
    "              ↓                                │\n",
    "        ┌──────────┐                          │\n",
    "        │ Conv 1x1 │  減少 channels           │\n",
    "        │   BN     │  (e.g., 256 -> 64)       │\n",
    "        │  ReLU    │                          │\n",
    "        └────┬─────┘                          │\n",
    "             │                                │\n",
    "             ↓                                │\n",
    "        ┌──────────┐                          │\n",
    "        │ Conv 3x3 │  主要計算                │ shortcut\n",
    "        │   BN     │  (64 channels)           │\n",
    "        │  ReLU    │                          │\n",
    "        └────┬─────┘                          │\n",
    "             │                                │\n",
    "             ↓                                │\n",
    "        ┌──────────┐                          │\n",
    "        │ Conv 1x1 │  恢復 channels           │\n",
    "        │   BN     │  (64 -> 256)             │\n",
    "        └────┬─────┘                          │\n",
    "             │                                │\n",
    "             +────────────────────────────────┘\n",
    "             │\n",
    "           ReLU\n",
    "             │\n",
    "             ↓\n",
    "```\n",
    "\n",
    "**為什麼這樣設計？**\n",
    "- 3x3 conv 的計算量正比於 $C^2$（input channels × output channels）\n",
    "- 先用 1x1 conv 把 channels 降到 1/4，再做 3x3，再恢復\n",
    "- 計算量：$(1/4)^2 = 1/16$ 的 3x3 conv 計算量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock:\n",
    "    \"\"\"\n",
    "    ResNet Bottleneck Block:\n",
    "    x -> Conv1x1 -> BN -> ReLU -> Conv3x3 -> BN -> ReLU -> Conv1x1 -> BN -> (+x) -> ReLU\n",
    "    \n",
    "    expansion = 4: 輸出 channels = 4 * mid_channels\n",
    "    \"\"\"\n",
    "    \n",
    "    expansion = 4  # 輸出 channels 是中間 channels 的 4 倍\n",
    "    \n",
    "    def __init__(self, in_channels, mid_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            輸入 channels\n",
    "        mid_channels : int\n",
    "            中間層（3x3 conv）的 channels\n",
    "        stride : int\n",
    "            3x3 conv 的 stride（用於 downsample）\n",
    "        \"\"\"\n",
    "        self.in_channels = in_channels\n",
    "        self.mid_channels = mid_channels\n",
    "        self.out_channels = mid_channels * self.expansion\n",
    "        self.stride = stride\n",
    "        \n",
    "        # 1x1 conv: 降維\n",
    "        self.conv1 = Conv2D(in_channels, mid_channels, kernel_size=1,\n",
    "                           stride=1, padding=0)\n",
    "        self.bn1 = BatchNorm2D(mid_channels)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        # 3x3 conv: 主要計算（可能有 stride）\n",
    "        self.conv2 = Conv2D(mid_channels, mid_channels, kernel_size=3,\n",
    "                           stride=stride, padding=1)\n",
    "        self.bn2 = BatchNorm2D(mid_channels)\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        # 1x1 conv: 升維\n",
    "        self.conv3 = Conv2D(mid_channels, self.out_channels, kernel_size=1,\n",
    "                           stride=1, padding=0)\n",
    "        self.bn3 = BatchNorm2D(self.out_channels)\n",
    "        \n",
    "        # Final ReLU\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        # Shortcut\n",
    "        self.use_shortcut_conv = (stride != 1) or (in_channels != self.out_channels)\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_conv = Conv2D(in_channels, self.out_channels, kernel_size=1,\n",
    "                                        stride=stride, padding=0)\n",
    "            self.shortcut_bn = BatchNorm2D(self.out_channels)\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        # 1x1 conv (reduce)\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        # 3x3 conv\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        out = self.relu2.forward(out)\n",
    "        \n",
    "        # 1x1 conv (expand)\n",
    "        out = self.conv3.forward(out)\n",
    "        out = self.bn3.forward(out)\n",
    "        \n",
    "        # Shortcut\n",
    "        if self.use_shortcut_conv:\n",
    "            identity = self.shortcut_conv.forward(x)\n",
    "            identity = self.shortcut_bn.forward(identity)\n",
    "        \n",
    "        # Residual addition\n",
    "        out = out + identity\n",
    "        \n",
    "        # Final ReLU\n",
    "        out = self.relu3.forward(out)\n",
    "        \n",
    "        self.cache = (x,)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        x, = self.cache\n",
    "        \n",
    "        # Final ReLU backward\n",
    "        dout = self.relu3.backward(dout)\n",
    "        \n",
    "        # Split gradient\n",
    "        d_main = dout\n",
    "        d_shortcut = dout.copy()\n",
    "        \n",
    "        # Main path backward\n",
    "        d_main = self.bn3.backward(d_main)\n",
    "        d_main = self.conv3.backward(d_main)\n",
    "        \n",
    "        d_main = self.relu2.backward(d_main)\n",
    "        d_main = self.bn2.backward(d_main)\n",
    "        d_main = self.conv2.backward(d_main)\n",
    "        \n",
    "        d_main = self.relu1.backward(d_main)\n",
    "        d_main = self.bn1.backward(d_main)\n",
    "        d_main = self.conv1.backward(d_main)\n",
    "        \n",
    "        # Shortcut backward\n",
    "        if self.use_shortcut_conv:\n",
    "            d_shortcut = self.shortcut_bn.backward(d_shortcut)\n",
    "            d_shortcut = self.shortcut_conv.backward(d_shortcut)\n",
    "        \n",
    "        # Merge gradients\n",
    "        dx = d_main + d_shortcut\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Set to training mode\"\"\"\n",
    "        for bn in [self.bn1, self.bn2, self.bn3]:\n",
    "            bn.training = True\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_bn.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        for bn in [self.bn1, self.bn2, self.bn3]:\n",
    "            bn.training = False\n",
    "        if self.use_shortcut_conv:\n",
    "            self.shortcut_bn.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 BottleneckBlock\n",
    "\n",
    "# 標準配置：in=64 -> mid=64 -> out=256\n",
    "bottleneck1 = BottleneckBlock(in_channels=64, mid_channels=64, stride=1)\n",
    "x1 = np.random.randn(2, 64, 8, 8)\n",
    "out1 = bottleneck1.forward(x1)\n",
    "print(f\"Bottleneck (standard): input {x1.shape} -> output {out1.shape}\")\n",
    "print(f\"  Expected output channels: 64 * 4 = 256\")\n",
    "\n",
    "# Downsample: in=256 -> mid=128 -> out=512\n",
    "bottleneck2 = BottleneckBlock(in_channels=256, mid_channels=128, stride=2)\n",
    "x2 = np.random.randn(2, 256, 8, 8)\n",
    "out2 = bottleneck2.forward(x2)\n",
    "print(f\"\\nBottleneck (downsample): input {x2.shape} -> output {out2.shape}\")\n",
    "print(f\"  Expected output channels: 128 * 4 = 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算量比較\n",
    "\n",
    "def count_conv_flops(in_c, out_c, k, h, w):\n",
    "    \"\"\"計算卷積的浮點運算數（乘加次數）\"\"\"\n",
    "    # 每個輸出點需要 in_c * k * k 次乘加\n",
    "    return out_c * h * w * (in_c * k * k)\n",
    "\n",
    "# 假設輸入 256 channels, 56x56 feature map\n",
    "h, w = 56, 56\n",
    "in_c = 256\n",
    "\n",
    "# Basic Block: 兩個 3x3 conv\n",
    "basic_flops = count_conv_flops(in_c, in_c, 3, h, w) * 2\n",
    "\n",
    "# Bottleneck: 1x1 (256->64) + 3x3 (64->64) + 1x1 (64->256)\n",
    "mid_c = 64\n",
    "bottleneck_flops = (count_conv_flops(in_c, mid_c, 1, h, w) +  # 1x1 reduce\n",
    "                   count_conv_flops(mid_c, mid_c, 3, h, w) +   # 3x3\n",
    "                   count_conv_flops(mid_c, in_c, 1, h, w))     # 1x1 expand\n",
    "\n",
    "print(f\"Input: {in_c} channels, {h}x{w} feature map\")\n",
    "print(f\"\\nBasic Block FLOPs:     {basic_flops:,}\")\n",
    "print(f\"Bottleneck Block FLOPs: {bottleneck_flops:,}\")\n",
    "print(f\"\\nSpeedup ratio: {basic_flops / bottleneck_flops:.2f}x\")\n",
    "print(f\"\\n注意：Bottleneck 的輸出 channels 是 {mid_c * 4} = {mid_c}*4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第六部分：組裝 ResNet-style 網路\n",
    "\n",
    "現在我們用 BasicBlock 組裝一個小型 ResNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2D:\n",
    "    \"\"\"Global Average Pooling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"(N, C, H, W) -> (N, C)\"\"\"\n",
    "        self.cache = x.shape\n",
    "        return x.mean(axis=(2, 3))\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"(N, C) -> (N, C, H, W)\"\"\"\n",
    "        N, C, H, W = self.cache\n",
    "        dx = dout.reshape(N, C, 1, 1) / (H * W)\n",
    "        dx = np.broadcast_to(dx, (N, C, H, W)).copy()\n",
    "        return dx\n",
    "\n",
    "\n",
    "class FullyConnected:\n",
    "    \"\"\"Fully Connected Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        scale = np.sqrt(2.0 / in_features)\n",
    "        self.W = np.random.randn(in_features, out_features) * scale\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.cache = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache = x\n",
    "        return x @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x = self.cache\n",
    "        self.dW = x.T @ dout\n",
    "        self.db = dout.sum(axis=0)\n",
    "        return dout @ self.W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyResNet:\n",
    "    \"\"\"\n",
    "    一個小型 ResNet-style 網路\n",
    "    \n",
    "    結構：\n",
    "    Conv3x3 -> BN -> ReLU -> \n",
    "    BasicBlock (16) x 2 ->\n",
    "    BasicBlock (32, stride=2) -> BasicBlock (32) ->\n",
    "    GlobalAvgPool -> FC -> Softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 初始卷積\n",
    "        self.conv1 = Conv2D(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = BatchNorm2D(16)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        # Layer 1: 2 x BasicBlock (16 channels)\n",
    "        self.layer1_block1 = BasicBlock(16, 16, stride=1)\n",
    "        self.layer1_block2 = BasicBlock(16, 16, stride=1)\n",
    "        \n",
    "        # Layer 2: 2 x BasicBlock (32 channels), first one downsamples\n",
    "        self.layer2_block1 = BasicBlock(16, 32, stride=2)\n",
    "        self.layer2_block2 = BasicBlock(32, 32, stride=1)\n",
    "        \n",
    "        # Global Average Pooling + FC\n",
    "        self.gap = GlobalAvgPool2D()\n",
    "        self.fc = FullyConnected(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, 1, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : np.ndarray, shape (N, num_classes)\n",
    "        \"\"\"\n",
    "        # Initial conv\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        # Layer 1\n",
    "        out = self.layer1_block1.forward(out)\n",
    "        out = self.layer1_block2.forward(out)\n",
    "        \n",
    "        # Layer 2\n",
    "        out = self.layer2_block1.forward(out)\n",
    "        out = self.layer2_block2.forward(out)\n",
    "        \n",
    "        # GAP + FC\n",
    "        out = self.gap.forward(out)\n",
    "        logits = self.fc.forward(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        # FC + GAP\n",
    "        dout = self.fc.backward(dlogits)\n",
    "        dout = self.gap.backward(dout)\n",
    "        \n",
    "        # Layer 2\n",
    "        dout = self.layer2_block2.backward(dout)\n",
    "        dout = self.layer2_block1.backward(dout)\n",
    "        \n",
    "        # Layer 1\n",
    "        dout = self.layer1_block2.backward(dout)\n",
    "        dout = self.layer1_block1.backward(dout)\n",
    "        \n",
    "        # Initial conv\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"獲取所有參數和梯度\"\"\"\n",
    "        params = []\n",
    "        grads = []\n",
    "        \n",
    "        # Conv1\n",
    "        params.extend([self.conv1.W, self.conv1.b, self.bn1.gamma, self.bn1.beta])\n",
    "        grads.extend([self.conv1.dW, self.conv1.db, self.bn1.dgamma, self.bn1.dbeta])\n",
    "        \n",
    "        # Layer 1\n",
    "        for block in [self.layer1_block1, self.layer1_block2]:\n",
    "            params.extend([block.conv1.W, block.conv1.b, block.bn1.gamma, block.bn1.beta,\n",
    "                          block.conv2.W, block.conv2.b, block.bn2.gamma, block.bn2.beta])\n",
    "            grads.extend([block.conv1.dW, block.conv1.db, block.bn1.dgamma, block.bn1.dbeta,\n",
    "                         block.conv2.dW, block.conv2.db, block.bn2.dgamma, block.bn2.dbeta])\n",
    "        \n",
    "        # Layer 2\n",
    "        for block in [self.layer2_block1, self.layer2_block2]:\n",
    "            params.extend([block.conv1.W, block.conv1.b, block.bn1.gamma, block.bn1.beta,\n",
    "                          block.conv2.W, block.conv2.b, block.bn2.gamma, block.bn2.beta])\n",
    "            grads.extend([block.conv1.dW, block.conv1.db, block.bn1.dgamma, block.bn1.dbeta,\n",
    "                         block.conv2.dW, block.conv2.db, block.bn2.dgamma, block.bn2.dbeta])\n",
    "            if block.use_shortcut_conv:\n",
    "                params.extend([block.shortcut_conv.W, block.shortcut_conv.b,\n",
    "                              block.shortcut_bn.gamma, block.shortcut_bn.beta])\n",
    "                grads.extend([block.shortcut_conv.dW, block.shortcut_conv.db,\n",
    "                             block.shortcut_bn.dgamma, block.shortcut_bn.dbeta])\n",
    "        \n",
    "        # FC\n",
    "        params.extend([self.fc.W, self.fc.b])\n",
    "        grads.extend([self.fc.dW, self.fc.db])\n",
    "        \n",
    "        return params, grads\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Set to training mode\"\"\"\n",
    "        self.bn1.training = True\n",
    "        for block in [self.layer1_block1, self.layer1_block2,\n",
    "                      self.layer2_block1, self.layer2_block2]:\n",
    "            block.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        \"\"\"Set to evaluation mode\"\"\"\n",
    "        self.bn1.training = False\n",
    "        for block in [self.layer1_block1, self.layer1_block2,\n",
    "                      self.layer2_block1, self.layer2_block2]:\n",
    "            block.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 TinyResNet\n",
    "\n",
    "model = TinyResNet(num_classes=10)\n",
    "x = np.random.randn(4, 1, 32, 32)  # 類似 MNIST 的輸入（但是 32x32）\n",
    "\n",
    "# Forward\n",
    "logits = model.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "\n",
    "# 統計參數數量\n",
    "params, _ = model.get_params_and_grads()\n",
    "total_params = sum(p.size for p in params)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第七部分：訓練比較 - 有無 Residual Connection\n",
    "\n",
    "為了展示 residual connection 的效果，我們比較有無 residual 的網路訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個沒有 residual connection 的對照網路\n",
    "\n",
    "class PlainBlock:\n",
    "    \"\"\"沒有 residual connection 的 block\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.conv1 = Conv2D(in_channels, out_channels, kernel_size=3,\n",
    "                           stride=stride, padding=1)\n",
    "        self.bn1 = BatchNorm2D(out_channels)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        self.conv2 = Conv2D(out_channels, out_channels, kernel_size=3,\n",
    "                           stride=1, padding=1)\n",
    "        self.bn2 = BatchNorm2D(out_channels)\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out)\n",
    "        out = self.relu2.forward(out)  # 沒有 + x\n",
    "        \n",
    "        self.cache = (x,)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        \n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def train(self):\n",
    "        self.bn1.training = True\n",
    "        self.bn2.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.bn1.training = False\n",
    "        self.bn2.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyPlainNet:\n",
    "    \"\"\"沒有 residual connection 的網路\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = Conv2D(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = BatchNorm2D(16)\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        self.layer1_block1 = PlainBlock(16, 16, stride=1)\n",
    "        self.layer1_block2 = PlainBlock(16, 16, stride=1)\n",
    "        \n",
    "        # 需要額外處理 channel 變化\n",
    "        self.transition = Conv2D(16, 32, kernel_size=1, stride=2, padding=0)\n",
    "        self.transition_bn = BatchNorm2D(32)\n",
    "        self.transition_relu = ReLU()\n",
    "        \n",
    "        self.layer2_block1 = PlainBlock(32, 32, stride=1)\n",
    "        self.layer2_block2 = PlainBlock(32, 32, stride=1)\n",
    "        \n",
    "        self.gap = GlobalAvgPool2D()\n",
    "        self.fc = FullyConnected(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out)\n",
    "        out = self.relu1.forward(out)\n",
    "        \n",
    "        out = self.layer1_block1.forward(out)\n",
    "        out = self.layer1_block2.forward(out)\n",
    "        \n",
    "        out = self.transition.forward(out)\n",
    "        out = self.transition_bn.forward(out)\n",
    "        out = self.transition_relu.forward(out)\n",
    "        \n",
    "        out = self.layer2_block1.forward(out)\n",
    "        out = self.layer2_block2.forward(out)\n",
    "        \n",
    "        out = self.gap.forward(out)\n",
    "        logits = self.fc.forward(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        dout = self.fc.backward(dlogits)\n",
    "        dout = self.gap.backward(dout)\n",
    "        \n",
    "        dout = self.layer2_block2.backward(dout)\n",
    "        dout = self.layer2_block1.backward(dout)\n",
    "        \n",
    "        dout = self.transition_relu.backward(dout)\n",
    "        dout = self.transition_bn.backward(dout)\n",
    "        dout = self.transition.backward(dout)\n",
    "        \n",
    "        dout = self.layer1_block2.backward(dout)\n",
    "        dout = self.layer1_block1.backward(dout)\n",
    "        \n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        \n",
    "        return dout\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        params = []\n",
    "        grads = []\n",
    "        \n",
    "        params.extend([self.conv1.W, self.conv1.b, self.bn1.gamma, self.bn1.beta])\n",
    "        grads.extend([self.conv1.dW, self.conv1.db, self.bn1.dgamma, self.bn1.dbeta])\n",
    "        \n",
    "        for block in [self.layer1_block1, self.layer1_block2]:\n",
    "            params.extend([block.conv1.W, block.conv1.b, block.bn1.gamma, block.bn1.beta,\n",
    "                          block.conv2.W, block.conv2.b, block.bn2.gamma, block.bn2.beta])\n",
    "            grads.extend([block.conv1.dW, block.conv1.db, block.bn1.dgamma, block.bn1.dbeta,\n",
    "                         block.conv2.dW, block.conv2.db, block.bn2.dgamma, block.bn2.dbeta])\n",
    "        \n",
    "        params.extend([self.transition.W, self.transition.b,\n",
    "                      self.transition_bn.gamma, self.transition_bn.beta])\n",
    "        grads.extend([self.transition.dW, self.transition.db,\n",
    "                     self.transition_bn.dgamma, self.transition_bn.dbeta])\n",
    "        \n",
    "        for block in [self.layer2_block1, self.layer2_block2]:\n",
    "            params.extend([block.conv1.W, block.conv1.b, block.bn1.gamma, block.bn1.beta,\n",
    "                          block.conv2.W, block.conv2.b, block.bn2.gamma, block.bn2.beta])\n",
    "            grads.extend([block.conv1.dW, block.conv1.db, block.bn1.dgamma, block.bn1.dbeta,\n",
    "                         block.conv2.dW, block.conv2.db, block.bn2.dgamma, block.bn2.dbeta])\n",
    "        \n",
    "        params.extend([self.fc.W, self.fc.b])\n",
    "        grads.extend([self.fc.dW, self.fc.db])\n",
    "        \n",
    "        return params, grads\n",
    "    \n",
    "    def train(self):\n",
    "        self.bn1.training = True\n",
    "        self.transition_bn.training = True\n",
    "        for block in [self.layer1_block1, self.layer1_block2,\n",
    "                      self.layer2_block1, self.layer2_block2]:\n",
    "            block.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        self.bn1.training = False\n",
    "        self.transition_bn.training = False\n",
    "        for block in [self.layer1_block1, self.layer1_block2,\n",
    "                      self.layer2_block1, self.layer2_block2]:\n",
    "            block.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成合成數據集\n",
    "\n",
    "def generate_simple_shapes(n_samples, img_size=32):\n",
    "    \"\"\"生成簡單的形狀分類數據\n",
    "    \n",
    "    4 類：水平線、垂直線、對角線（左上到右下）、對角線（右上到左下）\n",
    "    \"\"\"\n",
    "    X = np.zeros((n_samples, 1, img_size, img_size))\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        label = np.random.randint(4)\n",
    "        y[i] = label\n",
    "        \n",
    "        # 隨機位置和一點噪音\n",
    "        noise = np.random.randn(img_size, img_size) * 0.1\n",
    "        \n",
    "        if label == 0:  # 水平線\n",
    "            row = np.random.randint(5, img_size - 5)\n",
    "            X[i, 0, row-1:row+2, 5:-5] = 1\n",
    "        elif label == 1:  # 垂直線\n",
    "            col = np.random.randint(5, img_size - 5)\n",
    "            X[i, 0, 5:-5, col-1:col+2] = 1\n",
    "        elif label == 2:  # 對角線 (\\\\)\n",
    "            for j in range(-1, 2):\n",
    "                np.fill_diagonal(X[i, 0, :, max(0, j):], 1)\n",
    "        else:  # 對角線 (/)\n",
    "            for j in range(-1, 2):\n",
    "                np.fill_diagonal(np.fliplr(X[i, 0])[:, max(0, j):], 1)\n",
    "        \n",
    "        X[i, 0] += noise\n",
    "    \n",
    "    return X.astype(np.float32), y\n",
    "\n",
    "# 生成數據\n",
    "np.random.seed(42)\n",
    "X_train, y_train = generate_simple_shapes(400, img_size=32)\n",
    "X_test, y_test = generate_simple_shapes(100, img_size=32)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# 顯示樣本\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "labels = ['Horizontal', 'Vertical', 'Diagonal \\\\', 'Diagonal /']\n",
    "for i in range(4):\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[0, i].imshow(X_train[idx, 0], cmap='gray')\n",
    "    axes[0, i].set_title(labels[i])\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    idx = np.where(y_train == i)[0][1]\n",
    "    axes[1, i].imshow(X_train[idx, 0], cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    x_max = np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, y):\n",
    "    \"\"\"Cross-entropy loss\"\"\"\n",
    "    N = logits.shape[0]\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.mean(np.log(probs[np.arange(N), y] + 1e-8))\n",
    "    \n",
    "    # Gradient\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[np.arange(N), y] -= 1\n",
    "    dlogits /= N\n",
    "    \n",
    "    return loss, dlogits\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    \"\"\"Compute accuracy\"\"\"\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return np.mean(preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, \n",
    "                epochs=30, batch_size=32, lr=0.01, momentum=0.9):\n",
    "    \"\"\"訓練模型\"\"\"\n",
    "    n_samples = X_train.shape[0]\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    # Momentum velocities\n",
    "    params, _ = model.get_params_and_grads()\n",
    "    velocities = [np.zeros_like(p) for p in params]\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward\n",
    "            logits = model.forward(X_batch)\n",
    "            loss, dlogits = cross_entropy_loss(logits, y_batch)\n",
    "            \n",
    "            # Backward\n",
    "            model.backward(dlogits)\n",
    "            \n",
    "            # Update with momentum SGD\n",
    "            params, grads = model.get_params_and_grads()\n",
    "            for j, (p, g) in enumerate(zip(params, grads)):\n",
    "                if g is not None:\n",
    "                    velocities[j] = momentum * velocities[j] - lr * g\n",
    "                    p += velocities[j]\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_correct += np.sum(np.argmax(logits, axis=1) == y_batch)\n",
    "        \n",
    "        # Epoch metrics\n",
    "        train_loss = epoch_loss / n_batches\n",
    "        train_acc = epoch_correct / (n_batches * batch_size)\n",
    "        \n",
    "        # Test accuracy\n",
    "        model.eval()\n",
    "        test_logits = model.forward(X_test)\n",
    "        test_acc = accuracy(test_logits, y_test)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: loss={train_loss:.4f}, \"\n",
    "                  f\"train_acc={train_acc:.4f}, test_acc={test_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練兩個網路\n",
    "\n",
    "print(\"Training ResNet (with residual connections):\")\n",
    "print(\"=\" * 50)\n",
    "np.random.seed(42)\n",
    "resnet = TinyResNet(num_classes=4)\n",
    "resnet_losses, resnet_train_accs, resnet_test_accs = train_model(\n",
    "    resnet, X_train, y_train, X_test, y_test, \n",
    "    epochs=30, batch_size=32, lr=0.01\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training PlainNet (without residual connections):\")\n",
    "print(\"=\" * 50)\n",
    "np.random.seed(42)\n",
    "plainnet = TinyPlainNet(num_classes=4)\n",
    "plain_losses, plain_train_accs, plain_test_accs = train_model(\n",
    "    plainnet, X_train, y_train, X_test, y_test,\n",
    "    epochs=30, batch_size=32, lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較結果\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(resnet_losses, 'b-', label='ResNet')\n",
    "axes[0].plot(plain_losses, 'r-', label='PlainNet')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Train accuracy\n",
    "axes[1].plot(resnet_train_accs, 'b-', label='ResNet')\n",
    "axes[1].plot(plain_train_accs, 'r-', label='PlainNet')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "axes[2].plot(resnet_test_accs, 'b-', label='ResNet')\n",
    "axes[2].plot(plain_test_accs, 'r-', label='PlainNet')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Test Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"ResNet   - Train Acc: {resnet_train_accs[-1]:.4f}, Test Acc: {resnet_test_accs[-1]:.4f}\")\n",
    "print(f\"PlainNet - Train Acc: {plain_train_accs[-1]:.4f}, Test Acc: {plain_test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第八部分：梯度流動分析\n",
    "\n",
    "觀察兩種網路的梯度在各層的分佈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model, X_batch, y_batch):\n",
    "    \"\"\"分析各層梯度的統計量\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Forward and backward\n",
    "    logits = model.forward(X_batch)\n",
    "    loss, dlogits = cross_entropy_loss(logits, y_batch)\n",
    "    model.backward(dlogits)\n",
    "    \n",
    "    # Collect gradient statistics\n",
    "    _, grads = model.get_params_and_grads()\n",
    "    \n",
    "    grad_norms = []\n",
    "    for g in grads:\n",
    "        if g is not None:\n",
    "            grad_norms.append(np.linalg.norm(g))\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# 比較梯度\n",
    "X_batch = X_train[:32]\n",
    "y_batch = y_train[:32]\n",
    "\n",
    "resnet_grads = analyze_gradients(resnet, X_batch, y_batch)\n",
    "plain_grads = analyze_gradients(plainnet, X_batch, y_batch)\n",
    "\n",
    "# 繪圖\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(len(resnet_grads)), resnet_grads, alpha=0.7)\n",
    "axes[0].set_xlabel('Parameter Index')\n",
    "axes[0].set_ylabel('Gradient Norm')\n",
    "axes[0].set_title('ResNet Gradient Norms')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].bar(range(len(plain_grads)), plain_grads, alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Parameter Index')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('PlainNet Gradient Norms')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ResNet gradient norm range: [{min(resnet_grads):.2e}, {max(resnet_grads):.2e}]\")\n",
    "print(f\"PlainNet gradient norm range: [{min(plain_grads):.2e}, {max(plain_grads):.2e}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### ResNet 的核心貢獻\n",
    "\n",
    "1. **Residual Connection**: $H(x) = F(x) + x$\n",
    "   - 提供「高速公路」讓梯度直接回傳\n",
    "   - 使得網路只需學習殘差 $F(x) = H(x) - x$\n",
    "\n",
    "2. **為什麼有效**：\n",
    "   - 梯度至少為 1（$\\frac{\\partial H}{\\partial x} = \\frac{\\partial F}{\\partial x} + I$）\n",
    "   - 如果最優解接近 identity，學習 $F \\approx 0$ 很容易\n",
    "   - 允許訓練非常深的網路（152 層甚至更深）\n",
    "\n",
    "3. **實作要點**：\n",
    "   - Basic Block: Conv3x3 → BN → ReLU → Conv3x3 → BN → (+x) → ReLU\n",
    "   - Bottleneck Block: 1x1 → 3x3 → 1x1 來減少計算量\n",
    "   - 維度不匹配時用 1x1 conv 調整 shortcut\n",
    "\n",
    "### 下一步\n",
    "\n",
    "- Module 6 Part 3: U-Net 架構（encoder-decoder + skip connections）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
