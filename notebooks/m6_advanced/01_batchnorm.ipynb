{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Batch Normalization\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解 Batch Normalization 解決什麼問題\n",
    "2. 實作 BatchNorm 的 forward（含 training/inference 模式）\n",
    "3. 推導並實作 BatchNorm 的 backward\n",
    "4. 使用梯度檢驗驗證實作\n",
    "\n",
    "## 為什麼需要 Batch Normalization？\n",
    "\n",
    "**Internal Covariate Shift 問題**：在訓練過程中，每一層的輸入分佈會隨著前面層參數的改變而改變。這使得後面的層需要不斷適應新的輸入分佈。\n",
    "\n",
    "**Batch Normalization** (Ioffe & Szegedy, 2015) 的核心思想：\n",
    "- 在每一層的激活前，將輸入正規化到均值 0、方差 1\n",
    "- 然後用可學習的參數 $\\gamma$（scale）和 $\\beta$（shift）來恢復網路的表達能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Batch Normalization module loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：BatchNorm 前向傳播\n",
    "\n",
    "### 公式\n",
    "\n",
    "給定一個 mini-batch $\\mathcal{B} = \\{x_1, ..., x_m\\}$：\n",
    "\n",
    "1. **計算 batch 統計量**：\n",
    "   $$\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$$\n",
    "   $$\\sigma^2_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$$\n",
    "\n",
    "2. **正規化**：\n",
    "   $$\\hat{x}_i = \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}}$$\n",
    "\n",
    "3. **Scale 和 Shift**：\n",
    "   $$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "其中 $\\gamma$ 和 $\\beta$ 是可學習的參數。\n",
    "\n",
    "### Training vs Inference\n",
    "\n",
    "- **Training**：使用當前 batch 的 $\\mu$ 和 $\\sigma^2$，同時更新 running statistics\n",
    "- **Inference**：使用 running mean 和 running variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1D:\n",
    "    \"\"\"\n",
    "    一維 Batch Normalization（用於全連接層後）\n",
    "    \n",
    "    輸入形狀：(N, D)\n",
    "    對每個特徵維度 D 分別正規化\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features : int\n",
    "            特徵維度 D\n",
    "        eps : float\n",
    "            數值穩定性常數\n",
    "        momentum : float\n",
    "            running statistics 的動量\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # 可學習參數\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        # Running statistics（用於 inference）\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        \n",
    "        # 快取\n",
    "        self.cache = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, D)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y : np.ndarray, shape (N, D)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # 計算 batch 統計量\n",
    "            batch_mean = np.mean(x, axis=0)  # (D,)\n",
    "            batch_var = np.var(x, axis=0)    # (D,)\n",
    "            \n",
    "            # 正規化\n",
    "            x_centered = x - batch_mean\n",
    "            std = np.sqrt(batch_var + self.eps)\n",
    "            x_norm = x_centered / std\n",
    "            \n",
    "            # Scale 和 Shift\n",
    "            y = self.gamma * x_norm + self.beta\n",
    "            \n",
    "            # 更新 running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "            \n",
    "            # 儲存快取\n",
    "            self.cache = (x, x_centered, std, x_norm)\n",
    "        else:\n",
    "            # 使用 running statistics\n",
    "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
    "            y = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, D)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray, shape (N, D)\n",
    "        \"\"\"\n",
    "        x, x_centered, std, x_norm = self.cache\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # 對 gamma 和 beta 的梯度\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        self.dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        # 對 x 的梯度（這是最複雜的部分）\n",
    "        # 推導見下方說明\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # 計算 d(1/std)\n",
    "        dvar = np.sum(dx_norm * x_centered, axis=0) * (-0.5) * (std ** -3)\n",
    "        \n",
    "        # 計算 d(mean)\n",
    "        dmean = np.sum(dx_norm * (-1 / std), axis=0) + dvar * np.mean(-2 * x_centered, axis=0)\n",
    "        \n",
    "        # 最終的 dx\n",
    "        dx = dx_norm / std + dvar * 2 * x_centered / N + dmean / N\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "# 測試\n",
    "bn = BatchNorm1D(num_features=5)\n",
    "x = np.random.randn(10, 5) * 3 + 2  # 非標準分佈\n",
    "\n",
    "print(\"輸入統計量:\")\n",
    "print(f\"  mean: {np.mean(x, axis=0)}\")\n",
    "print(f\"  std: {np.std(x, axis=0)}\")\n",
    "\n",
    "y = bn.forward(x)\n",
    "\n",
    "print(\"\\n輸出統計量（正規化後）:\")\n",
    "print(f\"  mean: {np.mean(y, axis=0)}\")\n",
    "print(f\"  std: {np.std(y, axis=0)}\")\n",
    "print(\"  （應該接近 mean=0, std=1）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：反向傳播推導\n",
    "\n",
    "這是 BatchNorm 中最複雜的部分。讓我們用計算圖來推導。\n",
    "\n",
    "### 計算圖\n",
    "\n",
    "```\n",
    "x → [mean] → μ\n",
    "         ↓\n",
    "x → [x - μ] → x_centered → [mean(x²)] → var\n",
    "                    ↓                      ↓\n",
    "                    ↓                [sqrt(var + ε)] → std\n",
    "                    ↓                      ↓\n",
    "                    └──────→ [x_centered / std] → x_norm\n",
    "                                                    ↓\n",
    "                                            [γ * x_norm + β] → y\n",
    "```\n",
    "\n",
    "### 梯度推導\n",
    "\n",
    "設 $m = N$（batch size），逐步反向傳播：\n",
    "\n",
    "1. **對 $\\gamma$ 和 $\\beta$**：\n",
    "   $$\\frac{\\partial L}{\\partial \\gamma} = \\sum_i \\frac{\\partial L}{\\partial y_i} \\hat{x}_i$$\n",
    "   $$\\frac{\\partial L}{\\partial \\beta} = \\sum_i \\frac{\\partial L}{\\partial y_i}$$\n",
    "\n",
    "2. **對 $\\hat{x}$**：\n",
    "   $$\\frac{\\partial L}{\\partial \\hat{x}_i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma$$\n",
    "\n",
    "3. **對 $x$**（最複雜）：\n",
    "   $\\hat{x} = \\frac{x - \\mu}{\\sigma}$ 其中 $\\mu$ 和 $\\sigma$ 都依賴於 $x$\n",
    "\n",
    "   最終結果：\n",
    "   $$\\frac{\\partial L}{\\partial x_i} = \\frac{1}{m\\sigma} \\left[ m \\frac{\\partial L}{\\partial \\hat{x}_i} - \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} - \\hat{x}_i \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} \\hat{x}_j \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度檢驗\n",
    "def gradient_check_bn1d(bn, x, eps=1e-5):\n",
    "    \"\"\"\n",
    "    對 BatchNorm1D 進行梯度檢驗\n",
    "    \"\"\"\n",
    "    bn.train()\n",
    "    \n",
    "    # 前向傳播\n",
    "    y = bn.forward(x)\n",
    "    \n",
    "    # 假設 loss = sum(y^2)\n",
    "    dout = 2 * y\n",
    "    \n",
    "    # 反向傳播\n",
    "    dx = bn.backward(dout)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # === 檢驗 dgamma ===\n",
    "    print(\"=== 檢驗 dgamma ===\")\n",
    "    dgamma_numerical = np.zeros_like(bn.gamma)\n",
    "    \n",
    "    for j in range(len(bn.gamma)):\n",
    "        old_val = bn.gamma[j]\n",
    "        \n",
    "        bn.gamma[j] = old_val + eps\n",
    "        y_plus = bn.forward(x)\n",
    "        loss_plus = np.sum(y_plus ** 2)\n",
    "        \n",
    "        bn.gamma[j] = old_val - eps\n",
    "        y_minus = bn.forward(x)\n",
    "        loss_minus = np.sum(y_minus ** 2)\n",
    "        \n",
    "        bn.gamma[j] = old_val\n",
    "        dgamma_numerical[j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(bn.dgamma - dgamma_numerical) / (np.abs(bn.dgamma) + np.abs(dgamma_numerical) + 1e-8))\n",
    "    print(f\"  最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"  通過: {rel_error < 1e-4}\")\n",
    "    if rel_error > 1e-4:\n",
    "        all_passed = False\n",
    "    \n",
    "    # === 檢驗 dbeta ===\n",
    "    print(\"\\n=== 檢驗 dbeta ===\")\n",
    "    dbeta_numerical = np.zeros_like(bn.beta)\n",
    "    \n",
    "    for j in range(len(bn.beta)):\n",
    "        old_val = bn.beta[j]\n",
    "        \n",
    "        bn.beta[j] = old_val + eps\n",
    "        y_plus = bn.forward(x)\n",
    "        loss_plus = np.sum(y_plus ** 2)\n",
    "        \n",
    "        bn.beta[j] = old_val - eps\n",
    "        y_minus = bn.forward(x)\n",
    "        loss_minus = np.sum(y_minus ** 2)\n",
    "        \n",
    "        bn.beta[j] = old_val\n",
    "        dbeta_numerical[j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(bn.dbeta - dbeta_numerical) / (np.abs(bn.dbeta) + np.abs(dbeta_numerical) + 1e-8))\n",
    "    print(f\"  最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"  通過: {rel_error < 1e-4}\")\n",
    "    if rel_error > 1e-4:\n",
    "        all_passed = False\n",
    "    \n",
    "    # === 檢驗 dx ===\n",
    "    print(\"\\n=== 檢驗 dx ===\")\n",
    "    dx_numerical = np.zeros_like(x)\n",
    "    x_test = x.copy()\n",
    "    \n",
    "    num_checks = min(10, x.size)\n",
    "    indices = np.random.choice(x.size, num_checks, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, x.shape)\n",
    "        old_val = x_test[multi_idx]\n",
    "        \n",
    "        x_test[multi_idx] = old_val + eps\n",
    "        y_plus = bn.forward(x_test)\n",
    "        loss_plus = np.sum(y_plus ** 2)\n",
    "        \n",
    "        x_test[multi_idx] = old_val - eps\n",
    "        y_minus = bn.forward(x_test)\n",
    "        loss_minus = np.sum(y_minus ** 2)\n",
    "        \n",
    "        x_test[multi_idx] = old_val\n",
    "        dx_numerical[multi_idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    # 比較抽查的位置\n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, x.shape)\n",
    "        ana = dx[multi_idx]\n",
    "        num = dx_numerical[multi_idx]\n",
    "        error = abs(ana - num) / (abs(ana) + abs(num) + 1e-8)\n",
    "        if error > 1e-4:\n",
    "            print(f\"  位置 {multi_idx}: 解析={ana:.6f}, 數值={num:.6f}, 誤差={error:.2e} ❌\")\n",
    "            all_passed = False\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"  抽查 {num_checks} 個位置全部通過 ✓\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# 執行梯度檢驗\n",
    "bn = BatchNorm1D(num_features=5)\n",
    "x = np.random.randn(10, 5)\n",
    "gradient_check_bn1d(bn, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：BatchNorm2D（用於卷積層）\n",
    "\n",
    "對於卷積層，輸入形狀是 $(N, C, H, W)$。BatchNorm2D 對每個通道 $C$ 分別計算統計量，跨 $N, H, W$ 維度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2D:\n",
    "    \"\"\"\n",
    "    二維 Batch Normalization（用於卷積層後）\n",
    "    \n",
    "    輸入形狀：(N, C, H, W)\n",
    "    對每個通道 C 分別正規化（跨 N, H, W 維度）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_features : int\n",
    "            通道數 C\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # 可學習參數\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "        \n",
    "        # Running statistics\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        self.cache = None\n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y : np.ndarray, shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        if self.training:\n",
    "            # 計算每個通道的 mean 和 var（跨 N, H, W）\n",
    "            batch_mean = np.mean(x, axis=(0, 2, 3))  # (C,)\n",
    "            batch_var = np.var(x, axis=(0, 2, 3))    # (C,)\n",
    "            \n",
    "            # 廣播形狀 (C,) -> (1, C, 1, 1)\n",
    "            mean_bc = batch_mean.reshape(1, C, 1, 1)\n",
    "            var_bc = batch_var.reshape(1, C, 1, 1)\n",
    "            \n",
    "            # 正規化\n",
    "            x_centered = x - mean_bc\n",
    "            std = np.sqrt(var_bc + self.eps)\n",
    "            x_norm = x_centered / std\n",
    "            \n",
    "            # Scale 和 Shift\n",
    "            gamma_bc = self.gamma.reshape(1, C, 1, 1)\n",
    "            beta_bc = self.beta.reshape(1, C, 1, 1)\n",
    "            y = gamma_bc * x_norm + beta_bc\n",
    "            \n",
    "            # 更新 running statistics\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
    "            \n",
    "            self.cache = (x, x_centered, std, x_norm, batch_mean, batch_var)\n",
    "        else:\n",
    "            mean_bc = self.running_mean.reshape(1, C, 1, 1)\n",
    "            var_bc = self.running_var.reshape(1, C, 1, 1)\n",
    "            gamma_bc = self.gamma.reshape(1, C, 1, 1)\n",
    "            beta_bc = self.beta.reshape(1, C, 1, 1)\n",
    "            \n",
    "            x_norm = (x - mean_bc) / np.sqrt(var_bc + self.eps)\n",
    "            y = gamma_bc * x_norm + beta_bc\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \"\"\"\n",
    "        x, x_centered, std, x_norm, batch_mean, batch_var = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        m = N * H * W  # 統計量計算的元素數\n",
    "        \n",
    "        # 廣播形狀\n",
    "        gamma_bc = self.gamma.reshape(1, C, 1, 1)\n",
    "        \n",
    "        # 對 gamma 和 beta 的梯度\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=(0, 2, 3))\n",
    "        self.dbeta = np.sum(dout, axis=(0, 2, 3))\n",
    "        \n",
    "        # 對 x 的梯度\n",
    "        dx_norm = dout * gamma_bc\n",
    "        \n",
    "        # 使用簡化的公式（效率更高）\n",
    "        # dx = (1/m) / std * (m * dx_norm - sum(dx_norm) - x_norm * sum(dx_norm * x_norm))\n",
    "        sum_dx_norm = np.sum(dx_norm, axis=(0, 2, 3), keepdims=True)\n",
    "        sum_dx_norm_xnorm = np.sum(dx_norm * x_norm, axis=(0, 2, 3), keepdims=True)\n",
    "        \n",
    "        dx = (1.0 / m) / std * (m * dx_norm - sum_dx_norm - x_norm * sum_dx_norm_xnorm)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def train(self):\n",
    "        self.training = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self.training = False\n",
    "\n",
    "# 測試 BatchNorm2D\n",
    "bn2d = BatchNorm2D(num_features=3)\n",
    "x = np.random.randn(4, 3, 8, 8) * 5 + 3  # 非標準分佈\n",
    "\n",
    "print(\"輸入統計量（每個通道）:\")\n",
    "for c in range(3):\n",
    "    print(f\"  通道 {c}: mean={np.mean(x[:, c]):.4f}, std={np.std(x[:, c]):.4f}\")\n",
    "\n",
    "y = bn2d.forward(x)\n",
    "\n",
    "print(\"\\n輸出統計量（正規化後）:\")\n",
    "for c in range(3):\n",
    "    print(f\"  通道 {c}: mean={np.mean(y[:, c]):.4f}, std={np.std(y[:, c]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm2D 梯度檢驗\n",
    "def gradient_check_bn2d(bn, x, eps=1e-5):\n",
    "    \"\"\"對 BatchNorm2D 進行梯度檢驗\"\"\"\n",
    "    bn.train()\n",
    "    \n",
    "    y = bn.forward(x)\n",
    "    dout = 2 * y\n",
    "    dx = bn.backward(dout)\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    # 檢驗 dgamma\n",
    "    print(\"=== 檢驗 dgamma ===\")\n",
    "    dgamma_numerical = np.zeros_like(bn.gamma)\n",
    "    for j in range(len(bn.gamma)):\n",
    "        old_val = bn.gamma[j]\n",
    "        \n",
    "        bn.gamma[j] = old_val + eps\n",
    "        loss_plus = np.sum(bn.forward(x) ** 2)\n",
    "        \n",
    "        bn.gamma[j] = old_val - eps\n",
    "        loss_minus = np.sum(bn.forward(x) ** 2)\n",
    "        \n",
    "        bn.gamma[j] = old_val\n",
    "        dgamma_numerical[j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    rel_error = np.max(np.abs(bn.dgamma - dgamma_numerical) / (np.abs(bn.dgamma) + np.abs(dgamma_numerical) + 1e-8))\n",
    "    print(f\"  最大相對誤差: {rel_error:.2e}\")\n",
    "    print(f\"  通過: {rel_error < 1e-4}\")\n",
    "    if rel_error > 1e-4:\n",
    "        all_passed = False\n",
    "    \n",
    "    # 檢驗 dx（抽樣）\n",
    "    print(\"\\n=== 檢驗 dx ===\")\n",
    "    dx_numerical = np.zeros_like(x)\n",
    "    x_test = x.copy()\n",
    "    \n",
    "    num_checks = min(10, x.size)\n",
    "    indices = np.random.choice(x.size, num_checks, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, x.shape)\n",
    "        old_val = x_test[multi_idx]\n",
    "        \n",
    "        x_test[multi_idx] = old_val + eps\n",
    "        loss_plus = np.sum(bn.forward(x_test) ** 2)\n",
    "        \n",
    "        x_test[multi_idx] = old_val - eps\n",
    "        loss_minus = np.sum(bn.forward(x_test) ** 2)\n",
    "        \n",
    "        x_test[multi_idx] = old_val\n",
    "        dx_numerical[multi_idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "    \n",
    "    max_error = 0\n",
    "    for idx in indices:\n",
    "        multi_idx = np.unravel_index(idx, x.shape)\n",
    "        error = abs(dx[multi_idx] - dx_numerical[multi_idx]) / (abs(dx[multi_idx]) + abs(dx_numerical[multi_idx]) + 1e-8)\n",
    "        max_error = max(max_error, error)\n",
    "    \n",
    "    print(f\"  最大相對誤差: {max_error:.2e}\")\n",
    "    print(f\"  通過: {max_error < 1e-4}\")\n",
    "    if max_error > 1e-4:\n",
    "        all_passed = False\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "bn2d = BatchNorm2D(num_features=3)\n",
    "x = np.random.randn(2, 3, 4, 4)\n",
    "gradient_check_bn2d(bn2d, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：BatchNorm 的效果\n",
    "\n",
    "讓我們視覺化 BatchNorm 對訓練的影響。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的網路來比較有無 BatchNorm 的效果\n",
    "\n",
    "class FCWithBN:\n",
    "    \"\"\"帶 BatchNorm 的全連接層\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        std = np.sqrt(2.0 / in_features)\n",
    "        self.W = np.random.randn(in_features, out_features) * std\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.bn = BatchNorm1D(out_features)\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = X\n",
    "        z = X @ self.W + self.b\n",
    "        out = self.bn.forward(z)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dz = self.bn.backward(dout)\n",
    "        X = self.cache\n",
    "        self.dW = X.T @ dz\n",
    "        self.db = np.sum(dz, axis=0)\n",
    "        return dz @ self.W.T\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_backward(dout, x):\n",
    "    return dout * (x > 0)\n",
    "\n",
    "\n",
    "class DeepNetWithBN:\n",
    "    \"\"\"深度網路（有 BatchNorm）\"\"\"\n",
    "    def __init__(self, layer_dims):\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            self.layers.append(FCWithBN(layer_dims[i], layer_dims[i+1]))\n",
    "        self.relu_cache = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.relu_cache = []\n",
    "        out = X\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            out = layer.forward(out)\n",
    "            self.relu_cache.append(out)\n",
    "            out = relu(out)\n",
    "        out = self.layers[-1].forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.layers[-1].backward(dout)\n",
    "        for i, layer in enumerate(reversed(self.layers[:-1])):\n",
    "            dout = relu_backward(dout, self.relu_cache[-(i+1)])\n",
    "            dout = layer.backward(dout)\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.append((layer.W, layer.dW))\n",
    "            params.append((layer.b, layer.db))\n",
    "            params.append((layer.bn.gamma, layer.bn.dgamma))\n",
    "            params.append((layer.bn.beta, layer.bn.dbeta))\n",
    "        return params\n",
    "\n",
    "\n",
    "class DeepNetNoBN:\n",
    "    \"\"\"深度網路（無 BatchNorm）\"\"\"\n",
    "    def __init__(self, layer_dims):\n",
    "        self.Ws = []\n",
    "        self.bs = []\n",
    "        self.dWs = []\n",
    "        self.dbs = []\n",
    "        \n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            std = np.sqrt(2.0 / layer_dims[i])\n",
    "            self.Ws.append(np.random.randn(layer_dims[i], layer_dims[i+1]) * std)\n",
    "            self.bs.append(np.zeros(layer_dims[i+1]))\n",
    "            self.dWs.append(None)\n",
    "            self.dbs.append(None)\n",
    "        \n",
    "        self.caches = []\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.caches = [X]\n",
    "        out = X\n",
    "        for i in range(len(self.Ws) - 1):\n",
    "            out = out @ self.Ws[i] + self.bs[i]\n",
    "            self.caches.append(out)\n",
    "            out = relu(out)\n",
    "        out = out @ self.Ws[-1] + self.bs[-1]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        self.dWs[-1] = self.caches[-1].T @ dout if len(self.caches) > len(self.Ws) else relu(self.caches[-1]).T @ dout\n",
    "        # 簡化版本\n",
    "        pass\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        params = []\n",
    "        for i in range(len(self.Ws)):\n",
    "            params.append((self.Ws[i], self.dWs[i]))\n",
    "            params.append((self.bs[i], self.dbs[i]))\n",
    "        return params\n",
    "\n",
    "print(\"網路定義完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 BatchNorm 對激活值分佈的影響\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# 深度網路參數\n",
    "layer_dims = [20, 50, 50, 50, 50, 10]\n",
    "\n",
    "# 無 BatchNorm\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 20)\n",
    "activations_no_bn = [X]\n",
    "\n",
    "for i in range(len(layer_dims) - 1):\n",
    "    W = np.random.randn(layer_dims[i], layer_dims[i+1]) * 0.1\n",
    "    b = np.zeros(layer_dims[i+1])\n",
    "    out = X @ W + b\n",
    "    if i < len(layer_dims) - 2:\n",
    "        out = relu(out)\n",
    "    activations_no_bn.append(out)\n",
    "    X = out\n",
    "\n",
    "# 有 BatchNorm\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 20)\n",
    "activations_bn = [X]\n",
    "\n",
    "for i in range(len(layer_dims) - 1):\n",
    "    W = np.random.randn(layer_dims[i], layer_dims[i+1]) * 0.1\n",
    "    b = np.zeros(layer_dims[i+1])\n",
    "    out = X @ W + b\n",
    "    \n",
    "    # BatchNorm\n",
    "    mean = np.mean(out, axis=0)\n",
    "    var = np.var(out, axis=0)\n",
    "    out = (out - mean) / np.sqrt(var + 1e-5)\n",
    "    \n",
    "    if i < len(layer_dims) - 2:\n",
    "        out = relu(out)\n",
    "    activations_bn.append(out)\n",
    "    X = out\n",
    "\n",
    "# 繪圖\n",
    "for i in range(4):\n",
    "    # 無 BatchNorm\n",
    "    ax = axes[0, i]\n",
    "    ax.hist(activations_no_bn[i+1].flatten(), bins=50, alpha=0.7)\n",
    "    ax.set_title(f'No BN - Layer {i+1}')\n",
    "    ax.set_xlim(-3, 3)\n",
    "    \n",
    "    # 有 BatchNorm\n",
    "    ax = axes[1, i]\n",
    "    ax.hist(activations_bn[i+1].flatten(), bins=50, alpha=0.7, color='orange')\n",
    "    ax.set_title(f'With BN - Layer {i+1}')\n",
    "    ax.set_xlim(-3, 3)\n",
    "\n",
    "axes[0, 0].set_ylabel('No BatchNorm')\n",
    "axes[1, 0].set_ylabel('With BatchNorm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"- 無 BatchNorm：激活值分佈逐層變化，可能會很窄或很寬\")\n",
    "print(\"- 有 BatchNorm：激活值分佈保持穩定，接近標準正態分佈\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習題\n",
    "\n",
    "### 練習 1：實作 Layer Normalization\n",
    "\n",
    "Layer Normalization 與 Batch Normalization 不同，它是對每個樣本的特徵維度進行正規化，而不是跨 batch。\n",
    "\n",
    "- **BatchNorm**：跨 batch 維度正規化\n",
    "- **LayerNorm**：跨 feature 維度正規化\n",
    "\n",
    "LayerNorm 在 Transformer 中被廣泛使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    \"\"\"\n",
    "    Layer Normalization\n",
    "    \n",
    "    對每個樣本的特徵維度正規化\n",
    "    輸入形狀：(N, D) 或 (N, C, H, W)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        normalized_shape : int or tuple\n",
    "            正規化的維度\n",
    "        \"\"\"\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        # 可學習參數\n",
    "        self.gamma = np.ones(normalized_shape)\n",
    "        self.beta = np.zeros(normalized_shape)\n",
    "        \n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向傳播\n",
    "        \"\"\"\n",
    "        # 解答：\n",
    "        # 確定正規化的軸（最後幾個維度）\n",
    "        num_axes = len(self.normalized_shape)\n",
    "        axes = tuple(range(-num_axes, 0))\n",
    "        \n",
    "        # 計算每個樣本的 mean 和 var\n",
    "        mean = np.mean(x, axis=axes, keepdims=True)\n",
    "        var = np.var(x, axis=axes, keepdims=True)\n",
    "        \n",
    "        # 正規化\n",
    "        x_centered = x - mean\n",
    "        std = np.sqrt(var + self.eps)\n",
    "        x_norm = x_centered / std\n",
    "        \n",
    "        # Scale 和 Shift\n",
    "        y = self.gamma * x_norm + self.beta\n",
    "        \n",
    "        self.cache = (x, x_centered, std, x_norm, axes)\n",
    "        return y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        反向傳播\n",
    "        \"\"\"\n",
    "        x, x_centered, std, x_norm, axes = self.cache\n",
    "        \n",
    "        # 計算正規化維度的元素數\n",
    "        m = 1\n",
    "        for ax in axes:\n",
    "            m *= x.shape[ax]\n",
    "        \n",
    "        # 對 gamma 和 beta 的梯度\n",
    "        self.dgamma = np.sum(dout * x_norm, axis=tuple(range(x.ndim - len(self.normalized_shape))))\n",
    "        self.dbeta = np.sum(dout, axis=tuple(range(x.ndim - len(self.normalized_shape))))\n",
    "        \n",
    "        # 對 x 的梯度\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # 使用簡化公式\n",
    "        sum_dx_norm = np.sum(dx_norm, axis=axes, keepdims=True)\n",
    "        sum_dx_norm_xnorm = np.sum(dx_norm * x_norm, axis=axes, keepdims=True)\n",
    "        \n",
    "        dx = (1.0 / m) / std * (m * dx_norm - sum_dx_norm - x_norm * sum_dx_norm_xnorm)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "# 測試 LayerNorm\n",
    "ln = LayerNorm(normalized_shape=10)\n",
    "x = np.random.randn(5, 10) * 3 + 2\n",
    "\n",
    "print(\"輸入統計量（每個樣本）:\")\n",
    "for i in range(3):\n",
    "    print(f\"  樣本 {i}: mean={np.mean(x[i]):.4f}, std={np.std(x[i]):.4f}\")\n",
    "\n",
    "y = ln.forward(x)\n",
    "\n",
    "print(\"\\n輸出統計量（正規化後）:\")\n",
    "for i in range(3):\n",
    "    print(f\"  樣本 {i}: mean={np.mean(y[i]):.4f}, std={np.std(y[i]):.4f}\")\n",
    "\n",
    "print(\"\\n比較：\")\n",
    "print(\"- BatchNorm 對每個特徵維度，跨樣本正規化\")\n",
    "print(\"- LayerNorm 對每個樣本，跨特徵維度正規化\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個 notebook 中，我們學習了：\n",
    "\n",
    "### Batch Normalization 公式\n",
    "\n",
    "1. **計算統計量**：$\\mu = \\frac{1}{m}\\sum x_i$, $\\sigma^2 = \\frac{1}{m}\\sum (x_i - \\mu)^2$\n",
    "2. **正規化**：$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "3. **縮放和平移**：$y = \\gamma \\hat{x} + \\beta$\n",
    "\n",
    "### Training vs Inference\n",
    "\n",
    "| 階段 | 使用的統計量 | 更新 running stats |\n",
    "|------|-------------|--------------------|\n",
    "| Training | batch mean/var | 是 |\n",
    "| Inference | running mean/var | 否 |\n",
    "\n",
    "### BatchNorm 的好處\n",
    "\n",
    "1. **減少 Internal Covariate Shift**：穩定訓練\n",
    "2. **允許更大的學習率**：加速收斂\n",
    "3. **輕微的正則化效果**：因為每個 batch 的統計量不同\n",
    "4. **減少對初始化的敏感度**\n",
    "\n",
    "### BatchNorm vs LayerNorm\n",
    "\n",
    "| 特性 | BatchNorm | LayerNorm |\n",
    "|------|-----------|----------|\n",
    "| 正規化軸 | 跨 batch | 跨 feature |\n",
    "| 適用場景 | CNN | Transformer, RNN |\n",
    "| 依賴 batch size | 是 | 否 |\n",
    "\n",
    "### 下一步\n",
    "\n",
    "接下來我們將學習 **ResNet Block**，利用 BatchNorm 和殘差連接構建更深的網路！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
