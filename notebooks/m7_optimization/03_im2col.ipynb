{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2col 技巧 - 卷積加速的核心\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解 im2col 如何把卷積轉換成矩陣乘法\n",
    "2. 實作 im2col forward 和 col2im backward\n",
    "3. 使用 GEMM（矩陣乘法）加速卷積\n",
    "4. 比較不同實作的效能\n",
    "\n",
    "## 核心想法\n",
    "\n",
    "> **卷積的本質是大量的向量內積**，而矩陣乘法正是把多個內積打包在一起。\n",
    "> 透過 im2col，我們可以利用高度優化的 BLAS 矩陣乘法來計算卷積。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：卷積到矩陣乘法的轉換\n",
    "\n",
    "### 1.1 為什麼這樣做？\n",
    "\n",
    "卷積的計算模式：\n",
    "```\n",
    "output[i,j] = Σ input_patch[i,j] * kernel\n",
    "            = inner_product(input_patch.flatten(), kernel.flatten())\n",
    "```\n",
    "\n",
    "如果把所有 patch 排成一個矩陣，卷積就變成矩陣乘法：\n",
    "\n",
    "```\n",
    "┌──────────────────────┐   ┌──────────┐   ┌─────────────┐\n",
    "│  patch_0 (flattened) │   │ kernel_0 │   │  output_0   │\n",
    "│  patch_1 (flattened) │ × │ kernel_1 │ = │  output_1   │\n",
    "│  patch_2 (flattened) │   │   ...    │   │    ...      │\n",
    "│       ...            │   │ kernel_n │   │  output_n   │\n",
    "└──────────────────────┘   └──────────┘   └─────────────┘\n",
    "   (M, K)                     (K, N)          (M, N)\n",
    "\n",
    "M = 輸出位置數 (out_H * out_W * N_batch)\n",
    "K = patch 大小 (C_in * kH * kW)\n",
    "N = 輸出 channel 數 (C_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 im2col 的概念\n",
    "\n",
    "def visualize_im2col_concept():\n",
    "    \"\"\"用簡單例子展示 im2col\"\"\"\n",
    "    \n",
    "    # 4x4 輸入，2x2 kernel，stride=1\n",
    "    x = np.arange(16).reshape(4, 4)\n",
    "    print(\"Input (4x4):\")\n",
    "    print(x)\n",
    "    print()\n",
    "    \n",
    "    # 2x2 kernel，輸出 3x3 = 9 個位置\n",
    "    # 每個 patch 有 4 個元素\n",
    "    \n",
    "    print(\"Patches (9 patches, each 2x2=4 elements):\")\n",
    "    kH, kW = 2, 2\n",
    "    patches = []\n",
    "    for i in range(3):  # out_H\n",
    "        for j in range(3):  # out_W\n",
    "            patch = x[i:i+kH, j:j+kW].flatten()\n",
    "            patches.append(patch)\n",
    "            print(f\"  Patch ({i},{j}): {patch}\")\n",
    "    \n",
    "    # im2col 結果\n",
    "    col = np.array(patches)\n",
    "    print(f\"\\nim2col result (9 x 4):\")\n",
    "    print(col)\n",
    "    print(f\"\\nShape: {col.shape} = (out_positions, patch_size)\")\n",
    "    \n",
    "    # 展示矩陣乘法\n",
    "    kernel = np.array([[1, 0], [0, 1]])  # 簡單的對角 kernel\n",
    "    kernel_flat = kernel.flatten()  # (4,)\n",
    "    \n",
    "    print(f\"\\nKernel:\")\n",
    "    print(kernel)\n",
    "    print(f\"Kernel flattened: {kernel_flat}\")\n",
    "    \n",
    "    # 矩陣乘法\n",
    "    output = col @ kernel_flat  # (9,) @ (4,) 變成 (9,) ... 實際上是 (9, 4) @ (4,) = (9,)\n",
    "    output = output.reshape(3, 3)\n",
    "    \n",
    "    print(f\"\\nOutput (via matrix multiply):\")\n",
    "    print(output)\n",
    "    \n",
    "    # 驗證：和 naive 卷積相同\n",
    "    output_naive = np.zeros((3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            output_naive[i, j] = np.sum(x[i:i+2, j:j+2] * kernel)\n",
    "    \n",
    "    print(f\"\\nOutput (via naive convolution):\")\n",
    "    print(output_naive)\n",
    "    print(f\"\\nMatch: {np.allclose(output, output_naive)}\")\n",
    "\n",
    "visualize_im2col_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：im2col 實作\n",
    "\n",
    "### 2.1 Method 1: 使用迴圈（慢但清晰）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_loop(x, kH, kW, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    im2col 的迴圈實作（慢但清晰）\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray, shape (N, C, H, W)\n",
    "        輸入圖片\n",
    "    kH, kW : int\n",
    "        kernel 大小\n",
    "    stride : int\n",
    "        步幅\n",
    "    padding : int\n",
    "        零填充\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : np.ndarray, shape (N * out_H * out_W, C * kH * kW)\n",
    "        展開後的矩陣\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), \n",
    "                   mode='constant', constant_values=0)\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 輸出矩陣\n",
    "    col = np.zeros((N * out_H * out_W, C * kH * kW))\n",
    "    \n",
    "    # 填充\n",
    "    idx = 0\n",
    "    for n in range(N):\n",
    "        for i in range(out_H):\n",
    "            for j in range(out_W):\n",
    "                h_start = i * stride\n",
    "                w_start = j * stride\n",
    "                # 取出 patch 並展平\n",
    "                patch = x[n, :, h_start:h_start+kH, w_start:w_start+kW]\n",
    "                col[idx] = patch.flatten()\n",
    "                idx += 1\n",
    "    \n",
    "    return col\n",
    "\n",
    "\n",
    "# 測試\n",
    "x = np.random.randn(2, 3, 8, 8)  # 2 張 3 channel 的 8x8 圖片\n",
    "col = im2col_loop(x, kH=3, kW=3, stride=1, padding=1)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"im2col output shape: {col.shape}\")\n",
    "print(f\"Expected: (N*out_H*out_W, C*kH*kW) = (2*8*8, 3*3*3) = (128, 27)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Method 2: 使用 stride_tricks（快）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_strided(x, kH, kW, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    im2col 的高效實作（使用 stride_tricks）\n",
    "    \n",
    "    核心想法：\n",
    "    1. 用 as_strided 建立一個 view，包含所有 patches\n",
    "    2. 只是改變了 strides，沒有實際複製數據\n",
    "    3. reshape 時才複製（如果需要）\n",
    "    \"\"\"\n",
    "    N, C, H, W = x.shape\n",
    "    \n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)),\n",
    "                   mode='constant', constant_values=0)\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # 計算 strides\n",
    "    # x 的 strides: (N_stride, C_stride, H_stride, W_stride)\n",
    "    # 我們要建立 shape 為 (N, C, kH, kW, out_H, out_W) 的 view\n",
    "    \n",
    "    # 原始 strides（以 bytes 為單位）\n",
    "    s0, s1, s2, s3 = x.strides\n",
    "    \n",
    "    # 新的 shape 和 strides\n",
    "    shape = (N, C, kH, kW, out_H, out_W)\n",
    "    strides = (s0,                  # 跨 batch\n",
    "               s1,                  # 跨 channel\n",
    "               s2,                  # 跨 kernel height\n",
    "               s3,                  # 跨 kernel width\n",
    "               s2 * stride,         # 跨 output height\n",
    "               s3 * stride)         # 跨 output width\n",
    "    \n",
    "    # 建立 strided view\n",
    "    col_strided = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n",
    "    \n",
    "    # Reshape: (N, C, kH, kW, out_H, out_W) -> (N * out_H * out_W, C * kH * kW)\n",
    "    # 先 transpose 成 (N, out_H, out_W, C, kH, kW)\n",
    "    col = col_strided.transpose(0, 4, 5, 1, 2, 3)\n",
    "    # 再 reshape\n",
    "    col = col.reshape(N * out_H * out_W, -1)\n",
    "    \n",
    "    return col\n",
    "\n",
    "\n",
    "# 測試和驗證\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "col_loop = im2col_loop(x, 3, 3, stride=1, padding=1)\n",
    "col_strided = im2col_strided(x, 3, 3, stride=1, padding=1)\n",
    "\n",
    "print(f\"Loop output shape:    {col_loop.shape}\")\n",
    "print(f\"Strided output shape: {col_strided.shape}\")\n",
    "print(f\"Results match: {np.allclose(col_loop, col_strided)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 效能比較\n",
    "\n",
    "x = np.random.randn(4, 32, 64, 64).astype(np.float32)\n",
    "\n",
    "# Loop version\n",
    "start = time.perf_counter()\n",
    "col1 = im2col_loop(x, 3, 3, stride=1, padding=1)\n",
    "time_loop = time.perf_counter() - start\n",
    "\n",
    "# Strided version\n",
    "start = time.perf_counter()\n",
    "col2 = im2col_strided(x, 3, 3, stride=1, padding=1)\n",
    "time_strided = time.perf_counter() - start\n",
    "\n",
    "print(f\"im2col performance (4, 32, 64, 64) with 3x3 kernel:\")\n",
    "print(f\"  Loop:    {time_loop:.4f}s\")\n",
    "print(f\"  Strided: {time_strided:.4f}s\")\n",
    "print(f\"  Speedup: {time_loop / time_strided:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：col2im（backward 用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, x_shape, kH, kW, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    im2col 的反操作（用於 backward）\n",
    "    \n",
    "    把 col 矩陣「折回」成原始的圖片格式。\n",
    "    注意：重疊的位置會被加總（這正是 backward 需要的行為）。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : np.ndarray, shape (N * out_H * out_W, C * kH * kW)\n",
    "    x_shape : tuple\n",
    "        原始輸入的 shape (N, C, H, W)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray, shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    \n",
    "    # 計算 padding 後的大小\n",
    "    H_pad = H + 2 * padding\n",
    "    W_pad = W + 2 * padding\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # Reshape col: (N*out_H*out_W, C*kH*kW) -> (N, out_H, out_W, C, kH, kW)\n",
    "    col = col.reshape(N, out_H, out_W, C, kH, kW)\n",
    "    # Transpose: (N, C, kH, kW, out_H, out_W)\n",
    "    col = col.transpose(0, 3, 4, 5, 1, 2)\n",
    "    \n",
    "    # 初始化輸出（帶 padding）\n",
    "    x_pad = np.zeros((N, C, H_pad, W_pad), dtype=col.dtype)\n",
    "    \n",
    "    # 把 col 的值加回去\n",
    "    # col shape: (N, C, kH, kW, out_H, out_W)\n",
    "    for i in range(kH):\n",
    "        i_max = i + stride * out_H\n",
    "        for j in range(kW):\n",
    "            j_max = j + stride * out_W\n",
    "            # 這裡用 += 因為重疊的位置需要加總\n",
    "            x_pad[:, :, i:i_max:stride, j:j_max:stride] += col[:, :, i, j, :, :]\n",
    "    \n",
    "    # 移除 padding\n",
    "    if padding > 0:\n",
    "        return x_pad[:, :, padding:-padding, padding:-padding]\n",
    "    return x_pad\n",
    "\n",
    "\n",
    "# 測試 col2im\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "col = im2col_strided(x, 3, 3, stride=1, padding=1)\n",
    "x_reconstructed = col2im(col, x.shape, 3, 3, stride=1, padding=1)\n",
    "\n",
    "print(f\"Original x shape: {x.shape}\")\n",
    "print(f\"col shape: {col.shape}\")\n",
    "print(f\"Reconstructed x shape: {x_reconstructed.shape}\")\n",
    "\n",
    "# 注意：因為 3x3 kernel 會有 9 倍的重疊，重建不會完全相等\n",
    "# 但中間的值（被完整覆蓋 9 次）應該是 9 倍\n",
    "print(f\"\\nCenter values comparison (should be 9x):\")\n",
    "print(f\"  Original center: {x[0, 0, 3, 3]:.4f}\")\n",
    "print(f\"  Reconstructed:   {x_reconstructed[0, 0, 3, 3]:.4f}\")\n",
    "print(f\"  Ratio:           {x_reconstructed[0, 0, 3, 3] / x[0, 0, 3, 3]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：使用 im2col 實作卷積"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D_im2col:\n",
    "    \"\"\"\n",
    "    使用 im2col 加速的 2D 卷積層\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # He initialization\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.W = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.b = np.zeros(out_channels)\n",
    "        \n",
    "        # 梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        # Cache\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass using im2col + matrix multiplication\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (N, C_in, H, W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : np.ndarray, shape (N, C_out, H_out, W_out)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        # 計算輸出大小\n",
    "        H_pad = H + 2 * self.padding\n",
    "        W_pad = W + 2 * self.padding\n",
    "        out_H = (H_pad - kH) // self.stride + 1\n",
    "        out_W = (W_pad - kW) // self.stride + 1\n",
    "        \n",
    "        # im2col: (N*out_H*out_W, C_in*kH*kW)\n",
    "        col = im2col_strided(x, kH, kW, self.stride, self.padding)\n",
    "        \n",
    "        # Reshape weights: (C_out, C_in*kH*kW)\n",
    "        W_col = self.W.reshape(self.out_channels, -1)\n",
    "        \n",
    "        # Matrix multiplication: (N*out_H*out_W, C_in*kH*kW) @ (C_in*kH*kW, C_out)\n",
    "        # Result: (N*out_H*out_W, C_out)\n",
    "        out_col = col @ W_col.T + self.b\n",
    "        \n",
    "        # Reshape output: (N, out_H, out_W, C_out) -> (N, C_out, out_H, out_W)\n",
    "        out = out_col.reshape(N, out_H, out_W, self.out_channels).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # Cache for backward\n",
    "        self.cache = (x, col, W_col)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        Backward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dout : np.ndarray, shape (N, C_out, out_H, out_W)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : np.ndarray, shape (N, C_in, H, W)\n",
    "        \"\"\"\n",
    "        x, col, W_col = self.cache\n",
    "        N, C, H, W = x.shape\n",
    "        kH = kW = self.kernel_size\n",
    "        \n",
    "        # Reshape dout: (N, C_out, out_H, out_W) -> (N*out_H*out_W, C_out)\n",
    "        dout_col = dout.transpose(0, 2, 3, 1).reshape(-1, self.out_channels)\n",
    "        \n",
    "        # Gradient for weights: dW = col.T @ dout_col\n",
    "        # col: (N*out_H*out_W, C_in*kH*kW)\n",
    "        # dout_col: (N*out_H*out_W, C_out)\n",
    "        # dW_col: (C_in*kH*kW, C_out)\n",
    "        dW_col = col.T @ dout_col  # (C_in*kH*kW, C_out)\n",
    "        self.dW = dW_col.T.reshape(self.W.shape)  # (C_out, C_in, kH, kW)\n",
    "        \n",
    "        # Gradient for bias\n",
    "        self.db = dout_col.sum(axis=0)\n",
    "        \n",
    "        # Gradient for input: dcol = dout_col @ W_col\n",
    "        # dout_col: (N*out_H*out_W, C_out)\n",
    "        # W_col: (C_out, C_in*kH*kW)\n",
    "        dcol = dout_col @ W_col  # (N*out_H*out_W, C_in*kH*kW)\n",
    "        \n",
    "        # col2im\n",
    "        dx = col2im(dcol, x.shape, kH, kW, self.stride, self.padding)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "# 測試\n",
    "conv = Conv2D_im2col(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "x = np.random.randn(2, 3, 32, 32)\n",
    "\n",
    "# Forward\n",
    "out = conv.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Backward\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx = conv.backward(dout)\n",
    "print(f\"dx shape: {dx.shape}\")\n",
    "print(f\"dW shape: {conv.dW.shape}\")\n",
    "print(f\"db shape: {conv.db.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check\n",
    "\n",
    "def numerical_gradient(f, x, eps=1e-5):\n",
    "    \"\"\"數值梯度\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + eps\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = old_val - eps\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * eps)\n",
    "        x[idx] = old_val\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# 小型測試\n",
    "np.random.seed(42)\n",
    "conv = Conv2D_im2col(in_channels=2, out_channels=3, kernel_size=3, padding=1)\n",
    "x = np.random.randn(1, 2, 4, 4)\n",
    "\n",
    "# Forward and backward\n",
    "out = conv.forward(x)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx_analytic = conv.backward(dout)\n",
    "\n",
    "# Numerical gradient for dx\n",
    "def f_x(x_):\n",
    "    return np.sum(conv.forward(x_) * dout)\n",
    "\n",
    "dx_numeric = numerical_gradient(f_x, x.copy())\n",
    "\n",
    "# Compare\n",
    "diff = np.abs(dx_analytic - dx_numeric)\n",
    "max_diff = np.max(diff)\n",
    "rel_error = max_diff / (np.maximum(np.abs(dx_analytic).max(), np.abs(dx_numeric).max()) + 1e-8)\n",
    "\n",
    "print(f\"Gradient check for dx:\")\n",
    "print(f\"  Max difference: {max_diff:.2e}\")\n",
    "print(f\"  Relative error: {rel_error:.2e}\")\n",
    "print(f\"  Status: {'PASS' if rel_error < 1e-4 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第五部分：效能比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_naive_4d(x, W, b=None, stride=1, padding=0):\n",
    "    \"\"\"Naive 4D 卷積（6 層迴圈）\"\"\"\n",
    "    N, C_in, H, W_in = x.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    if padding > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)))\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    output = np.zeros((N, C_out, out_H, out_W))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for c_out in range(C_out):\n",
    "            for i in range(out_H):\n",
    "                for j in range(out_W):\n",
    "                    h_start = i * stride\n",
    "                    w_start = j * stride\n",
    "                    patch = x[n, :, h_start:h_start+kH, w_start:w_start+kW]\n",
    "                    output[n, c_out, i, j] = np.sum(patch * W[c_out])\n",
    "    \n",
    "    if b is not None:\n",
    "        output += b.reshape(1, -1, 1, 1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def benchmark_conv(sizes, C_in=3, C_out=32, kernel_size=3):\n",
    "    \"\"\"比較 naive 和 im2col 的效能\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    W = np.random.randn(C_out, C_in, kernel_size, kernel_size).astype(np.float32)\n",
    "    b = np.random.randn(C_out).astype(np.float32)\n",
    "    conv = Conv2D_im2col(C_in, C_out, kernel_size, padding=1)\n",
    "    conv.W = W.copy()\n",
    "    conv.b = b.copy()\n",
    "    \n",
    "    print(f\"{'Size':>10} {'Naive':>12} {'im2col':>12} {'Speedup':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        x = np.random.randn(1, C_in, size, size).astype(np.float32)\n",
    "        \n",
    "        # Naive (skip if too slow)\n",
    "        if size <= 32:\n",
    "            start = time.perf_counter()\n",
    "            out_naive = conv2d_naive_4d(x, W, b, padding=1)\n",
    "            time_naive = time.perf_counter() - start\n",
    "        else:\n",
    "            time_naive = np.nan\n",
    "        \n",
    "        # im2col\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            out_im2col = conv.forward(x)\n",
    "        time_im2col = (time.perf_counter() - start) / 10\n",
    "        \n",
    "        if not np.isnan(time_naive):\n",
    "            speedup = time_naive / time_im2col\n",
    "            print(f\"{size:>10} {time_naive:>10.4f}s {time_im2col:>10.4f}s {speedup:>9.1f}x\")\n",
    "        else:\n",
    "            print(f\"{size:>10} {'skip':>12} {time_im2col:>10.4f}s {'N/A':>10}\")\n",
    "        \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'naive': time_naive,\n",
    "            'im2col': time_im2col\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Convolution Benchmark (3x3 kernel, padding=1):\")\n",
    "print(\"=\" * 50)\n",
    "results = benchmark_conv([8, 16, 32, 64, 128, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製效能圖\n",
    "\n",
    "sizes = [r['size'] for r in results]\n",
    "times_naive = [r['naive'] for r in results]\n",
    "times_im2col = [r['im2col'] for r in results]\n",
    "\n",
    "# 過濾 NaN\n",
    "valid = [(s, n, i) for s, n, i in zip(sizes, times_naive, times_im2col) if not np.isnan(n)]\n",
    "sizes_valid = [v[0] for v in valid]\n",
    "naive_valid = [v[1] for v in valid]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 時間比較\n",
    "axes[0].semilogy(sizes_valid, naive_valid, 'o-', label='Naive (6 loops)', linewidth=2, markersize=8)\n",
    "axes[0].semilogy(sizes, times_im2col, 's-', label='im2col + GEMM', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Image Size (pixels)', fontsize=12)\n",
    "axes[0].set_ylabel('Time (seconds, log scale)', fontsize=12)\n",
    "axes[0].set_title('Convolution Performance', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "speedups = [n / i for n, i in zip(naive_valid, [r['im2col'] for r in results[:len(naive_valid)]])]\n",
    "axes[1].bar(range(len(sizes_valid)), speedups, color='green', alpha=0.7)\n",
    "axes[1].set_xticks(range(len(sizes_valid)))\n",
    "axes[1].set_xticklabels(sizes_valid)\n",
    "axes[1].set_xlabel('Image Size (pixels)', fontsize=12)\n",
    "axes[1].set_ylabel('Speedup (x times)', fontsize=12)\n",
    "axes[1].set_title('im2col Speedup over Naive', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第六部分：im2col 的記憶體考量\n",
    "\n",
    "im2col 的缺點是**記憶體使用量大**：\n",
    "\n",
    "- 原始輸入：`N * C * H * W`\n",
    "- im2col 後：`N * out_H * out_W * C * kH * kW`\n",
    "\n",
    "對於 3x3 kernel，記憶體增加約 **9 倍**！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(N, C, H, W, kH, kW, padding=0):\n",
    "    \"\"\"分析 im2col 的記憶體使用\"\"\"\n",
    "    bytes_per_float = 4  # float32\n",
    "    \n",
    "    # 原始輸入\n",
    "    input_size = N * C * H * W\n",
    "    input_bytes = input_size * bytes_per_float\n",
    "    \n",
    "    # im2col 輸出\n",
    "    out_H = H + 2 * padding - kH + 1\n",
    "    out_W = W + 2 * padding - kW + 1\n",
    "    col_size = N * out_H * out_W * C * kH * kW\n",
    "    col_bytes = col_size * bytes_per_float\n",
    "    \n",
    "    # 輸出\n",
    "    # (假設 C_out = C)\n",
    "    output_size = N * C * out_H * out_W\n",
    "    output_bytes = output_size * bytes_per_float\n",
    "    \n",
    "    print(f\"Memory analysis for ({N}, {C}, {H}, {W}) with {kH}x{kW} kernel:\")\n",
    "    print(f\"  Input:    {input_bytes / 1024**2:.2f} MB ({input_size:,} floats)\")\n",
    "    print(f\"  im2col:   {col_bytes / 1024**2:.2f} MB ({col_size:,} floats)\")\n",
    "    print(f\"  Output:   {output_bytes / 1024**2:.2f} MB\")\n",
    "    print(f\"  Memory expansion: {col_bytes / input_bytes:.1f}x\")\n",
    "    print(f\"  Total peak memory: {(input_bytes + col_bytes + output_bytes) / 1024**2:.2f} MB\")\n",
    "\n",
    "# 分析不同配置\n",
    "print(\"Small model:\")\n",
    "analyze_memory_usage(32, 64, 56, 56, 3, 3, padding=1)\n",
    "\n",
    "print(\"\\nLarge model:\")\n",
    "analyze_memory_usage(32, 256, 56, 56, 3, 3, padding=1)\n",
    "\n",
    "print(\"\\n7x7 kernel:\")\n",
    "analyze_memory_usage(32, 64, 56, 56, 7, 7, padding=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### im2col 的核心概念\n",
    "\n",
    "1. **把卷積轉成矩陣乘法**\n",
    "   - 所有 patch 展開成一個大矩陣\n",
    "   - 用高度優化的 GEMM 計算\n",
    "\n",
    "2. **im2col**：Forward 用\n",
    "   - 把輸入展開成 `(N*out_H*out_W, C*kH*kW)`\n",
    "   - 使用 `stride_tricks` 避免實際複製\n",
    "\n",
    "3. **col2im**：Backward 用\n",
    "   - 把梯度折回原始 shape\n",
    "   - 重疊位置會被加總\n",
    "\n",
    "### 效能提升\n",
    "\n",
    "| 方法 | 時間複雜度相同，但... |\n",
    "|-----|----------------------|\n",
    "| Naive | 6 層 Python 迴圈開銷大 |\n",
    "| im2col | 1 次 GEMM，利用 BLAS 多執行緒 |\n",
    "\n",
    "### 取捨\n",
    "\n",
    "- **優點**：速度快 10-100 倍\n",
    "- **缺點**：記憶體增加 kH*kW 倍（3x3 kernel 約 9 倍）\n",
    "\n",
    "### 實際應用\n",
    "\n",
    "- 所有主流深度學習框架都用 im2col 或類似技術\n",
    "- GPU 上還有更高效的 CUDA 實作（cuDNN）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
