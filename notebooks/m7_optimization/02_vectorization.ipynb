{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量化 vs For-Loop\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解為什麼向量化（vectorization）快\n",
    "2. 學習把 Python for-loop 改寫成 numpy 操作\n",
    "3. 掌握 broadcasting 技巧\n",
    "4. 實際比較不同實作的效能差異\n",
    "\n",
    "## 核心原則\n",
    "\n",
    "> **把計算從 Python 移到 C**：每次呼叫 Python 都有開銷，\n",
    "> 向量化讓你用一次 Python 呼叫完成大量計算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第一部分：為什麼向量化快？\n",
    "\n",
    "### 1.1 Python 迴圈的開銷\n",
    "\n",
    "Python 是解釋型語言，每行程式碼執行時需要：\n",
    "\n",
    "1. **Type checking**：Python 是動態型別，每次操作都要檢查\n",
    "2. **Method lookup**：找到 `+` 對應的 `__add__` 方法\n",
    "3. **Python object overhead**：每個 Python 物件都有額外的記憶體開銷\n",
    "4. **GIL**：全域鎖限制並行\n",
    "\n",
    "```python\n",
    "# Python for-loop：每次迭代都有上述開銷\n",
    "for i in range(1000000):\n",
    "    result[i] = a[i] + b[i]  # 100 萬次 Python 操作\n",
    "\n",
    "# NumPy 向量化：開銷只有一次\n",
    "result = a + b  # 1 次 Python 呼叫，100 萬次 C 操作\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實驗：簡單的向量加法\n",
    "\n",
    "def add_forloop(a, b):\n",
    "    \"\"\"純 Python for-loop 實作\"\"\"\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] + b[i]\n",
    "    return result\n",
    "\n",
    "def add_vectorized(a, b):\n",
    "    \"\"\"NumPy 向量化實作\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 測試不同大小\n",
    "sizes = [1000, 10000, 100000, 1000000]\n",
    "\n",
    "print(\"Vector Addition Benchmark:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Size':>10} {'For-loop':>15} {'Vectorized':>15} {'Speedup':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in sizes:\n",
    "    a = np.random.randn(size)\n",
    "    b = np.random.randn(size)\n",
    "    \n",
    "    # For-loop\n",
    "    start = time.perf_counter()\n",
    "    result1 = add_forloop(a, b)\n",
    "    time_forloop = time.perf_counter() - start\n",
    "    \n",
    "    # Vectorized\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):  # 多次測量取平均\n",
    "        result2 = add_vectorized(a, b)\n",
    "    time_vectorized = (time.perf_counter() - start) / 100\n",
    "    \n",
    "    speedup = time_forloop / time_vectorized\n",
    "    print(f\"{size:>10,} {time_forloop:>12.6f}s {time_vectorized:>12.6f}s {speedup:>9.1f}x\")\n",
    "    \n",
    "    # 驗證結果相同\n",
    "    assert np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 向量化的優勢\n",
    "\n",
    "1. **減少 interpreter overhead**：一次 Python 呼叫 vs N 次\n",
    "2. **SIMD 指令**：CPU 可以同時處理多個數據（Single Instruction Multiple Data）\n",
    "3. **Cache-friendly**：連續記憶體存取更快\n",
    "4. **可能使用多執行緒**：某些操作可以自動並行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更複雜的例子：計算歐氏距離\n",
    "\n",
    "def euclidean_distance_forloop(X, y):\n",
    "    \"\"\"計算 X 中每個點到 y 的距離（for-loop）\n",
    "    \n",
    "    X: (N, D) - N 個 D 維向量\n",
    "    y: (D,) - 一個 D 維向量\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    distances = np.empty(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        diff = X[i] - y\n",
    "        dist = 0\n",
    "        for j in range(len(diff)):\n",
    "            dist += diff[j] ** 2\n",
    "        distances[i] = np.sqrt(dist)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "def euclidean_distance_vectorized(X, y):\n",
    "    \"\"\"計算 X 中每個點到 y 的距離（向量化）\"\"\"\n",
    "    diff = X - y  # broadcasting: (N, D) - (D,) = (N, D)\n",
    "    return np.sqrt(np.sum(diff ** 2, axis=1))  # (N,)\n",
    "\n",
    "# 測試\n",
    "N, D = 10000, 100\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randn(D)\n",
    "\n",
    "# For-loop\n",
    "start = time.perf_counter()\n",
    "dist1 = euclidean_distance_forloop(X, y)\n",
    "time_forloop = time.perf_counter() - start\n",
    "\n",
    "# Vectorized\n",
    "start = time.perf_counter()\n",
    "for _ in range(100):\n",
    "    dist2 = euclidean_distance_vectorized(X, y)\n",
    "time_vectorized = (time.perf_counter() - start) / 100\n",
    "\n",
    "print(f\"Euclidean distance ({N} points, {D} dims):\")\n",
    "print(f\"  For-loop:   {time_forloop:.4f}s\")\n",
    "print(f\"  Vectorized: {time_vectorized:.6f}s\")\n",
    "print(f\"  Speedup:    {time_forloop / time_vectorized:.1f}x\")\n",
    "print(f\"  Results match: {np.allclose(dist1, dist2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第二部分：Broadcasting 技巧\n",
    "\n",
    "### 2.1 Broadcasting 規則\n",
    "\n",
    "NumPy 會自動「擴展」較小的陣列來匹配較大的：\n",
    "\n",
    "```\n",
    "規則 1: 如果維度數不同，在前面補 1\n",
    "規則 2: 如果某維度大小為 1，擴展到匹配的大小\n",
    "規則 3: 如果大小不為 1 且不相等，報錯\n",
    "```\n",
    "\n",
    "範例：\n",
    "```\n",
    "(3, 4) + (4,)    →  (3, 4) + (1, 4)  →  (3, 4)\n",
    "(3, 4) + (3, 1)  →  (3, 4)\n",
    "(3, 4) + (1,)    →  (3, 4) + (1, 1)  →  (3, 4)\n",
    "(3, 4) + (5,)    →  Error! (4 != 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting 範例\n",
    "\n",
    "# 1. 向每一行加上不同的值\n",
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "row_add = np.array([10, 20, 30])\n",
    "\n",
    "print(\"A (2, 3):\")\n",
    "print(A)\n",
    "print(\"\\nrow_add (3,):\")\n",
    "print(row_add)\n",
    "print(\"\\nA + row_add (broadcast over rows):\")\n",
    "print(A + row_add)\n",
    "\n",
    "# 2. 向每一列加上不同的值\n",
    "col_add = np.array([[100], [200]])  # shape (2, 1)\n",
    "\n",
    "print(\"\\ncol_add (2, 1):\")\n",
    "print(col_add)\n",
    "print(\"\\nA + col_add (broadcast over columns):\")\n",
    "print(A + col_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting 的強大應用：距離矩陣\n",
    "\n",
    "def pairwise_distance_forloop(X, Y):\n",
    "    \"\"\"計算 X 和 Y 之間的距離矩陣（for-loop）\n",
    "    \n",
    "    X: (N, D)\n",
    "    Y: (M, D)\n",
    "    返回: (N, M) 距離矩陣\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    M = Y.shape[0]\n",
    "    D = np.zeros((N, M))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            diff = X[i] - Y[j]\n",
    "            D[i, j] = np.sqrt(np.sum(diff ** 2))\n",
    "    \n",
    "    return D\n",
    "\n",
    "def pairwise_distance_broadcast(X, Y):\n",
    "    \"\"\"計算距離矩陣（使用 broadcasting）\n",
    "    \n",
    "    技巧：\n",
    "    X: (N, D) -> (N, 1, D)\n",
    "    Y: (M, D) -> (1, M, D)\n",
    "    X - Y: (N, M, D)  -> 每對點的差\n",
    "    \"\"\"\n",
    "    # 擴展維度\n",
    "    X_expanded = X[:, np.newaxis, :]  # (N, 1, D)\n",
    "    Y_expanded = Y[np.newaxis, :, :]  # (1, M, D)\n",
    "    \n",
    "    # 差值（broadcasting）\n",
    "    diff = X_expanded - Y_expanded  # (N, M, D)\n",
    "    \n",
    "    # 距離\n",
    "    return np.sqrt(np.sum(diff ** 2, axis=2))  # (N, M)\n",
    "\n",
    "def pairwise_distance_efficient(X, Y):\n",
    "    \"\"\"更記憶體高效的版本\n",
    "    \n",
    "    使用恆等式：||x-y||^2 = ||x||^2 + ||y||^2 - 2*x.y\n",
    "    \"\"\"\n",
    "    X_sq = np.sum(X ** 2, axis=1, keepdims=True)  # (N, 1)\n",
    "    Y_sq = np.sum(Y ** 2, axis=1, keepdims=True)  # (M, 1)\n",
    "    \n",
    "    # X_sq: (N, 1) broadcast to (N, M)\n",
    "    # Y_sq.T: (1, M) broadcast to (N, M)\n",
    "    # X @ Y.T: (N, M)\n",
    "    D_sq = X_sq + Y_sq.T - 2 * X @ Y.T\n",
    "    \n",
    "    # 數值穩定性：可能有極小的負數\n",
    "    D_sq = np.maximum(D_sq, 0)\n",
    "    \n",
    "    return np.sqrt(D_sq)\n",
    "\n",
    "# 測試\n",
    "N, M, D = 500, 300, 50\n",
    "X = np.random.randn(N, D)\n",
    "Y = np.random.randn(M, D)\n",
    "\n",
    "# For-loop\n",
    "start = time.perf_counter()\n",
    "D1 = pairwise_distance_forloop(X, Y)\n",
    "time_forloop = time.perf_counter() - start\n",
    "\n",
    "# Broadcasting\n",
    "start = time.perf_counter()\n",
    "D2 = pairwise_distance_broadcast(X, Y)\n",
    "time_broadcast = time.perf_counter() - start\n",
    "\n",
    "# Efficient\n",
    "start = time.perf_counter()\n",
    "D3 = pairwise_distance_efficient(X, Y)\n",
    "time_efficient = time.perf_counter() - start\n",
    "\n",
    "print(f\"Pairwise distance ({N}x{M}, {D} dims):\")\n",
    "print(f\"  For-loop:    {time_forloop:.4f}s\")\n",
    "print(f\"  Broadcasting: {time_broadcast:.4f}s  ({time_forloop/time_broadcast:.1f}x faster)\")\n",
    "print(f\"  Efficient:   {time_efficient:.4f}s  ({time_forloop/time_efficient:.1f}x faster)\")\n",
    "print(f\"\\n  Results match: broadcast={np.allclose(D1, D2)}, efficient={np.allclose(D1, D3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 常見的 Broadcasting 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常見模式 1：標準化（每行減去該行的平均）\n",
    "\n",
    "X = np.random.randn(100, 50)\n",
    "\n",
    "# For-loop 版本\n",
    "def normalize_forloop(X):\n",
    "    result = np.empty_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        mean = np.mean(X[i])\n",
    "        std = np.std(X[i])\n",
    "        result[i] = (X[i] - mean) / (std + 1e-8)\n",
    "    return result\n",
    "\n",
    "# 向量化版本\n",
    "def normalize_vectorized(X):\n",
    "    mean = X.mean(axis=1, keepdims=True)  # (N, 1)\n",
    "    std = X.std(axis=1, keepdims=True)    # (N, 1)\n",
    "    return (X - mean) / (std + 1e-8)      # broadcasting\n",
    "\n",
    "# 測試\n",
    "%timeit normalize_forloop(X)\n",
    "%timeit normalize_vectorized(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常見模式 2：Softmax\n",
    "\n",
    "def softmax_forloop(X):\n",
    "    \"\"\"對每一行做 softmax（for-loop）\"\"\"\n",
    "    result = np.empty_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        exp_x = np.exp(X[i] - np.max(X[i]))  # 數值穩定\n",
    "        result[i] = exp_x / np.sum(exp_x)\n",
    "    return result\n",
    "\n",
    "def softmax_vectorized(X):\n",
    "    \"\"\"對每一行做 softmax（向量化）\"\"\"\n",
    "    X_max = X.max(axis=1, keepdims=True)  # (N, 1)\n",
    "    exp_x = np.exp(X - X_max)             # broadcasting\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)  # broadcasting\n",
    "\n",
    "# 測試\n",
    "X = np.random.randn(1000, 100)\n",
    "\n",
    "start = time.perf_counter()\n",
    "result1 = softmax_forloop(X)\n",
    "time_forloop = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "result2 = softmax_vectorized(X)\n",
    "time_vectorized = time.perf_counter() - start\n",
    "\n",
    "print(f\"Softmax (1000 x 100):\")\n",
    "print(f\"  For-loop:   {time_forloop:.4f}s\")\n",
    "print(f\"  Vectorized: {time_vectorized:.4f}s\")\n",
    "print(f\"  Speedup:    {time_forloop / time_vectorized:.1f}x\")\n",
    "print(f\"  Match: {np.allclose(result1, result2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常見模式 3：外積（outer product）\n",
    "\n",
    "def outer_forloop(a, b):\n",
    "    \"\"\"計算外積 a @ b.T（for-loop）\"\"\"\n",
    "    result = np.empty((len(a), len(b)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            result[i, j] = a[i] * b[j]\n",
    "    return result\n",
    "\n",
    "def outer_vectorized(a, b):\n",
    "    \"\"\"計算外積（使用 broadcasting）\"\"\"\n",
    "    return a[:, np.newaxis] * b[np.newaxis, :]  # (N, 1) * (1, M) = (N, M)\n",
    "\n",
    "def outer_numpy(a, b):\n",
    "    \"\"\"使用 numpy 內建函數\"\"\"\n",
    "    return np.outer(a, b)\n",
    "\n",
    "# 測試\n",
    "a = np.random.randn(1000)\n",
    "b = np.random.randn(800)\n",
    "\n",
    "start = time.perf_counter()\n",
    "r1 = outer_forloop(a, b)\n",
    "time_forloop = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "r2 = outer_vectorized(a, b)\n",
    "time_vectorized = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "r3 = outer_numpy(a, b)\n",
    "time_numpy = time.perf_counter() - start\n",
    "\n",
    "print(f\"Outer product (1000 x 800):\")\n",
    "print(f\"  For-loop:    {time_forloop:.4f}s\")\n",
    "print(f\"  Broadcasting: {time_vectorized:.6f}s ({time_forloop/time_vectorized:.1f}x)\")\n",
    "print(f\"  np.outer:    {time_numpy:.6f}s ({time_forloop/time_numpy:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第三部分：2D Convolution 向量化\n",
    "\n",
    "這是本課程最重要的例子之一！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_naive(x, kernel):\n",
    "    \"\"\"最 naive 的 2D 卷積實作\n",
    "    \n",
    "    x: (H, W) 輸入圖片\n",
    "    kernel: (kH, kW) 卷積核\n",
    "    \"\"\"\n",
    "    H, W = x.shape\n",
    "    kH, kW = kernel.shape\n",
    "    out_H = H - kH + 1\n",
    "    out_W = W - kW + 1\n",
    "    \n",
    "    output = np.zeros((out_H, out_W))\n",
    "    \n",
    "    for i in range(out_H):\n",
    "        for j in range(out_W):\n",
    "            # 取出 patch\n",
    "            patch = x[i:i+kH, j:j+kW]\n",
    "            # 對應元素相乘再求和\n",
    "            output[i, j] = np.sum(patch * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def conv2d_strided(x, kernel):\n",
    "    \"\"\"使用 stride_tricks 的向量化卷積\n",
    "    \n",
    "    技巧：用 as_strided 建立所有 patch 的 view，然後一次計算\n",
    "    \"\"\"\n",
    "    H, W = x.shape\n",
    "    kH, kW = kernel.shape\n",
    "    out_H = H - kH + 1\n",
    "    out_W = W - kW + 1\n",
    "    \n",
    "    # 建立 strided view\n",
    "    # shape: (out_H, out_W, kH, kW)\n",
    "    # 這個 view 包含了所有的 patch\n",
    "    shape = (out_H, out_W, kH, kW)\n",
    "    strides = (x.strides[0], x.strides[1], x.strides[0], x.strides[1])\n",
    "    \n",
    "    patches = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n",
    "    \n",
    "    # 一次計算所有輸出\n",
    "    # patches: (out_H, out_W, kH, kW)\n",
    "    # kernel: (kH, kW)\n",
    "    # 使用 einsum: 對 kH, kW 維度求和\n",
    "    output = np.einsum('ijkl,kl->ij', patches, kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# 測試\n",
    "H, W = 128, 128\n",
    "kH, kW = 5, 5\n",
    "\n",
    "x = np.random.randn(H, W)\n",
    "kernel = np.random.randn(kH, kW)\n",
    "\n",
    "# Naive\n",
    "start = time.perf_counter()\n",
    "out1 = conv2d_naive(x, kernel)\n",
    "time_naive = time.perf_counter() - start\n",
    "\n",
    "# Strided\n",
    "start = time.perf_counter()\n",
    "out2 = conv2d_strided(x, kernel)\n",
    "time_strided = time.perf_counter() - start\n",
    "\n",
    "print(f\"2D Convolution ({H}x{W} image, {kH}x{kW} kernel):\")\n",
    "print(f\"  Naive:   {time_naive:.4f}s\")\n",
    "print(f\"  Strided: {time_strided:.6f}s\")\n",
    "print(f\"  Speedup: {time_naive / time_strided:.1f}x\")\n",
    "print(f\"  Match: {np.allclose(out1, out2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的 4D 卷積（batch + channels）\n",
    "\n",
    "def conv2d_4d_naive(x, W, stride=1, padding=0):\n",
    "    \"\"\"4D 卷積的 naive 實作\n",
    "    \n",
    "    x: (N, C_in, H, W)\n",
    "    W: (C_out, C_in, kH, kW)\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = x.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)))\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    output = np.zeros((N, C_out, out_H, out_W))\n",
    "    \n",
    "    for n in range(N):              # batch\n",
    "        for c_out in range(C_out):  # output channel\n",
    "            for i in range(out_H):  # height\n",
    "                for j in range(out_W):  # width\n",
    "                    # 取出 patch\n",
    "                    h_start = i * stride\n",
    "                    w_start = j * stride\n",
    "                    patch = x[n, :, h_start:h_start+kH, w_start:w_start+kW]\n",
    "                    # 對所有 input channels 求和\n",
    "                    output[n, c_out, i, j] = np.sum(patch * W[c_out])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def conv2d_4d_vectorized(x, W, stride=1, padding=0):\n",
    "    \"\"\"4D 卷積的向量化實作（使用 im2col）\n",
    "    \n",
    "    im2col 將卷積轉換成矩陣乘法\n",
    "    \"\"\"\n",
    "    N, C_in, H, W_in = x.shape\n",
    "    C_out, _, kH, kW = W.shape\n",
    "    \n",
    "    # Padding\n",
    "    if padding > 0:\n",
    "        x = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)))\n",
    "    \n",
    "    H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "    out_H = (H_pad - kH) // stride + 1\n",
    "    out_W = (W_pad - kW) // stride + 1\n",
    "    \n",
    "    # im2col: (N, C_in, H, W) -> (N*out_H*out_W, C_in*kH*kW)\n",
    "    shape = (N, C_in, kH, kW, out_H, out_W)\n",
    "    strides = (x.strides[0], x.strides[1], x.strides[2], x.strides[3],\n",
    "               x.strides[2] * stride, x.strides[3] * stride)\n",
    "    \n",
    "    col = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_H * out_W, -1)\n",
    "    \n",
    "    # Reshape kernel: (C_out, C_in*kH*kW)\n",
    "    W_col = W.reshape(C_out, -1)\n",
    "    \n",
    "    # Matrix multiplication\n",
    "    out_col = col @ W_col.T  # (N*out_H*out_W, C_out)\n",
    "    \n",
    "    # Reshape output: (N, out_H, out_W, C_out) -> (N, C_out, out_H, out_W)\n",
    "    output = out_col.reshape(N, out_H, out_W, C_out).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# 測試\n",
    "N, C_in, H, W_in = 2, 3, 32, 32\n",
    "C_out, kH, kW = 16, 3, 3\n",
    "\n",
    "x = np.random.randn(N, C_in, H, W_in)\n",
    "W = np.random.randn(C_out, C_in, kH, kW)\n",
    "\n",
    "# Naive\n",
    "start = time.perf_counter()\n",
    "out1 = conv2d_4d_naive(x, W, padding=1)\n",
    "time_naive = time.perf_counter() - start\n",
    "\n",
    "# Vectorized\n",
    "start = time.perf_counter()\n",
    "out2 = conv2d_4d_vectorized(x, W, padding=1)\n",
    "time_vectorized = time.perf_counter() - start\n",
    "\n",
    "print(f\"4D Convolution:\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Kernel: {W.shape}\")\n",
    "print(f\"  Output: {out1.shape}\")\n",
    "print(f\"\\n  Naive:      {time_naive:.4f}s\")\n",
    "print(f\"  Vectorized: {time_vectorized:.4f}s\")\n",
    "print(f\"  Speedup:    {time_naive / time_vectorized:.1f}x\")\n",
    "print(f\"  Match: {np.allclose(out1, out2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 第四部分：綜合效能比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同輸入大小下的效能比較\n",
    "\n",
    "def benchmark_conv2d(sizes, C_in=3, C_out=16, kH=3):\n",
    "    \"\"\"測試不同輸入大小的卷積效能\"\"\"\n",
    "    results = {'size': [], 'naive': [], 'vectorized': []}\n",
    "    \n",
    "    for size in sizes:\n",
    "        x = np.random.randn(1, C_in, size, size).astype(np.float32)\n",
    "        W = np.random.randn(C_out, C_in, kH, kH).astype(np.float32)\n",
    "        \n",
    "        # Naive (只測小的，否則太慢)\n",
    "        if size <= 64:\n",
    "            start = time.perf_counter()\n",
    "            out1 = conv2d_4d_naive(x, W, padding=1)\n",
    "            time_naive = time.perf_counter() - start\n",
    "        else:\n",
    "            time_naive = np.nan\n",
    "        \n",
    "        # Vectorized\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(5):\n",
    "            out2 = conv2d_4d_vectorized(x, W, padding=1)\n",
    "        time_vectorized = (time.perf_counter() - start) / 5\n",
    "        \n",
    "        results['size'].append(size)\n",
    "        results['naive'].append(time_naive)\n",
    "        results['vectorized'].append(time_vectorized)\n",
    "        \n",
    "        if not np.isnan(time_naive):\n",
    "            print(f\"Size {size:4d}x{size:4d}: naive={time_naive:.4f}s, \"\n",
    "                  f\"vectorized={time_vectorized:.4f}s, \"\n",
    "                  f\"speedup={time_naive/time_vectorized:.1f}x\")\n",
    "        else:\n",
    "            print(f\"Size {size:4d}x{size:4d}: naive=skipped, \"\n",
    "                  f\"vectorized={time_vectorized:.4f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "sizes = [16, 32, 64, 128, 256]\n",
    "results = benchmark_conv2d(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製效能圖\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sizes = results['size']\n",
    "naive = results['naive']\n",
    "vectorized = results['vectorized']\n",
    "\n",
    "# 過濾掉 NaN\n",
    "valid_idx = [i for i, v in enumerate(naive) if not np.isnan(v)]\n",
    "sizes_valid = [sizes[i] for i in valid_idx]\n",
    "naive_valid = [naive[i] for i in valid_idx]\n",
    "\n",
    "ax.semilogy(sizes_valid, naive_valid, 'o-', label='Naive (for-loop)', linewidth=2)\n",
    "ax.semilogy(sizes, vectorized, 's-', label='Vectorized (im2col)', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Image Size (pixels)', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds, log scale)', fontsize=12)\n",
    "ax.set_title('2D Convolution Performance: Naive vs Vectorized', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"1. 向量化版本快 10-100 倍以上\")\n",
    "print(\"2. 隨著輸入變大，差距更明顯\")\n",
    "print(\"3. im2col 把卷積轉成 GEMM，可以利用 BLAS 多執行緒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 總結\n",
    "\n",
    "### 向量化技巧總結\n",
    "\n",
    "1. **基本原則**：\n",
    "   - 把 Python 迴圈換成 numpy 操作\n",
    "   - 一次 numpy 呼叫 > 多次迴圈\n",
    "\n",
    "2. **Broadcasting**：\n",
    "   - 理解 broadcasting 規則\n",
    "   - 用 `keepdims=True` 保持維度\n",
    "   - 用 `np.newaxis` 增加維度\n",
    "\n",
    "3. **常用技巧**：\n",
    "   - `np.einsum` 處理複雜的張量操作\n",
    "   - `np.lib.stride_tricks.as_strided` 建立高效的 view\n",
    "   - `axis` 參數控制操作的維度\n",
    "\n",
    "4. **im2col**：\n",
    "   - 把卷積轉成矩陣乘法\n",
    "   - 利用 BLAS 的高度優化\n",
    "   - 詳細實作見下一個 notebook\n",
    "\n",
    "### 效能提升幅度\n",
    "\n",
    "| 操作 | For-loop vs Vectorized |\n",
    "|-----|------------------------|\n",
    "| 向量加法 | 10-100x |\n",
    "| 距離計算 | 100-1000x |\n",
    "| Softmax | 10-50x |\n",
    "| 2D 卷積 | 50-200x |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
